2024-04-19 20:53:24,079 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,079 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,079 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,079 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,312 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,312 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,312 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,312 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,434 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,434 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,434 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,434 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,545 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,545 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,545 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,545 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:41237'
2024-04-19 20:53:24,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:44255'
2024-04-19 20:53:24,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:45273'
2024-04-19 20:53:24,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:44679'
2024-04-19 20:53:24,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.138:33111'
2024-04-19 20:53:24,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.138:40229'
2024-04-19 20:53:24,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.138:33565'
2024-04-19 20:53:24,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.138:40483'
2024-04-19 20:53:25,528 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,528 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,529 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,529 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,529 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,530 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,529 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,530 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,536 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,536 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,536 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,536 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,537 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,537 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,537 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,537 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,574 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,574 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,574 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,574 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,582 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,582 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,583 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,583 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:26,534 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,534 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,534 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,534 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,539 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,539 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,539 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,540 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:27,863 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b53f82ad-2428-409c-8dc8-36ca73fd4b08
2024-04-19 20:53:27,863 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:33103
2024-04-19 20:53:27,863 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:33103
2024-04-19 20:53:27,863 - distributed.worker - INFO -          dashboard at:         10.201.2.137:43995
2024-04-19 20:53:27,863 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,863 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,863 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,863 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ay16kvb
2024-04-19 20:53:27,863 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,865 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9dc5cfc9-0aff-4396-b276-958a8761b100
2024-04-19 20:53:27,865 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.138:44031
2024-04-19 20:53:27,866 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.138:44031
2024-04-19 20:53:27,866 - distributed.worker - INFO -          dashboard at:         10.201.2.138:42285
2024-04-19 20:53:27,866 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,866 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,866 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,866 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l0oexy_n
2024-04-19 20:53:27,866 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,867 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2eb37396-bf70-4ac8-840a-88c2cacb60e2
2024-04-19 20:53:27,867 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35219
2024-04-19 20:53:27,867 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35219
2024-04-19 20:53:27,868 - distributed.worker - INFO -          dashboard at:         10.201.2.137:40519
2024-04-19 20:53:27,868 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,868 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,868 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,868 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,868 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sgj9swym
2024-04-19 20:53:27,868 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,869 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3ef730ba-23d6-4c4d-b0b2-b16d00d30a94
2024-04-19 20:53:27,869 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.138:37261
2024-04-19 20:53:27,869 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.138:37261
2024-04-19 20:53:27,869 - distributed.worker - INFO -          dashboard at:         10.201.2.138:37873
2024-04-19 20:53:27,869 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,869 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,869 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,869 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,869 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5nen9ed9
2024-04-19 20:53:27,870 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,871 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-99f5403e-2150-4f64-9207-77efeaebd885
2024-04-19 20:53:27,871 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35455
2024-04-19 20:53:27,871 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35455
2024-04-19 20:53:27,872 - distributed.worker - INFO -          dashboard at:         10.201.2.137:36645
2024-04-19 20:53:27,872 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,872 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,872 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,872 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,872 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u6x2nob_
2024-04-19 20:53:27,872 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,873 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-6e1b7412-2c82-4649-be8d-0ae0ea912925
2024-04-19 20:53:27,873 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.138:44891
2024-04-19 20:53:27,873 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.138:44891
2024-04-19 20:53:27,873 - distributed.worker - INFO -          dashboard at:         10.201.2.138:35875
2024-04-19 20:53:27,873 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,873 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,873 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,873 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-82o75p_i
2024-04-19 20:53:27,873 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,874 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0ad56516-3c4a-4874-916f-dcfc04877d36
2024-04-19 20:53:27,875 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:36763
2024-04-19 20:53:27,875 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:36763
2024-04-19 20:53:27,875 - distributed.worker - INFO -          dashboard at:         10.201.2.137:44455
2024-04-19 20:53:27,875 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,875 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,875 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,875 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,875 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xbssy_ya
2024-04-19 20:53:27,875 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,876 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-bff7a711-0cfb-454b-8ddc-f9f15a4c8626
2024-04-19 20:53:27,876 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.138:38125
2024-04-19 20:53:27,876 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.138:38125
2024-04-19 20:53:27,877 - distributed.worker - INFO -          dashboard at:         10.201.2.138:37939
2024-04-19 20:53:27,877 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 20:53:27,877 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,877 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,877 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hoyuz1e_
2024-04-19 20:53:27,877 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,187 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,187 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,188 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,188 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,190 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,190 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,190 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,193 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,193 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,194 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,194 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,195 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,195 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,195 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,196 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,196 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,197 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,197 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,198 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,198 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,198 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,199 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:31,199 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 20:53:31,199 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,199 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 20:53:40,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:40,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:40,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:40,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,850 - distributed.utils_perf - INFO - full garbage collection released 156.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:48,015 - distributed.utils_perf - INFO - full garbage collection released 363.76 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:48,926 - distributed.utils_perf - INFO - full garbage collection released 511.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,046 - distributed.utils_perf - INFO - full garbage collection released 0.93 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,098 - distributed.utils_perf - INFO - full garbage collection released 0.98 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,725 - distributed.utils_perf - INFO - full garbage collection released 287.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:50,271 - distributed.utils_perf - INFO - full garbage collection released 1.47 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:50,736 - distributed.utils_perf - INFO - full garbage collection released 293.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:51,635 - distributed.utils_perf - INFO - full garbage collection released 465.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:52,390 - distributed.utils_perf - INFO - full garbage collection released 671.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:52,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:52,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:52,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,371 - distributed.utils_perf - INFO - full garbage collection released 159.28 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:54,186 - distributed.utils_perf - INFO - full garbage collection released 1.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:54,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:55,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:55,641 - distributed.utils_perf - INFO - full garbage collection released 1.11 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:56,242 - distributed.utils_perf - INFO - full garbage collection released 242.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:56,322 - distributed.utils_perf - INFO - full garbage collection released 2.35 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:58,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:59,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:59,496 - distributed.utils_perf - INFO - full garbage collection released 1.77 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:00,819 - distributed.utils_perf - INFO - full garbage collection released 2.04 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:03,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:03,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:04,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:06,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:07,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:07,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:07,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:09,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:10,599 - distributed.utils_perf - INFO - full garbage collection released 161.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:11,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:12,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:13,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:14,619 - distributed.utils_perf - INFO - full garbage collection released 366.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:16,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:17,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:17,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:18,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:20,106 - distributed.utils_perf - INFO - full garbage collection released 514.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:20,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:21,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:23,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:27,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:28,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:29,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:32,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:32,329 - distributed.utils_perf - INFO - full garbage collection released 323.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:34,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:35,340 - distributed.utils_perf - INFO - full garbage collection released 500.32 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:37,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:39,654 - distributed.utils_perf - INFO - full garbage collection released 17.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:42,957 - distributed.utils_perf - INFO - full garbage collection released 172.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:45,170 - distributed.utils_perf - INFO - full garbage collection released 222.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:45,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:45,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:46,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:46,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:49,882 - distributed.utils_perf - INFO - full garbage collection released 130.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:50,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:50,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:50,756 - distributed.utils_perf - INFO - full garbage collection released 333.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:51,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:54,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:57,220 - distributed.utils_perf - INFO - full garbage collection released 3.08 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:58,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:59,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:00,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:02,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:04,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:04,962 - distributed.utils_perf - INFO - full garbage collection released 224.53 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:05,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:08,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:12,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:12,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:13,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:19,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:19,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:21,221 - distributed.utils_perf - INFO - full garbage collection released 47.01 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:21,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:21,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:22,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:24,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:26,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:27,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:29,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:29,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:30,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:32,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:33,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:35,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:35,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:37,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:42,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:43,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:43,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:43,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:44,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:45,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:50,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:51,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:54,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:55,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:00,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:01,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:03,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:06,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:10,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:14,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:18,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:22,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:24,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:26,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:30,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:30,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:34,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:36,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:40,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:40,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:42,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:42,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:42,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:43,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:47,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:48,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:49,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:50,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:51,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:52,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:52,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:58,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:59,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:01,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:04,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:07,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:07,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:09,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:10,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:11,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:12,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:14,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:18,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:23,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:23,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:25,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:28,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:29,108 - distributed.utils_perf - INFO - full garbage collection released 2.95 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:30,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:30,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:31,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:32,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:34,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:35,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:37,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:39,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:41,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:42,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:45,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:54,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:55,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:56,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:56,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:56,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:58,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:59,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:01,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:03,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:06,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:06,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:08,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:09,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:11,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:13,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:16,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:17,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:24,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:24,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:25,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:26,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:26,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:31,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:36,223 - distributed.utils_perf - INFO - full garbage collection released 283.22 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:36,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:36,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:37,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:41,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:43,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:46,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:47,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:47,214 - distributed.utils_perf - INFO - full garbage collection released 105.16 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:50,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:51,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:54,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:54,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:55,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:55,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:56,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:57,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:02,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:02,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:03,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:03,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:05,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:09,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:09,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:10,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:12,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:14,012 - distributed.utils_perf - INFO - full garbage collection released 1.83 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:59:15,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:19,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:21,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:24,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:27,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:29,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:31,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:31,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:31,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:34,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:36,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:36,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:37,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:40,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:41,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:43,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:45,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:46,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:50,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:51,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:53,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:54,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:55,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:59,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:03,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:04,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:06,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:09,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:10,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:12,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:12,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:15,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:17,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:19,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:20,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:20,624 - distributed.utils_perf - INFO - full garbage collection released 2.70 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:21,993 - distributed.utils_perf - INFO - full garbage collection released 600.86 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:23,548 - distributed.utils_perf - INFO - full garbage collection released 1.21 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:23,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:26,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:29,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:29,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:29,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:31,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:32,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:33,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:35,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:39,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:42,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:42,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:44,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:44,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:47,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:48,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:48,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:49,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:51,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:51,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:54,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:55,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:59,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:59,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:00,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:05,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:05,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:05,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:06,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:06,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:08,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,884 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:11,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:11,914 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:13,194 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:13,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:13,905 - distributed.utils_perf - INFO - full garbage collection released 636.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:14,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:14,800 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:16,748 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:16,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:18,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:19,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:19,167 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 21:01:19,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:20,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:21,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:21,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:25,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:25,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:26,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:38,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:38,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:39,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:40,763 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:40,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:41,583 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:42,597 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:01:43,850 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:01:44,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:45,425 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:47,374 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:48,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:49,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:49,787 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:50,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:50,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:52,759 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:56,428 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 21:01:56,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:00,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:00,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:04,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:06,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:06,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:07,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:08,021 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 21:02:09,105 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 21:02:10,441 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 21:02:12,152 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 21:02:12,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:14,252 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 21:02:16,831 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 21:02:18,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:18,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:20,004 - distributed.utils_perf - WARNING - full garbage collections took 37% CPU time recently (threshold: 10%)
2024-04-19 21:02:20,284 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:20,774 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:21,370 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 21:02:22,104 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 21:02:23,011 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 21:02:24,135 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 21:02:25,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:25,523 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:27,258 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:27,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:29,403 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:29,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:30,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:31,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:32,063 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 21:02:39,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:39,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:45,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:57,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:21,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:31,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:33,191 - distributed.utils_perf - INFO - full garbage collection released 306.20 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:03:43,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:04:16] task [xgboost.dask-tcp://10.201.2.137:33103]:tcp://10.201.2.137:33103 got new rank 0
[21:04:16] task [xgboost.dask-tcp://10.201.2.137:35219]:tcp://10.201.2.137:35219 got new rank 1
[21:04:16] task [xgboost.dask-tcp://10.201.2.137:35455]:tcp://10.201.2.137:35455 got new rank 2
[21:04:16] task [xgboost.dask-tcp://10.201.2.137:36763]:tcp://10.201.2.137:36763 got new rank 3
[21:04:16] task [xgboost.dask-tcp://10.201.2.138:37261]:tcp://10.201.2.138:37261 got new rank 4
[21:04:16] task [xgboost.dask-tcp://10.201.2.138:38125]:tcp://10.201.2.138:38125 got new rank 5
[21:04:16] task [xgboost.dask-tcp://10.201.2.138:44031]:tcp://10.201.2.138:44031 got new rank 6
[21:04:16] task [xgboost.dask-tcp://10.201.2.138:44891]:tcp://10.201.2.138:44891 got new rank 7
2024-04-19 21:06:35,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:07:23] task [xgboost.dask-tcp://10.201.2.137:33103]:tcp://10.201.2.137:33103 got new rank 0
[21:07:23] task [xgboost.dask-tcp://10.201.2.137:35219]:tcp://10.201.2.137:35219 got new rank 1
[21:07:23] task [xgboost.dask-tcp://10.201.2.137:35455]:tcp://10.201.2.137:35455 got new rank 2
[21:07:23] task [xgboost.dask-tcp://10.201.2.137:36763]:tcp://10.201.2.137:36763 got new rank 3
[21:07:23] task [xgboost.dask-tcp://10.201.2.138:37261]:tcp://10.201.2.138:37261 got new rank 4
[21:07:23] task [xgboost.dask-tcp://10.201.2.138:38125]:tcp://10.201.2.138:38125 got new rank 5
[21:07:23] task [xgboost.dask-tcp://10.201.2.138:44031]:tcp://10.201.2.138:44031 got new rank 6
[21:07:23] task [xgboost.dask-tcp://10.201.2.138:44891]:tcp://10.201.2.138:44891 got new rank 7
2024-04-19 21:09:56,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:09:57,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:10:28] task [xgboost.dask-tcp://10.201.2.137:33103]:tcp://10.201.2.137:33103 got new rank 0
[21:10:28] task [xgboost.dask-tcp://10.201.2.137:35219]:tcp://10.201.2.137:35219 got new rank 1
[21:10:28] task [xgboost.dask-tcp://10.201.2.137:35455]:tcp://10.201.2.137:35455 got new rank 2
[21:10:28] task [xgboost.dask-tcp://10.201.2.137:36763]:tcp://10.201.2.137:36763 got new rank 3
[21:10:28] task [xgboost.dask-tcp://10.201.2.138:37261]:tcp://10.201.2.138:37261 got new rank 4
[21:10:28] task [xgboost.dask-tcp://10.201.2.138:38125]:tcp://10.201.2.138:38125 got new rank 5
[21:10:28] task [xgboost.dask-tcp://10.201.2.138:44031]:tcp://10.201.2.138:44031 got new rank 6
[21:10:28] task [xgboost.dask-tcp://10.201.2.138:44891]:tcp://10.201.2.138:44891 got new rank 7
2024-04-19 21:12:40,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:41,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:41,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:41,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:42,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:13:27] task [xgboost.dask-tcp://10.201.2.137:33103]:tcp://10.201.2.137:33103 got new rank 0
[21:13:27] task [xgboost.dask-tcp://10.201.2.137:35219]:tcp://10.201.2.137:35219 got new rank 1
[21:13:27] task [xgboost.dask-tcp://10.201.2.137:35455]:tcp://10.201.2.137:35455 got new rank 2
[21:13:27] task [xgboost.dask-tcp://10.201.2.137:36763]:tcp://10.201.2.137:36763 got new rank 3
[21:13:27] task [xgboost.dask-tcp://10.201.2.138:37261]:tcp://10.201.2.138:37261 got new rank 4
[21:13:27] task [xgboost.dask-tcp://10.201.2.138:38125]:tcp://10.201.2.138:38125 got new rank 5
[21:13:27] task [xgboost.dask-tcp://10.201.2.138:44031]:tcp://10.201.2.138:44031 got new rank 6
[21:13:27] task [xgboost.dask-tcp://10.201.2.138:44891]:tcp://10.201.2.138:44891 got new rank 7
2024-04-19 21:16:26,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:40,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:41,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:41,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:41,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:42,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:42,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:43,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:16:43,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:17:29] task [xgboost.dask-tcp://10.201.2.137:33103]:tcp://10.201.2.137:33103 got new rank 0
[21:17:29] task [xgboost.dask-tcp://10.201.2.137:35219]:tcp://10.201.2.137:35219 got new rank 1
[21:17:29] task [xgboost.dask-tcp://10.201.2.137:35455]:tcp://10.201.2.137:35455 got new rank 2
[21:17:29] task [xgboost.dask-tcp://10.201.2.138:37261]:tcp://10.201.2.138:37261 got new rank 3
[21:17:29] task [xgboost.dask-tcp://10.201.2.138:38125]:tcp://10.201.2.138:38125 got new rank 4
[21:17:29] task [xgboost.dask-tcp://10.201.2.138:44031]:tcp://10.201.2.138:44031 got new rank 5
[21:17:29] task [xgboost.dask-tcp://10.201.2.138:44891]:tcp://10.201.2.138:44891 got new rank 6
2024-04-19 21:20:05,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35219. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:33103. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35455. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:36763. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.138:44891. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.138:44031. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.138:37261. Reason: scheduler-close
2024-04-19 21:20:39,808 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.138:38125. Reason: scheduler-close
2024-04-19 21:20:39,809 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:44255'. Reason: scheduler-close
2024-04-19 21:20:39,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.138:40483'. Reason: scheduler-close
2024-04-19 21:20:39,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.138:33565'. Reason: scheduler-close
2024-04-19 21:20:39,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.138:40229'. Reason: scheduler-close
2024-04-19 21:20:39,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.138:33111'. Reason: scheduler-close
2024-04-19 21:20:39,809 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34432 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34432 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:39,809 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34446 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34446 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:39,809 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34444 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:34444 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:39,814 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:45273'. Reason: scheduler-close
2024-04-19 21:20:39,814 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:41237'. Reason: scheduler-close
2024-04-19 21:20:39,814 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:44679'. Reason: scheduler-close
2024-04-19 21:20:40,595 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:20:40,595 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:20:40,595 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:34492 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.137:34474 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.137:58434 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:40,596 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:34458 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:40,601 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,601 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,602 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.138:36856 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:40,603 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:20:40,603 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:20:40,602 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.138:36860 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.138:36838 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.138:36844 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 21:20:40,623 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,623 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,625 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,626 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,666 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,666 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,690 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,691 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,706 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,706 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,710 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,710 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:40,748 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 21:20:40,748 - distributed.nanny - INFO - Worker closed
2024-04-19 21:20:42,661 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:20:42,694 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:20:42,711 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:20:42,749 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:20:43,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:45273'. Reason: nanny-close-gracefully
2024-04-19 21:20:43,376 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:43,435 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:44679'. Reason: nanny-close-gracefully
2024-04-19 21:20:43,435 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:43,450 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:41237'. Reason: nanny-close-gracefully
2024-04-19 21:20:43,451 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:43,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:44255'. Reason: nanny-close-gracefully
2024-04-19 21:20:43,915 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:44,164 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.138:33111'. Reason: nanny-close-gracefully
2024-04-19 21:20:44,165 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:44,272 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.138:40229'. Reason: nanny-close-gracefully
2024-04-19 21:20:44,272 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:44,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.138:40483'. Reason: nanny-close-gracefully
2024-04-19 21:20:44,279 - distributed.dask_worker - INFO - End worker
2024-04-19 21:20:44,631 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.138:33565'. Reason: nanny-close-gracefully
2024-04-19 21:20:44,632 - distributed.dask_worker - INFO - End worker
