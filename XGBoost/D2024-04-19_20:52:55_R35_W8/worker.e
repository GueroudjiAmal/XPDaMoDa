2024-04-19 20:53:23,734 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:23,734 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:23,734 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:23,734 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,003 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,022 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,022 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,022 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,023 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.228:32971'
2024-04-19 20:53:24,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.228:46083'
2024-04-19 20:53:24,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.228:44035'
2024-04-19 20:53:24,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.228:34927'
2024-04-19 20:53:24,237 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,237 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,237 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,237 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.229:45493'
2024-04-19 20:53:24,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.229:45155'
2024-04-19 20:53:24,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.229:46267'
2024-04-19 20:53:24,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.229:38333'
2024-04-19 20:53:25,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,124 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,124 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,125 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,125 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,214 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,214 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,214 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,214 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,214 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,259 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,259 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,259 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,259 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:26,065 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,065 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,065 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,065 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,206 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,206 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,206 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,206 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:27,129 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-23c716e1-58bb-4239-86ab-d95875a44aed
2024-04-19 20:53:27,129 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0c9e1cb1-e443-4b98-a56d-53143f463952
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.228:45787
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.228:35435
2024-04-19 20:53:27,129 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.228:35435
2024-04-19 20:53:27,129 - distributed.worker - INFO -          dashboard at:         10.201.3.228:45753
2024-04-19 20:53:27,129 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,129 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vim_o4zu
2024-04-19 20:53:27,129 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-77bf0eb5-18b2-4bbd-9031-8a3a7922dd39
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.228:36941
2024-04-19 20:53:27,129 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.228:36941
2024-04-19 20:53:27,129 - distributed.worker - INFO -          dashboard at:         10.201.3.228:41069
2024-04-19 20:53:27,129 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,129 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_x77gp1p
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f562c952-9c4e-4dc6-9f0f-10f0246dcca1
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.228:33595
2024-04-19 20:53:27,129 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.228:33595
2024-04-19 20:53:27,129 - distributed.worker - INFO -          dashboard at:         10.201.3.228:38327
2024-04-19 20:53:27,129 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,129 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wf05hlm7
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.228:45787
2024-04-19 20:53:27,129 - distributed.worker - INFO -          dashboard at:         10.201.3.228:39337
2024-04-19 20:53:27,129 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,129 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,129 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t3ufl0qz
2024-04-19 20:53:27,130 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,129 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,343 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-75fd9144-9cdd-4713-b3b8-b7b00aecb0cc
2024-04-19 20:53:27,343 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.229:45743
2024-04-19 20:53:27,343 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.229:45743
2024-04-19 20:53:27,343 - distributed.worker - INFO -          dashboard at:         10.201.3.229:35219
2024-04-19 20:53:27,343 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,343 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,343 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c2e019df-23a3-44c5-a94c-f57fda36a900
2024-04-19 20:53:27,343 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,343 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o93owy5j
2024-04-19 20:53:27,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,343 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d5ba2c8e-10bc-43b9-9410-da9be3b77a44
2024-04-19 20:53:27,343 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-425f5147-3cd7-456f-9c10-3daa2b661e5d
2024-04-19 20:53:27,343 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.229:44769
2024-04-19 20:53:27,343 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.229:44769
2024-04-19 20:53:27,343 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.229:44787
2024-04-19 20:53:27,343 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.229:44787
2024-04-19 20:53:27,343 - distributed.worker - INFO -          dashboard at:         10.201.3.229:35169
2024-04-19 20:53:27,343 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.229:37599
2024-04-19 20:53:27,343 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.229:37599
2024-04-19 20:53:27,343 - distributed.worker - INFO -          dashboard at:         10.201.3.229:42031
2024-04-19 20:53:27,343 - distributed.worker - INFO -          dashboard at:         10.201.3.229:46301
2024-04-19 20:53:27,343 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,343 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,343 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,343 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,344 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f6nthfe6
2024-04-19 20:53:27,343 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.238:8786
2024-04-19 20:53:27,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,344 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,344 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p_xe4j48
2024-04-19 20:53:27,344 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,344 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5wn9z0g9
2024-04-19 20:53:27,344 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,344 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,329 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,329 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,329 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,330 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,330 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,330 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,331 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,334 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,334 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,334 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,335 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,335 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,336 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,335 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,336 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,336 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,337 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,337 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,337 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,338 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,338 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,338 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,339 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:30,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:30,339 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.238:8786
2024-04-19 20:53:30,339 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:30,340 - distributed.core - INFO - Starting established connection to tcp://10.201.3.238:8786
2024-04-19 20:53:40,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:40,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:42,906 - distributed.utils_perf - INFO - full garbage collection released 104.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:44,933 - distributed.utils_perf - INFO - full garbage collection released 753.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:45,197 - distributed.utils_perf - INFO - full garbage collection released 781.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:48,136 - distributed.utils_perf - INFO - full garbage collection released 355.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:48,763 - distributed.utils_perf - INFO - full garbage collection released 259.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,014 - distributed.utils_perf - INFO - full garbage collection released 839.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,313 - distributed.utils_perf - INFO - full garbage collection released 55.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:50,037 - distributed.utils_perf - INFO - full garbage collection released 1.70 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:52,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:54,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:57,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,349 - distributed.utils_perf - INFO - full garbage collection released 6.96 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:58,744 - distributed.utils_perf - INFO - full garbage collection released 11.04 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:01,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:05,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:06,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:08,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:09,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:10,684 - distributed.utils_perf - INFO - full garbage collection released 21.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:11,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:11,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:12,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:13,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:15,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:16,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:16,902 - distributed.utils_perf - INFO - full garbage collection released 80.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:17,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:20,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:20,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:22,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:26,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:26,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:26,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:29,703 - distributed.utils_perf - INFO - full garbage collection released 63.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:34,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:34,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:35,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:37,484 - distributed.utils_perf - INFO - full garbage collection released 426.14 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:40,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:41,660 - distributed.utils_perf - INFO - full garbage collection released 217.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:42,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:42,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:43,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:48,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:48,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:51,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:51,812 - distributed.utils_perf - INFO - full garbage collection released 141.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:54,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:57,121 - distributed.utils_perf - INFO - full garbage collection released 180.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:57,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:59,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:00,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:00,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:01,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:03,166 - distributed.utils_perf - INFO - full garbage collection released 240.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:04,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:04,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:06,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:08,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:09,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:09,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:11,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:12,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:14,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:17,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:18,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:19,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:21,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:21,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:24,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:25,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:26,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:27,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:27,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:28,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:31,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:36,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,012 - distributed.utils_perf - INFO - full garbage collection released 119.43 MiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:38,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:39,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:39,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:42,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:42,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:45,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:45,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:46,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:48,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:49,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:52,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:54,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:55,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:56,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:57,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:01,081 - distributed.utils_perf - INFO - full garbage collection released 101.76 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:01,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:02,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:03,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:06,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:09,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:13,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:16,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:21,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:22,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:27,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:27,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:27,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:21,457 - distributed.utils_perf - INFO - full garbage collection released 1.98 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:30,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:34,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:34,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:34,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:35,557 - distributed.utils_perf - INFO - full garbage collection released 1.22 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:36,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:37,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:41,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:41,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:42,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:51,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:52,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:54,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:55,420 - distributed.utils_perf - INFO - full garbage collection released 15.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:55,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:55,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:56,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:56,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:56,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:56,943 - distributed.utils_perf - INFO - full garbage collection released 4.86 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:58,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:01,160 - distributed.utils_perf - INFO - full garbage collection released 678.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:03,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:04,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:06,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:09,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:11,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:12,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:13,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:14,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:15,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:17,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:20,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:21,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:21,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:24,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:27,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:27,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:29,152 - distributed.utils_perf - INFO - full garbage collection released 108.59 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:29,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:29,235 - distributed.utils_perf - INFO - full garbage collection released 212.17 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:29,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:31,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:33,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:34,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:35,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:37,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:38,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:39,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:40,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:40,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:44,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:47,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:48,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:49,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:51,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:52,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:54,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:55,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:56,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:57,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:58,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:59,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:00,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:02,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:04,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:05,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:05,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:10,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:13,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:14,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:17,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:18,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:19,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:21,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:21,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:23,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:27,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:27,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:30,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:32,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:35,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:36,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:39,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:40,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:42,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:43,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:49,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:51,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:51,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:54,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:55,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:57,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:58,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:59,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:00,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:01,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:02,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:06,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:10,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:11,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:12,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:13,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:18,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:20,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:20,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:26,049 - distributed.utils_perf - INFO - full garbage collection released 3.04 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:59:26,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:28,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:29,612 - distributed.utils_perf - INFO - full garbage collection released 3.93 GiB from 55 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:59:30,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:33,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:33,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:34,175 - distributed.utils_perf - INFO - full garbage collection released 169.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:59:34,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:37,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:40,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:43,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:43,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:47,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:49,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:53,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:54,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:55,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:59,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:00,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:05,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:05,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:07,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:09,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:14,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:14,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:15,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:17,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:18,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:18,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:22,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:24,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:30,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:30,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:31,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:34,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:34,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:36,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:36,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:46,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:46,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:47,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:47,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:48,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:50,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:52,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:52,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:56,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:58,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:58,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:59,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:59,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:01,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:02,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:06,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:08,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,111 - distributed.utils_perf - INFO - full garbage collection released 14.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:14,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:14,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:15,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:15,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:16,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:18,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:20,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:20,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:20,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:21,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:23,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:23,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:24,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:26,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:32,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:33,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:33,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:38,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:38,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:39,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:42,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:45,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:45,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:46,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:49,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:55,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:55,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:57,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:59,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:59,731 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 21:02:00,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:00,728 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:01,953 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:03,506 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:05,444 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:05,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:05,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:07,315 - distributed.utils_perf - INFO - full garbage collection released 851.03 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:02:07,812 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:10,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:10,745 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:11,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:11,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:14,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:18,648 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:19,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:20,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:21,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:23,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:27,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:28,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:31,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:32,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:42,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:43,654 - distributed.utils_perf - INFO - full garbage collection released 653.88 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:02:54,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:03:24] task [xgboost.dask-tcp://10.201.3.228:33595]:tcp://10.201.3.228:33595 got new rank 0
[21:03:24] task [xgboost.dask-tcp://10.201.3.228:35435]:tcp://10.201.3.228:35435 got new rank 1
[21:03:25] task [xgboost.dask-tcp://10.201.3.228:36941]:tcp://10.201.3.228:36941 got new rank 2
[21:03:25] task [xgboost.dask-tcp://10.201.3.228:45787]:tcp://10.201.3.228:45787 got new rank 3
[21:03:25] task [xgboost.dask-tcp://10.201.3.229:37599]:tcp://10.201.3.229:37599 got new rank 4
[21:03:25] task [xgboost.dask-tcp://10.201.3.229:44769]:tcp://10.201.3.229:44769 got new rank 5
[21:03:25] task [xgboost.dask-tcp://10.201.3.229:44787]:tcp://10.201.3.229:44787 got new rank 6
[21:03:25] task [xgboost.dask-tcp://10.201.3.229:45743]:tcp://10.201.3.229:45743 got new rank 7
[21:06:18] task [xgboost.dask-tcp://10.201.3.228:33595]:tcp://10.201.3.228:33595 got new rank 0
[21:06:18] task [xgboost.dask-tcp://10.201.3.228:35435]:tcp://10.201.3.228:35435 got new rank 1
[21:06:18] task [xgboost.dask-tcp://10.201.3.228:36941]:tcp://10.201.3.228:36941 got new rank 2
[21:06:18] task [xgboost.dask-tcp://10.201.3.228:45787]:tcp://10.201.3.228:45787 got new rank 3
[21:06:18] task [xgboost.dask-tcp://10.201.3.229:37599]:tcp://10.201.3.229:37599 got new rank 4
[21:06:18] task [xgboost.dask-tcp://10.201.3.229:44769]:tcp://10.201.3.229:44769 got new rank 5
[21:06:18] task [xgboost.dask-tcp://10.201.3.229:44787]:tcp://10.201.3.229:44787 got new rank 6
[21:06:18] task [xgboost.dask-tcp://10.201.3.229:45743]:tcp://10.201.3.229:45743 got new rank 7
2024-04-19 21:09:16,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:09:16,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:09:17,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:09:50] task [xgboost.dask-tcp://10.201.3.228:33595]:tcp://10.201.3.228:33595 got new rank 0
[21:09:50] task [xgboost.dask-tcp://10.201.3.228:35435]:tcp://10.201.3.228:35435 got new rank 1
[21:09:50] task [xgboost.dask-tcp://10.201.3.228:36941]:tcp://10.201.3.228:36941 got new rank 2
[21:09:50] task [xgboost.dask-tcp://10.201.3.228:45787]:tcp://10.201.3.228:45787 got new rank 3
[21:09:50] task [xgboost.dask-tcp://10.201.3.229:37599]:tcp://10.201.3.229:37599 got new rank 4
[21:09:50] task [xgboost.dask-tcp://10.201.3.229:44769]:tcp://10.201.3.229:44769 got new rank 5
[21:09:50] task [xgboost.dask-tcp://10.201.3.229:44787]:tcp://10.201.3.229:44787 got new rank 6
[21:09:50] task [xgboost.dask-tcp://10.201.3.229:45743]:tcp://10.201.3.229:45743 got new rank 7
2024-04-19 21:12:09,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:12:46] task [xgboost.dask-tcp://10.201.3.228:33595]:tcp://10.201.3.228:33595 got new rank 0
[21:12:46] task [xgboost.dask-tcp://10.201.3.228:35435]:tcp://10.201.3.228:35435 got new rank 1
[21:12:46] task [xgboost.dask-tcp://10.201.3.228:36941]:tcp://10.201.3.228:36941 got new rank 2
[21:12:46] task [xgboost.dask-tcp://10.201.3.228:45787]:tcp://10.201.3.228:45787 got new rank 3
[21:12:46] task [xgboost.dask-tcp://10.201.3.229:37599]:tcp://10.201.3.229:37599 got new rank 4
[21:12:46] task [xgboost.dask-tcp://10.201.3.229:44769]:tcp://10.201.3.229:44769 got new rank 5
[21:12:46] task [xgboost.dask-tcp://10.201.3.229:44787]:tcp://10.201.3.229:44787 got new rank 6
[21:12:46] task [xgboost.dask-tcp://10.201.3.229:45743]:tcp://10.201.3.229:45743 got new rank 7
2024-04-19 21:14:54,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:54,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:54,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:55,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:08,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:09,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:09,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:10,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:10,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:10,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:10,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:11,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:15:53] task [xgboost.dask-tcp://10.201.3.228:33595]:tcp://10.201.3.228:33595 got new rank 0
[21:15:53] task [xgboost.dask-tcp://10.201.3.228:35435]:tcp://10.201.3.228:35435 got new rank 1
[21:15:53] task [xgboost.dask-tcp://10.201.3.228:36941]:tcp://10.201.3.228:36941 got new rank 2
[21:15:53] task [xgboost.dask-tcp://10.201.3.228:45787]:tcp://10.201.3.228:45787 got new rank 3
[21:15:53] task [xgboost.dask-tcp://10.201.3.229:37599]:tcp://10.201.3.229:37599 got new rank 4
[21:15:53] task [xgboost.dask-tcp://10.201.3.229:44769]:tcp://10.201.3.229:44769 got new rank 5
[21:15:53] task [xgboost.dask-tcp://10.201.3.229:44787]:tcp://10.201.3.229:44787 got new rank 6
[21:15:53] task [xgboost.dask-tcp://10.201.3.229:45743]:tcp://10.201.3.229:45743 got new rank 7
2024-04-19 21:17:53,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:18:26,773 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.228:33595. Reason: scheduler-close
2024-04-19 21:18:26,773 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.228:45787. Reason: scheduler-close
2024-04-19 21:18:26,774 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.229:44787. Reason: scheduler-close
2024-04-19 21:18:26,773 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.228:35435. Reason: scheduler-close
2024-04-19 21:18:26,774 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.229:45743. Reason: scheduler-close
2024-04-19 21:18:26,774 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.229:37599. Reason: scheduler-close
2024-04-19 21:18:26,774 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.229:44769. Reason: scheduler-close
2024-04-19 21:18:26,773 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.228:36941. Reason: scheduler-close
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51676 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51676 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51690 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51690 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51686 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51686 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51668 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.228:51668 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.228:44035'. Reason: scheduler-close
2024-04-19 21:18:26,782 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.228:46083'. Reason: scheduler-close
2024-04-19 21:18:26,781 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.228:34927'. Reason: scheduler-close
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57390 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57390 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,782 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.228:32971'. Reason: scheduler-close
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57358 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57358 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57380 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57380 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,775 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57368 remote=tcp://10.201.3.238:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.229:57368 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:26,783 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.229:46267'. Reason: scheduler-close
2024-04-19 21:18:26,783 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.229:38333'. Reason: scheduler-close
2024-04-19 21:18:26,783 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.229:45493'. Reason: scheduler-close
2024-04-19 21:18:26,783 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.229:45155'. Reason: scheduler-close
2024-04-19 21:18:27,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.228:51712 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:27,564 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:18:27,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:18:27,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.228:51696 remote=tcp://10.201.3.238:8786>: Stream is closed
/10.201.3.228:51722 remote=tcp://10.201.3.238:8786>: Stream is closed
/10.201.3.228:47278 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:27,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:18:27,566 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:18:27,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.229:57414 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:27,565 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.229:47360 remote=tcp://10.201.3.238:8786>: Stream is closed
/10.201.3.229:47338 remote=tcp://10.201.3.238:8786>: Stream is closed
/10.201.3.229:47340 remote=tcp://10.201.3.238:8786>: Stream is closed
2024-04-19 21:18:27,632 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,632 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,647 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,647 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,651 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,651 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,665 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,665 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,673 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,674 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,674 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,674 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,679 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,680 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:27,693 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.238:8786; closing.
2024-04-19 21:18:27,693 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:29,635 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,650 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,653 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,667 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,675 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,676 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,681 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:29,694 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:30,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.228:46083'. Reason: nanny-close-gracefully
2024-04-19 21:18:30,890 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:30,897 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.228:32971'. Reason: nanny-close-gracefully
2024-04-19 21:18:30,897 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:30,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.228:44035'. Reason: nanny-close-gracefully
2024-04-19 21:18:30,977 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:31,046 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.229:45493'. Reason: nanny-close-gracefully
2024-04-19 21:18:31,047 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:31,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.229:46267'. Reason: nanny-close-gracefully
2024-04-19 21:18:31,055 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:31,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.229:45155'. Reason: nanny-close-gracefully
2024-04-19 21:18:31,155 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:31,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.228:34927'. Reason: nanny-close-gracefully
2024-04-19 21:18:31,283 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:31,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.229:38333'. Reason: nanny-close-gracefully
2024-04-19 21:18:31,504 - distributed.dask_worker - INFO - End worker
