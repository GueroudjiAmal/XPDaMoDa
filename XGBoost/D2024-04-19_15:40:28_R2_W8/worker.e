2024-04-19 15:40:51,356 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,356 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,356 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,357 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,382 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,383 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,383 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,383 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,578 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,578 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,578 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,579 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:38359'
2024-04-19 15:40:51,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:39437'
2024-04-19 15:40:51,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:37829'
2024-04-19 15:40:51,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:39239'
2024-04-19 15:40:51,611 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,611 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,611 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,611 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:46635'
2024-04-19 15:40:51,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:34989'
2024-04-19 15:40:51,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:33895'
2024-04-19 15:40:51,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:38979'
2024-04-19 15:40:52,423 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,425 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,425 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,425 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,425 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,426 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,426 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,426 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,467 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,468 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,469 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,469 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,585 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,585 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,586 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,586 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,587 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,587 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,587 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,587 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,631 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,631 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,631 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,632 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:53,413 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,413 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,413 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,413 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,593 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,593 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,593 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,594 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:54,477 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8c7036f8-e94a-4007-a94a-bc87312a1bff
2024-04-19 15:40:54,477 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-397d7256-f92e-4d65-9c4f-360d21cf7ac0
2024-04-19 15:40:54,477 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:40881
2024-04-19 15:40:54,477 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:40881
2024-04-19 15:40:54,477 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b4ef5596-d420-49b2-80a4-0b3b79aaa294
2024-04-19 15:40:54,477 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:45811
2024-04-19 15:40:54,477 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:45811
2024-04-19 15:40:54,477 - distributed.worker - INFO -          dashboard at:          10.201.2.17:36117
2024-04-19 15:40:54,477 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,477 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,477 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,478 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,477 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:42065
2024-04-19 15:40:54,477 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:42065
2024-04-19 15:40:54,477 - distributed.worker - INFO -          dashboard at:          10.201.2.17:35055
2024-04-19 15:40:54,477 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,477 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,477 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,478 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z56omrdj
2024-04-19 15:40:54,478 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,477 - distributed.worker - INFO -          dashboard at:          10.201.2.17:39065
2024-04-19 15:40:54,477 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,477 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,477 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,478 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7sda6uju
2024-04-19 15:40:54,478 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4y7_raz3
2024-04-19 15:40:54,478 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,477 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-55dfd38c-0f1d-411e-946e-617fdc31160a
2024-04-19 15:40:54,477 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:41045
2024-04-19 15:40:54,477 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:41045
2024-04-19 15:40:54,477 - distributed.worker - INFO -          dashboard at:          10.201.2.17:46763
2024-04-19 15:40:54,477 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,477 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,478 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,478 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5stijevn
2024-04-19 15:40:54,478 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,666 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9f9624fd-f728-4407-bed4-0cba1c62a5c4
2024-04-19 15:40:54,666 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:32929
2024-04-19 15:40:54,666 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-247a5d51-9529-4871-87db-382200d273a0
2024-04-19 15:40:54,666 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:44095
2024-04-19 15:40:54,666 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-bf82487d-14e2-4d72-9dc2-11d7459cb79c
2024-04-19 15:40:54,666 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:32953
2024-04-19 15:40:54,666 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:32953
2024-04-19 15:40:54,666 - distributed.worker - INFO -          dashboard at:          10.201.2.16:44681
2024-04-19 15:40:54,666 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,666 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,666 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,666 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,666 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-07f7ba88-209c-418e-a79f-345bac0762fe
2024-04-19 15:40:54,666 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:35877
2024-04-19 15:40:54,666 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:35877
2024-04-19 15:40:54,666 - distributed.worker - INFO -          dashboard at:          10.201.2.16:33107
2024-04-19 15:40:54,667 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,666 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:32929
2024-04-19 15:40:54,666 - distributed.worker - INFO -          dashboard at:          10.201.2.16:35069
2024-04-19 15:40:54,666 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,666 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,667 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,667 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hy3phqw2
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,666 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:44095
2024-04-19 15:40:54,666 - distributed.worker - INFO -          dashboard at:          10.201.2.16:34899
2024-04-19 15:40:54,667 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,667 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,667 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqa91r35
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,667 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,667 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzlqq20s
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hjmmo_cn
2024-04-19 15:40:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:56,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:56,951 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:56,952 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:56,952 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:56,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:56,953 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:56,953 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:56,953 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:56,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:56,954 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:56,954 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:56,955 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:56,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:56,955 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:56,955 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:56,956 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:57,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,004 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:57,005 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,005 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:57,005 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:57,005 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,006 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:57,006 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:57,007 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,007 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,007 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:40:57,007 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 15:40:57,007 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,008 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 15:41:00,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:03,759 - distributed.utils_perf - INFO - full garbage collection released 113.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:04,351 - distributed.utils_perf - INFO - full garbage collection released 124.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:05,310 - distributed.utils_perf - INFO - full garbage collection released 534.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:07,097 - distributed.utils_perf - INFO - full garbage collection released 14.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:10,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:11,556 - distributed.utils_perf - INFO - full garbage collection released 107.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:13,720 - distributed.utils_perf - INFO - full garbage collection released 0.99 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:14,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:14,996 - distributed.utils_perf - INFO - full garbage collection released 1.35 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:16,790 - distributed.utils_perf - INFO - full garbage collection released 43.92 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:17,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:17,328 - distributed.utils_perf - INFO - full garbage collection released 113.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:18,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:19,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:20,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:23,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:24,369 - distributed.utils_perf - INFO - full garbage collection released 2.91 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:24,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:24,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:26,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,636 - distributed.utils_perf - INFO - full garbage collection released 61.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:40,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:42,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:43,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,392 - distributed.utils_perf - INFO - full garbage collection released 115.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:53,610 - distributed.utils_perf - INFO - full garbage collection released 192.81 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:58,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:00,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,629 - distributed.utils_perf - INFO - full garbage collection released 72.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:03,351 - distributed.utils_perf - INFO - full garbage collection released 508.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:03,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:06,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:07,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:08,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:11,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:13,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:21,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:27,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:28,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,976 - distributed.utils_perf - INFO - full garbage collection released 1.42 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:30,988 - distributed.utils_perf - INFO - full garbage collection released 577.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:31,725 - distributed.utils_perf - INFO - full garbage collection released 2.14 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:31,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:35,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:35,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:37,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:38,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:38,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:40,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:42,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:48,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:48,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:49,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:55,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:56,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:07,539 - distributed.utils_perf - INFO - full garbage collection released 0.91 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:07,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:12,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:15,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:17,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:18,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:19,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:24,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:28,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:35,919 - distributed.utils_perf - INFO - full garbage collection released 698.73 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:37,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:38,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:39,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:40,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:40,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:43,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,811 - distributed.utils_perf - INFO - full garbage collection released 132.89 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:55,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:55,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:57,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:59,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:04,355 - distributed.utils_perf - INFO - full garbage collection released 88.05 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:04,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:07,903 - distributed.utils_perf - INFO - full garbage collection released 377.57 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:07,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:12,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:13,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:15,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:16,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:21,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:22,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:22,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:28,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:29,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,858 - distributed.utils_perf - INFO - full garbage collection released 239.25 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:35,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:44,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:47,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:50,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:50,875 - distributed.utils_perf - INFO - full garbage collection released 3.49 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:54,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:55,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:57,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:57,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:59,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:00,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:01,941 - distributed.utils_perf - INFO - full garbage collection released 702.75 MiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:02,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:09,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:09,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:14,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:18,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:19,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 66.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:22,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:23,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:30,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:35,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:37,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:37,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:42,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:44,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:45,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:45,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:55,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:59,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:01,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:03,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:04,091 - distributed.utils_perf - INFO - full garbage collection released 244.42 MiB from 169 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:12,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:13,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:14,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:15,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:17,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:20,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:23,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:23,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:24,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:25,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:29,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:29,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:33,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:36,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:36,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:39,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:39,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:41,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:42,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:43,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:44,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:47,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:48,278 - distributed.utils_perf - INFO - full garbage collection released 877.35 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:51,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:55,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:00,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:00,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:04,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:06,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:06,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:07,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:12,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:19,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:19,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,767 - distributed.utils_perf - WARNING - full garbage collections took 37% CPU time recently (threshold: 10%)
2024-04-19 15:47:27,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:30,032 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 15:47:32,822 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 15:47:39,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:39,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:39,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:41,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:41,927 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:47:42,827 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:47:43,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:43,932 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:47:45,310 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:47:46,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:47,057 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:47:47,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:49,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:53,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:56,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:01,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:04,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:07,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:08,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:14,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:15,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:19,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:20,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:21,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:28,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:29,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:30,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:32,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:37,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:37,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:42,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:48,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:51,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:51,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:53,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:54,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:56,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:00,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:03,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:06,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:08,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:10,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:17,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:18,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:19,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:20,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:30,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:32,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:32,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:32,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:39,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:42,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:44,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:47,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:52,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:59,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:01,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:07,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:08,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:11,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:13,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:14,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:15,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:20,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:28,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:29,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:38,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:41,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:47,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:48,393 - distributed.utils_perf - INFO - full garbage collection released 139.95 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:49,780 - distributed.utils_perf - INFO - full garbage collection released 118.00 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:51:09,771 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:51:29,973 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
[15:51:31] task [xgboost.dask-tcp://10.201.2.16:32929]:tcp://10.201.2.16:32929 got new rank 0
[15:51:31] task [xgboost.dask-tcp://10.201.2.16:32953]:tcp://10.201.2.16:32953 got new rank 1
[15:51:31] task [xgboost.dask-tcp://10.201.2.16:35877]:tcp://10.201.2.16:35877 got new rank 2
[15:51:31] task [xgboost.dask-tcp://10.201.2.16:44095]:tcp://10.201.2.16:44095 got new rank 3
[15:51:31] task [xgboost.dask-tcp://10.201.2.17:40881]:tcp://10.201.2.17:40881 got new rank 4
[15:51:31] task [xgboost.dask-tcp://10.201.2.17:41045]:tcp://10.201.2.17:41045 got new rank 5
[15:51:31] task [xgboost.dask-tcp://10.201.2.17:42065]:tcp://10.201.2.17:42065 got new rank 6
[15:51:31] task [xgboost.dask-tcp://10.201.2.17:45811]:tcp://10.201.2.17:45811 got new rank 7
2024-04-19 15:53:53,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:53,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:54,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:11,578 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
[15:54:32] task [xgboost.dask-tcp://10.201.2.16:32929]:tcp://10.201.2.16:32929 got new rank 0
[15:54:32] task [xgboost.dask-tcp://10.201.2.16:32953]:tcp://10.201.2.16:32953 got new rank 1
[15:54:32] task [xgboost.dask-tcp://10.201.2.16:35877]:tcp://10.201.2.16:35877 got new rank 2
[15:54:32] task [xgboost.dask-tcp://10.201.2.16:44095]:tcp://10.201.2.16:44095 got new rank 3
[15:54:32] task [xgboost.dask-tcp://10.201.2.17:40881]:tcp://10.201.2.17:40881 got new rank 4
[15:54:32] task [xgboost.dask-tcp://10.201.2.17:41045]:tcp://10.201.2.17:41045 got new rank 5
[15:54:32] task [xgboost.dask-tcp://10.201.2.17:42065]:tcp://10.201.2.17:42065 got new rank 6
[15:54:32] task [xgboost.dask-tcp://10.201.2.17:45811]:tcp://10.201.2.17:45811 got new rank 7
2024-04-19 15:57:03,146 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
[15:57:27] task [xgboost.dask-tcp://10.201.2.16:32929]:tcp://10.201.2.16:32929 got new rank 0
[15:57:27] task [xgboost.dask-tcp://10.201.2.16:32953]:tcp://10.201.2.16:32953 got new rank 1
[15:57:27] task [xgboost.dask-tcp://10.201.2.16:35877]:tcp://10.201.2.16:35877 got new rank 2
[15:57:27] task [xgboost.dask-tcp://10.201.2.16:44095]:tcp://10.201.2.16:44095 got new rank 3
[15:57:27] task [xgboost.dask-tcp://10.201.2.17:40881]:tcp://10.201.2.17:40881 got new rank 4
[15:57:27] task [xgboost.dask-tcp://10.201.2.17:41045]:tcp://10.201.2.17:41045 got new rank 5
[15:57:27] task [xgboost.dask-tcp://10.201.2.17:42065]:tcp://10.201.2.17:42065 got new rank 6
[15:57:27] task [xgboost.dask-tcp://10.201.2.17:45811]:tcp://10.201.2.17:45811 got new rank 7
2024-04-19 16:00:48,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:01:34] task [xgboost.dask-tcp://10.201.2.16:32929]:tcp://10.201.2.16:32929 got new rank 0
[16:01:34] task [xgboost.dask-tcp://10.201.2.16:32953]:tcp://10.201.2.16:32953 got new rank 1
[16:01:34] task [xgboost.dask-tcp://10.201.2.16:35877]:tcp://10.201.2.16:35877 got new rank 2
[16:01:34] task [xgboost.dask-tcp://10.201.2.16:44095]:tcp://10.201.2.16:44095 got new rank 3
[16:01:34] task [xgboost.dask-tcp://10.201.2.17:40881]:tcp://10.201.2.17:40881 got new rank 4
[16:01:34] task [xgboost.dask-tcp://10.201.2.17:41045]:tcp://10.201.2.17:41045 got new rank 5
[16:01:34] task [xgboost.dask-tcp://10.201.2.17:42065]:tcp://10.201.2.17:42065 got new rank 6
[16:01:34] task [xgboost.dask-tcp://10.201.2.17:45811]:tcp://10.201.2.17:45811 got new rank 7
2024-04-19 16:04:53,435 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
2024-04-19 16:05:01,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:01,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:02,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:14,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:14,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:15,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:15,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:15,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:16,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:16,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:05:16,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:05:59] task [xgboost.dask-tcp://10.201.2.16:32929]:tcp://10.201.2.16:32929 got new rank 0
[16:05:59] task [xgboost.dask-tcp://10.201.2.16:32953]:tcp://10.201.2.16:32953 got new rank 1
[16:05:59] task [xgboost.dask-tcp://10.201.2.16:35877]:tcp://10.201.2.16:35877 got new rank 2
[16:05:59] task [xgboost.dask-tcp://10.201.2.16:44095]:tcp://10.201.2.16:44095 got new rank 3
[16:05:59] task [xgboost.dask-tcp://10.201.2.17:40881]:tcp://10.201.2.17:40881 got new rank 4
[16:05:59] task [xgboost.dask-tcp://10.201.2.17:41045]:tcp://10.201.2.17:41045 got new rank 5
[16:05:59] task [xgboost.dask-tcp://10.201.2.17:42065]:tcp://10.201.2.17:42065 got new rank 6
[16:05:59] task [xgboost.dask-tcp://10.201.2.17:45811]:tcp://10.201.2.17:45811 got new rank 7
2024-04-19 16:08:12,687 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
2024-04-19 16:08:12,687 - distributed.utils_perf - INFO - full garbage collection released 1.77 GiB from 217 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:45811. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:41045. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:32929. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:42065. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:44095. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:40881. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:32953. Reason: scheduler-close
2024-04-19 16:08:39,470 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:35877. Reason: scheduler-close
2024-04-19 16:08:39,473 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:46635'. Reason: scheduler-close
2024-04-19 16:08:39,473 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:38979'. Reason: scheduler-close
2024-04-19 16:08:39,473 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:33895'. Reason: scheduler-close
2024-04-19 16:08:39,473 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:37829'. Reason: scheduler-close
2024-04-19 16:08:39,472 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45146 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45146 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:39,472 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45144 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45144 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:39,472 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45160 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.16:45160 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:39,472 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:46568 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:46568 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:39,480 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:39437'. Reason: scheduler-close
2024-04-19 16:08:39,480 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:39239'. Reason: scheduler-close
2024-04-19 16:08:39,480 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:38359'. Reason: scheduler-close
2024-04-19 16:08:39,481 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:34989'. Reason: scheduler-close
2024-04-19 16:08:40,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:08:40,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:59512 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:40,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:59514 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:40,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:59530 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.16:59516 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:40,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:08:40,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:08:40,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.17:52180 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:40,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.17:43432 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.17:43116 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.17:52178 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 16:08:40,335 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,336 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,356 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,357 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,359 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,359 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,361 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,362 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,388 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,389 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,417 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,417 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,424 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,425 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:40,452 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 16:08:40,452 - distributed.nanny - INFO - Worker closed
2024-04-19 16:08:42,338 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,361 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,365 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,373 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,390 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,419 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,427 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:42,454 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:08:43,591 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:39239'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,591 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:43,600 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:38359'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,601 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:43,608 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:39437'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,609 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:43,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:46635'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,718 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:43,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:38979'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,760 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:43,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:34989'. Reason: nanny-close-gracefully
2024-04-19 16:08:43,831 - distributed.dask_worker - INFO - End worker
2024-04-19 16:08:44,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:37829'. Reason: nanny-close-gracefully
2024-04-19 16:08:44,075 - distributed.dask_worker - INFO - End worker
