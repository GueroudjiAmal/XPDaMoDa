2024-04-19 17:15:31,444 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,444 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,444 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,444 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,496 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,496 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,496 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,496 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,698 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,698 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,698 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,698 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,732 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,732 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,732 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,733 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.87:45449'
2024-04-19 17:15:31,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.87:38421'
2024-04-19 17:15:31,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.87:45569'
2024-04-19 17:15:31,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.87:40627'
2024-04-19 17:15:31,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.122:39159'
2024-04-19 17:15:31,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.122:40369'
2024-04-19 17:15:31,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.122:36209'
2024-04-19 17:15:31,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.122:42891'
2024-04-19 17:15:32,705 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,706 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,707 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,707 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,707 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,708 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,708 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,708 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,717 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,718 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,718 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,718 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,719 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,719 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,719 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,719 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,750 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,751 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,752 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,752 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,763 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,764 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,764 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,764 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:33,698 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,698 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,698 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,699 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,720 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,720 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,720 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,720 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:34,747 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4cf3d467-d450-4971-bed9-52c942c621a7
2024-04-19 17:15:34,747 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8a754144-6ab3-44fe-8f14-efab1b23bf51
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.87:32819
2024-04-19 17:15:34,747 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.87:32819
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.87:33955
2024-04-19 17:15:34,747 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.87:33955
2024-04-19 17:15:34,747 - distributed.worker - INFO -          dashboard at:          10.201.3.87:44323
2024-04-19 17:15:34,747 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,747 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,747 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-063910b6-acc9-473f-b469-e99ef333e4f7
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.87:36713
2024-04-19 17:15:34,747 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.87:36713
2024-04-19 17:15:34,747 - distributed.worker - INFO -          dashboard at:          10.201.3.87:38733
2024-04-19 17:15:34,747 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,747 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b9fc62c0-50fb-4eee-8b68-5462ab385011
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.87:44003
2024-04-19 17:15:34,747 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.87:44003
2024-04-19 17:15:34,747 - distributed.worker - INFO -          dashboard at:          10.201.3.87:46163
2024-04-19 17:15:34,747 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,747 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bk871bku
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -          dashboard at:          10.201.3.87:37441
2024-04-19 17:15:34,747 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,747 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-exj9iu_s
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hm0u5j6q
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,747 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e1hki_2f
2024-04-19 17:15:34,747 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c579e37b-c6ed-40fa-9023-f960ea45cf0b
2024-04-19 17:15:34,788 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-83a7400b-608f-494e-a7ae-a4cb32aa83c3
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.122:39813
2024-04-19 17:15:34,788 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c7c87a4b-24cf-44ad-8feb-497fb460f7fd
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.122:39659
2024-04-19 17:15:34,788 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.122:39659
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.122:35627
2024-04-19 17:15:34,788 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.122:35627
2024-04-19 17:15:34,788 - distributed.worker - INFO -          dashboard at:         10.201.2.122:37753
2024-04-19 17:15:34,788 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,788 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-11f11dd5-3578-4cb6-8252-637b002e012e
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.122:33757
2024-04-19 17:15:34,788 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.122:33757
2024-04-19 17:15:34,788 - distributed.worker - INFO -          dashboard at:         10.201.2.122:36159
2024-04-19 17:15:34,788 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,788 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,788 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.122:39813
2024-04-19 17:15:34,788 - distributed.worker - INFO -          dashboard at:         10.201.2.122:46573
2024-04-19 17:15:34,788 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,788 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fvxo80sc
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -          dashboard at:         10.201.2.122:43941
2024-04-19 17:15:34,788 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.104:8786
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,788 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d8vlqmhi
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s9sgp31i
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3o036xg
2024-04-19 17:15:34,788 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,502 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,502 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,503 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,504 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,505 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,505 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,508 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,508 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,508 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,509 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,509 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,510 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,511 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,511 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,511 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,512 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,512 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,512 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,512 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,512 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,513 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:38,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,514 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.104:8786
2024-04-19 17:15:38,514 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,514 - distributed.core - INFO - Starting established connection to tcp://10.201.2.104:8786
2024-04-19 17:15:47,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:54,491 - distributed.utils_perf - INFO - full garbage collection released 1.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:55,426 - distributed.utils_perf - INFO - full garbage collection released 142.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:56,450 - distributed.utils_perf - INFO - full garbage collection released 418.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:59,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:01,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:01,423 - distributed.utils_perf - INFO - full garbage collection released 355.15 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:01,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:03,025 - distributed.utils_perf - INFO - full garbage collection released 126.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:03,682 - distributed.utils_perf - INFO - full garbage collection released 9.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:04,975 - distributed.utils_perf - INFO - full garbage collection released 4.10 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:05,823 - distributed.utils_perf - INFO - full garbage collection released 113.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:05,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:06,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:06,405 - distributed.utils_perf - INFO - full garbage collection released 585.05 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:07,534 - distributed.utils_perf - INFO - full garbage collection released 466.91 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:08,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:09,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:15,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:17,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:21,233 - distributed.utils_perf - INFO - full garbage collection released 252.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:22,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:22,793 - distributed.utils_perf - INFO - full garbage collection released 0.95 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:25,505 - distributed.utils_perf - INFO - full garbage collection released 328.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:25,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,339 - distributed.utils_perf - INFO - full garbage collection released 204.87 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:32,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:33,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:37,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,866 - distributed.utils_perf - INFO - full garbage collection released 406.78 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,322 - distributed.utils_perf - INFO - full garbage collection released 52.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:04,363 - distributed.utils_perf - INFO - full garbage collection released 118.81 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:04,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,279 - distributed.utils_perf - INFO - full garbage collection released 294.85 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:08,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,883 - distributed.utils_perf - INFO - full garbage collection released 79.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:17,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,864 - distributed.utils_perf - INFO - full garbage collection released 183.16 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:22,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,326 - distributed.utils_perf - INFO - full garbage collection released 48.10 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:31,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:41,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:47,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:32,258 - distributed.utils_perf - INFO - full garbage collection released 121.08 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:32,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:32,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:52,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:54,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,303 - distributed.utils_perf - INFO - full garbage collection released 5.13 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:07,425 - distributed.utils_perf - INFO - full garbage collection released 672.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:10,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,208 - distributed.utils_perf - INFO - full garbage collection released 1.01 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:11,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,338 - distributed.utils_perf - INFO - full garbage collection released 30.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:30,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:43,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:56,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:56,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,781 - distributed.utils_perf - INFO - full garbage collection released 5.19 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:10,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,348 - distributed.utils_perf - INFO - full garbage collection released 170.43 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:13,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:37,552 - distributed.utils_perf - INFO - full garbage collection released 358.09 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:40,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:50,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:59,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:03,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:08,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:14,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:27,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:48,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:53,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:53,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:00,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:02,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:18,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,818 - distributed.utils_perf - INFO - full garbage collection released 38.58 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:27,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:37,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:37,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,298 - distributed.utils_perf - INFO - full garbage collection released 126.26 MiB from 132 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:55,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,715 - distributed.utils_perf - INFO - full garbage collection released 22.75 MiB from 304 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:01,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,352 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:14,194 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:15,228 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:15,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,505 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:17,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,136 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:18,718 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:18,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,465 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:19,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,131 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:20,372 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:21,495 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:22,613 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:22,892 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:22,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,648 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:25,667 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:26,797 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:28,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,448 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:32,723 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:34,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:59,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:13,359 - distributed.utils_perf - INFO - full garbage collection released 25.72 MiB from 133 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:13,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:14,920 - distributed.utils_perf - INFO - full garbage collection released 11.83 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:15,527 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,073 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,746 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:17,566 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:17,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,571 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:19,792 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:21,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,336 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:21,795 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:22,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,945 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:23,233 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:23,238 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:23,711 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:24,289 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:24,393 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:24,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,981 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:25,579 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:25,837 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:26,202 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:26,899 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:28,207 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:28,446 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:28,483 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:29,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,866 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:30,966 - distributed.utils_perf - INFO - full garbage collection released 524.32 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:31,903 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:35,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:59,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:25:31] task [xgboost.dask-tcp://10.201.2.122:33757]:tcp://10.201.2.122:33757 got new rank 0
[17:25:31] task [xgboost.dask-tcp://10.201.2.122:35627]:tcp://10.201.2.122:35627 got new rank 1
[17:25:31] task [xgboost.dask-tcp://10.201.2.122:39659]:tcp://10.201.2.122:39659 got new rank 2
[17:25:31] task [xgboost.dask-tcp://10.201.2.122:39813]:tcp://10.201.2.122:39813 got new rank 3
[17:25:31] task [xgboost.dask-tcp://10.201.3.87:32819]:tcp://10.201.3.87:32819 got new rank 4
[17:25:31] task [xgboost.dask-tcp://10.201.3.87:33955]:tcp://10.201.3.87:33955 got new rank 5
[17:25:31] task [xgboost.dask-tcp://10.201.3.87:36713]:tcp://10.201.3.87:36713 got new rank 6
[17:25:31] task [xgboost.dask-tcp://10.201.3.87:44003]:tcp://10.201.3.87:44003 got new rank 7
2024-04-19 17:28:00,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:42] task [xgboost.dask-tcp://10.201.2.122:33757]:tcp://10.201.2.122:33757 got new rank 0
[17:28:42] task [xgboost.dask-tcp://10.201.2.122:35627]:tcp://10.201.2.122:35627 got new rank 1
[17:28:42] task [xgboost.dask-tcp://10.201.2.122:39659]:tcp://10.201.2.122:39659 got new rank 2
[17:28:42] task [xgboost.dask-tcp://10.201.2.122:39813]:tcp://10.201.2.122:39813 got new rank 3
[17:28:42] task [xgboost.dask-tcp://10.201.3.87:32819]:tcp://10.201.3.87:32819 got new rank 4
[17:28:42] task [xgboost.dask-tcp://10.201.3.87:33955]:tcp://10.201.3.87:33955 got new rank 5
[17:28:42] task [xgboost.dask-tcp://10.201.3.87:36713]:tcp://10.201.3.87:36713 got new rank 6
[17:28:42] task [xgboost.dask-tcp://10.201.3.87:44003]:tcp://10.201.3.87:44003 got new rank 7
2024-04-19 17:31:04,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:04,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:04,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:43] task [xgboost.dask-tcp://10.201.2.122:33757]:tcp://10.201.2.122:33757 got new rank 0
[17:31:43] task [xgboost.dask-tcp://10.201.2.122:35627]:tcp://10.201.2.122:35627 got new rank 1
[17:31:43] task [xgboost.dask-tcp://10.201.2.122:39659]:tcp://10.201.2.122:39659 got new rank 2
[17:31:43] task [xgboost.dask-tcp://10.201.2.122:39813]:tcp://10.201.2.122:39813 got new rank 3
[17:31:43] task [xgboost.dask-tcp://10.201.3.87:32819]:tcp://10.201.3.87:32819 got new rank 4
[17:31:43] task [xgboost.dask-tcp://10.201.3.87:33955]:tcp://10.201.3.87:33955 got new rank 5
[17:31:43] task [xgboost.dask-tcp://10.201.3.87:36713]:tcp://10.201.3.87:36713 got new rank 6
[17:31:43] task [xgboost.dask-tcp://10.201.3.87:44003]:tcp://10.201.3.87:44003 got new rank 7
2024-04-19 17:34:08,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:34:46] task [xgboost.dask-tcp://10.201.2.122:33757]:tcp://10.201.2.122:33757 got new rank 0
[17:34:46] task [xgboost.dask-tcp://10.201.2.122:35627]:tcp://10.201.2.122:35627 got new rank 1
[17:34:46] task [xgboost.dask-tcp://10.201.2.122:39659]:tcp://10.201.2.122:39659 got new rank 2
[17:34:46] task [xgboost.dask-tcp://10.201.2.122:39813]:tcp://10.201.2.122:39813 got new rank 3
[17:34:46] task [xgboost.dask-tcp://10.201.3.87:32819]:tcp://10.201.3.87:32819 got new rank 4
[17:34:46] task [xgboost.dask-tcp://10.201.3.87:33955]:tcp://10.201.3.87:33955 got new rank 5
[17:34:46] task [xgboost.dask-tcp://10.201.3.87:36713]:tcp://10.201.3.87:36713 got new rank 6
[17:34:46] task [xgboost.dask-tcp://10.201.3.87:44003]:tcp://10.201.3.87:44003 got new rank 7
2024-04-19 17:36:55,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:55,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:55,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:55,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:09,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:11,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:12,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:12,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:37:53] task [xgboost.dask-tcp://10.201.2.122:33757]:tcp://10.201.2.122:33757 got new rank 0
[17:37:53] task [xgboost.dask-tcp://10.201.2.122:35627]:tcp://10.201.2.122:35627 got new rank 1
[17:37:53] task [xgboost.dask-tcp://10.201.2.122:39659]:tcp://10.201.2.122:39659 got new rank 2
[17:37:53] task [xgboost.dask-tcp://10.201.2.122:39813]:tcp://10.201.2.122:39813 got new rank 3
[17:37:53] task [xgboost.dask-tcp://10.201.3.87:32819]:tcp://10.201.3.87:32819 got new rank 4
[17:37:53] task [xgboost.dask-tcp://10.201.3.87:33955]:tcp://10.201.3.87:33955 got new rank 5
[17:37:53] task [xgboost.dask-tcp://10.201.3.87:36713]:tcp://10.201.3.87:36713 got new rank 6
[17:37:53] task [xgboost.dask-tcp://10.201.3.87:44003]:tcp://10.201.3.87:44003 got new rank 7
2024-04-19 17:39:49,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.87:33955. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.87:32819. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.87:36713. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.87:44003. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.122:39659. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.122:35627. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.122:33757. Reason: scheduler-close
2024-04-19 17:40:22,971 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.122:39813. Reason: scheduler-close
2024-04-19 17:40:22,973 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.87:45449'. Reason: scheduler-close
2024-04-19 17:40:22,973 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.87:45569'. Reason: scheduler-close
2024-04-19 17:40:22,974 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.122:36209'. Reason: scheduler-close
2024-04-19 17:40:22,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.87:55756 remote=tcp://10.201.2.104:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.87:55756 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:22,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.87:55762 remote=tcp://10.201.2.104:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.87:55762 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:22,980 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.87:38421'. Reason: scheduler-close
2024-04-19 17:40:22,981 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.87:40627'. Reason: scheduler-close
2024-04-19 17:40:22,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45420 remote=tcp://10.201.2.104:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45420 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:22,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45406 remote=tcp://10.201.2.104:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45406 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:22,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45410 remote=tcp://10.201.2.104:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.122:45410 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:22,985 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.122:39159'. Reason: scheduler-close
2024-04-19 17:40:22,985 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.122:42891'. Reason: scheduler-close
2024-04-19 17:40:22,985 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.122:40369'. Reason: scheduler-close
2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.122:47866 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.122:47868 remote=tcp://10.201.2.104:8786>: Stream is closed
/10.201.2.122:36852 remote=tcp://10.201.2.104:8786>: Stream is closed
/10.201.2.122:36882 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:23,770 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:23,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.87:55812 remote=tcp://10.201.2.104:8786>: Stream is closed
/10.201.3.87:55800 remote=tcp://10.201.2.104:8786>: Stream is closed
/10.201.3.87:55808 remote=tcp://10.201.2.104:8786>: Stream is closed
/10.201.3.87:50964 remote=tcp://10.201.2.104:8786>: Stream is closed
2024-04-19 17:40:23,864 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,865 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,866 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,866 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,874 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,874 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,876 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,877 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,879 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,879 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,879 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,880 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,883 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,884 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:23,892 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.104:8786; closing.
2024-04-19 17:40:23,892 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:25,867 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,869 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,876 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,880 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,883 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:25,885 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:27,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.122:39159'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,071 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.122:40369'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,077 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.122:42891'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,183 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.87:38421'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,365 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.87:40627'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,473 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.87:45449'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,479 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.122:36209'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,552 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:27,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.87:45569'. Reason: nanny-close-gracefully
2024-04-19 17:40:27,827 - distributed.dask_worker - INFO - End worker
