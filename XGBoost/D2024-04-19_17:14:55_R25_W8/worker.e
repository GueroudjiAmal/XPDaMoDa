2024-04-19 17:15:59,569 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,569 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,569 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,569 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,577 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,578 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,579 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,580 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,861 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,861 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,862 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,861 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,862 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,862 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,862 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,862 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.115:34207'
2024-04-19 17:15:59,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.115:34351'
2024-04-19 17:15:59,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.115:35735'
2024-04-19 17:15:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.115:35805'
2024-04-19 17:15:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.114:32991'
2024-04-19 17:15:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.114:33449'
2024-04-19 17:15:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.114:40217'
2024-04-19 17:15:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.114:35021'
2024-04-19 17:16:00,929 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,929 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,929 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,929 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,930 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,930 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,930 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,930 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,939 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,939 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,939 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,939 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,939 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,940 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,940 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,940 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,974 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,975 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,975 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,975 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,985 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,992 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,992 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,992 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,992 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,004 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,004 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,004 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,004 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:03,080 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-18fcf520-dbb8-4326-99b7-6b6f25d2d7ff
2024-04-19 17:16:03,081 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.115:33323
2024-04-19 17:16:03,081 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.115:33323
2024-04-19 17:16:03,081 - distributed.worker - INFO -          dashboard at:         10.201.2.115:46177
2024-04-19 17:16:03,081 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,081 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,081 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,081 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,081 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r1c3dzkx
2024-04-19 17:16:03,081 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,082 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a390abf2-3355-43da-b829-c977703f53a1
2024-04-19 17:16:03,083 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.114:44279
2024-04-19 17:16:03,083 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.114:44279
2024-04-19 17:16:03,083 - distributed.worker - INFO -          dashboard at:         10.201.2.114:43303
2024-04-19 17:16:03,083 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,083 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,083 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,083 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dekz7lfo
2024-04-19 17:16:03,083 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-80e00add-e56a-493b-9190-d2be3003500f
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.115:43367
2024-04-19 17:16:03,084 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.115:43367
2024-04-19 17:16:03,084 - distributed.worker - INFO -          dashboard at:         10.201.2.115:37105
2024-04-19 17:16:03,084 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,084 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qzo31dfh
2024-04-19 17:16:03,085 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,086 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-851d4eb5-0a1d-470d-86e9-73278f735bf8
2024-04-19 17:16:03,086 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.114:33675
2024-04-19 17:16:03,086 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.114:33675
2024-04-19 17:16:03,086 - distributed.worker - INFO -          dashboard at:         10.201.2.114:44285
2024-04-19 17:16:03,086 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,086 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,086 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,086 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gjna5hj9
2024-04-19 17:16:03,086 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,087 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-498bd886-8660-4560-9d29-d4b81e6f0b80
2024-04-19 17:16:03,087 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.115:38827
2024-04-19 17:16:03,088 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.115:38827
2024-04-19 17:16:03,088 - distributed.worker - INFO -          dashboard at:         10.201.2.115:35189
2024-04-19 17:16:03,088 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,088 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,088 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,088 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,088 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r9_xfn4j
2024-04-19 17:16:03,088 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,090 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-efa392a7-ca52-4490-9b5a-b7d3b055927f
2024-04-19 17:16:03,090 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.114:37649
2024-04-19 17:16:03,090 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.114:37649
2024-04-19 17:16:03,090 - distributed.worker - INFO -          dashboard at:         10.201.2.114:36429
2024-04-19 17:16:03,090 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,090 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,090 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,090 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lbqg4rzu
2024-04-19 17:16:03,090 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,091 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9b9d5b54-0720-48af-9fb8-24ae027bfdc8
2024-04-19 17:16:03,091 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.115:41037
2024-04-19 17:16:03,091 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.115:41037
2024-04-19 17:16:03,091 - distributed.worker - INFO -          dashboard at:         10.201.2.115:36151
2024-04-19 17:16:03,092 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,092 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,092 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,092 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,092 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-coh_oegj
2024-04-19 17:16:03,092 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,093 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-68501007-4dd0-49b4-b05d-4862b737f042
2024-04-19 17:16:03,093 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.114:35193
2024-04-19 17:16:03,093 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.114:35193
2024-04-19 17:16:03,093 - distributed.worker - INFO -          dashboard at:         10.201.2.114:33879
2024-04-19 17:16:03,093 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.98:8786
2024-04-19 17:16:03,093 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,093 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,093 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8o3ezy1w
2024-04-19 17:16:03,094 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,900 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,900 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,901 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,902 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,902 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,902 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,905 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,905 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,906 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,906 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,906 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,907 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,908 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,908 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,908 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,909 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,909 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,910 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,910 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,910 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,911 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:06,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,912 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.98:8786
2024-04-19 17:16:06,912 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,912 - distributed.core - INFO - Starting established connection to tcp://10.201.2.98:8786
2024-04-19 17:16:16,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:20,215 - distributed.utils_perf - INFO - full garbage collection released 462.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:22,398 - distributed.utils_perf - INFO - full garbage collection released 129.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,578 - distributed.utils_perf - INFO - full garbage collection released 349.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,764 - distributed.utils_perf - INFO - full garbage collection released 210.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,024 - distributed.utils_perf - INFO - full garbage collection released 455.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,342 - distributed.utils_perf - INFO - full garbage collection released 167.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,590 - distributed.utils_perf - INFO - full garbage collection released 189.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:27,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,070 - distributed.utils_perf - INFO - full garbage collection released 3.34 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:33,094 - distributed.utils_perf - INFO - full garbage collection released 7.06 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:34,671 - distributed.utils_perf - INFO - full garbage collection released 287.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:35,954 - distributed.utils_perf - INFO - full garbage collection released 280.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:45,375 - distributed.utils_perf - INFO - full garbage collection released 86.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,993 - distributed.utils_perf - INFO - full garbage collection released 207.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,388 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:52,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,605 - distributed.utils_perf - INFO - full garbage collection released 63.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:54,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,926 - distributed.utils_perf - INFO - full garbage collection released 264.03 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:56,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:56,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:16,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:17,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,360 - distributed.utils_perf - INFO - full garbage collection released 292.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:21,243 - distributed.utils_perf - INFO - full garbage collection released 2.20 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:21,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,869 - distributed.utils_perf - INFO - full garbage collection released 224.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:25,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:26,888 - distributed.utils_perf - INFO - full garbage collection released 170.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:26,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:58,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:20,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:21,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:32,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:42,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:08,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:13,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:19,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:23,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:31,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,551 - distributed.utils_perf - INFO - full garbage collection released 271.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:38,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:39,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,784 - distributed.utils_perf - INFO - full garbage collection released 487.45 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:42,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:42,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,441 - distributed.utils_perf - INFO - full garbage collection released 247.83 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:53,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:58,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:58,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,568 - distributed.utils_perf - INFO - full garbage collection released 33.96 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:23,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:39,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,480 - distributed.utils_perf - INFO - full garbage collection released 3.79 GiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:55,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,946 - distributed.utils_perf - INFO - full garbage collection released 383.11 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:57,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,034 - distributed.utils_perf - INFO - full garbage collection released 378.37 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:06,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:08,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:11,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:27,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:31,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:31,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:43,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:50,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:53,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:06,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:51,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:41,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,844 - distributed.utils_perf - INFO - full garbage collection released 199.95 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:45,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:14,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:17,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:31,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:33,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,714 - distributed.utils_perf - INFO - full garbage collection released 256.49 MiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:36,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,945 - distributed.utils_perf - INFO - full garbage collection released 59.70 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:40,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:42,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:42,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:45,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,527 - distributed.utils_perf - INFO - full garbage collection released 16.94 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:50,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:54,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,044 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:57,202 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:57,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:59,852 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:25:01,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:03,155 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 17:25:03,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:11,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:22,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:25:53] task [xgboost.dask-tcp://10.201.2.114:33675]:tcp://10.201.2.114:33675 got new rank 0
[17:25:53] task [xgboost.dask-tcp://10.201.2.114:35193]:tcp://10.201.2.114:35193 got new rank 1
[17:25:53] task [xgboost.dask-tcp://10.201.2.114:37649]:tcp://10.201.2.114:37649 got new rank 2
[17:25:53] task [xgboost.dask-tcp://10.201.2.114:44279]:tcp://10.201.2.114:44279 got new rank 3
[17:25:53] task [xgboost.dask-tcp://10.201.2.115:33323]:tcp://10.201.2.115:33323 got new rank 4
[17:25:53] task [xgboost.dask-tcp://10.201.2.115:38827]:tcp://10.201.2.115:38827 got new rank 5
[17:25:53] task [xgboost.dask-tcp://10.201.2.115:41037]:tcp://10.201.2.115:41037 got new rank 6
[17:25:53] task [xgboost.dask-tcp://10.201.2.115:43367]:tcp://10.201.2.115:43367 got new rank 7
2024-04-19 17:28:09,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:53] task [xgboost.dask-tcp://10.201.2.114:33675]:tcp://10.201.2.114:33675 got new rank 0
[17:28:53] task [xgboost.dask-tcp://10.201.2.114:35193]:tcp://10.201.2.114:35193 got new rank 1
[17:28:53] task [xgboost.dask-tcp://10.201.2.114:37649]:tcp://10.201.2.114:37649 got new rank 2
[17:28:53] task [xgboost.dask-tcp://10.201.2.114:44279]:tcp://10.201.2.114:44279 got new rank 3
[17:28:53] task [xgboost.dask-tcp://10.201.2.115:33323]:tcp://10.201.2.115:33323 got new rank 4
[17:28:54] task [xgboost.dask-tcp://10.201.2.115:38827]:tcp://10.201.2.115:38827 got new rank 5
[17:28:54] task [xgboost.dask-tcp://10.201.2.115:41037]:tcp://10.201.2.115:41037 got new rank 6
[17:28:54] task [xgboost.dask-tcp://10.201.2.115:43367]:tcp://10.201.2.115:43367 got new rank 7
[17:31:59] task [xgboost.dask-tcp://10.201.2.114:33675]:tcp://10.201.2.114:33675 got new rank 0
[17:31:59] task [xgboost.dask-tcp://10.201.2.114:35193]:tcp://10.201.2.114:35193 got new rank 1
[17:31:59] task [xgboost.dask-tcp://10.201.2.114:37649]:tcp://10.201.2.114:37649 got new rank 2
[17:31:59] task [xgboost.dask-tcp://10.201.2.114:44279]:tcp://10.201.2.114:44279 got new rank 3
[17:31:59] task [xgboost.dask-tcp://10.201.2.115:33323]:tcp://10.201.2.115:33323 got new rank 4
[17:31:59] task [xgboost.dask-tcp://10.201.2.115:38827]:tcp://10.201.2.115:38827 got new rank 5
[17:31:59] task [xgboost.dask-tcp://10.201.2.115:41037]:tcp://10.201.2.115:41037 got new rank 6
[17:31:59] task [xgboost.dask-tcp://10.201.2.115:43367]:tcp://10.201.2.115:43367 got new rank 7
2024-04-19 17:34:33,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:34,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:34,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:14] task [xgboost.dask-tcp://10.201.2.114:33675]:tcp://10.201.2.114:33675 got new rank 0
[17:35:14] task [xgboost.dask-tcp://10.201.2.114:35193]:tcp://10.201.2.114:35193 got new rank 1
[17:35:14] task [xgboost.dask-tcp://10.201.2.114:37649]:tcp://10.201.2.114:37649 got new rank 2
[17:35:14] task [xgboost.dask-tcp://10.201.2.114:44279]:tcp://10.201.2.114:44279 got new rank 3
[17:35:14] task [xgboost.dask-tcp://10.201.2.115:33323]:tcp://10.201.2.115:33323 got new rank 4
[17:35:14] task [xgboost.dask-tcp://10.201.2.115:38827]:tcp://10.201.2.115:38827 got new rank 5
[17:35:14] task [xgboost.dask-tcp://10.201.2.115:41037]:tcp://10.201.2.115:41037 got new rank 6
[17:35:14] task [xgboost.dask-tcp://10.201.2.115:43367]:tcp://10.201.2.115:43367 got new rank 7
2024-04-19 17:37:26,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:27,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:40,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:40,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:41,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:41,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:42,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:42,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:42,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:38:27] task [xgboost.dask-tcp://10.201.2.114:33675]:tcp://10.201.2.114:33675 got new rank 0
[17:38:27] task [xgboost.dask-tcp://10.201.2.114:35193]:tcp://10.201.2.114:35193 got new rank 1
[17:38:27] task [xgboost.dask-tcp://10.201.2.114:37649]:tcp://10.201.2.114:37649 got new rank 2
[17:38:27] task [xgboost.dask-tcp://10.201.2.114:44279]:tcp://10.201.2.114:44279 got new rank 3
[17:38:27] task [xgboost.dask-tcp://10.201.2.115:33323]:tcp://10.201.2.115:33323 got new rank 4
[17:38:27] task [xgboost.dask-tcp://10.201.2.115:38827]:tcp://10.201.2.115:38827 got new rank 5
[17:38:27] task [xgboost.dask-tcp://10.201.2.115:41037]:tcp://10.201.2.115:41037 got new rank 6
[17:38:27] task [xgboost.dask-tcp://10.201.2.115:43367]:tcp://10.201.2.115:43367 got new rank 7
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.114:44279. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.115:38827. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.114:37649. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.114:35193. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.115:33323. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.115:43367. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.114:33675. Reason: scheduler-close
2024-04-19 17:41:26,236 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.115:41037. Reason: scheduler-close
2024-04-19 17:41:26,239 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.114:33449'. Reason: scheduler-close
2024-04-19 17:41:26,239 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.114:32991'. Reason: scheduler-close
2024-04-19 17:41:26,239 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.114:40217'. Reason: scheduler-close
2024-04-19 17:41:26,239 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.114:35021'. Reason: scheduler-close
2024-04-19 17:41:26,238 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58686 remote=tcp://10.201.2.98:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58686 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:26,240 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.115:35735'. Reason: scheduler-close
2024-04-19 17:41:26,238 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58718 remote=tcp://10.201.2.98:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58718 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:26,238 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58724 remote=tcp://10.201.2.98:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58724 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:26,238 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58702 remote=tcp://10.201.2.98:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.115:58702 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:26,245 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.115:34207'. Reason: scheduler-close
2024-04-19 17:41:26,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.115:34351'. Reason: scheduler-close
2024-04-19 17:41:26,246 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.115:35805'. Reason: scheduler-close
2024-04-19 17:41:27,034 - distributed.comm.tcp - INFO - Connection from tcp://10.201.2.114:58316 closed before handshake completed
2024-04-19 17:41:27,033 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.115:35328 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:27,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:27,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.115:35362 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:27,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.115:35346 remote=tcp://10.201.2.98:8786>: Stream is closed
/10.201.2.115:35344 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:27,033 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:27,035 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.114:52278 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:27,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:27,034 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.114:52250 remote=tcp://10.201.2.98:8786>: Stream is closed
/10.201.2.114:47962 remote=tcp://10.201.2.98:8786>: Stream is closed
/10.201.2.114:52292 remote=tcp://10.201.2.98:8786>: Stream is closed
2024-04-19 17:41:27,087 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,087 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,106 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,106 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,111 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,111 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,114 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,115 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,122 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,122 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,140 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,140 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,171 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,172 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:27,189 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.98:8786; closing.
2024-04-19 17:41:27,190 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:29,090 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,109 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,112 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,116 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,124 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,141 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:29,191 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:30,323 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.114:33449'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,324 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.114:35021'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,332 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.115:34351'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,374 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.115:35735'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,382 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.115:35805'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,421 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.114:40217'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,438 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,788 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.114:32991'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,789 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:30,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.115:34207'. Reason: nanny-close-gracefully
2024-04-19 17:41:30,833 - distributed.dask_worker - INFO - End worker
