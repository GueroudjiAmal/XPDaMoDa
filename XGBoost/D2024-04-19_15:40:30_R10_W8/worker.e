2024-04-19 15:41:05,712 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,712 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,712 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,712 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,979 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,980 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,980 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,980 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,109 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,109 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,109 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,109 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:36313'
2024-04-19 15:41:06,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:46137'
2024-04-19 15:41:06,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:38353'
2024-04-19 15:41:06,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:46559'
2024-04-19 15:41:06,288 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,288 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,288 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,288 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:32791'
2024-04-19 15:41:06,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:33333'
2024-04-19 15:41:06,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:41245'
2024-04-19 15:41:06,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:43769'
2024-04-19 15:41:07,109 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,109 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,110 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,110 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,111 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,111 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,112 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,112 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,155 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,155 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,156 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,156 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,278 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,278 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,278 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,278 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:07,279 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,279 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,279 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,279 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:07,325 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,325 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,325 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,326 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:08,199 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,199 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,199 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,199 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,303 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,303 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,303 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,303 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
2024-04-19 15:41:09,134 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-w00hzlzu/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-w00hzlzu/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 15:41:09,134 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-305sx063/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-305sx063/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 15:41:09,139 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:39333
2024-04-19 15:41:09,139 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:34861
2024-04-19 15:41:09,139 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:34861
2024-04-19 15:41:09,139 - distributed.worker - INFO -          dashboard at:          10.201.1.75:34011
2024-04-19 15:41:09,139 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,139 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,134 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-jzc14phd/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-jzc14phd/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 15:41:09,139 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:38113
2024-04-19 15:41:09,139 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:38113
2024-04-19 15:41:09,140 - distributed.worker - INFO -          dashboard at:          10.201.1.75:39301
2024-04-19 15:41:09,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,139 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:39333
2024-04-19 15:41:09,139 - distributed.worker - INFO -          dashboard at:          10.201.1.75:39019
2024-04-19 15:41:09,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,140 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-305sx063
2024-04-19 15:41:09,134 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-zlorly58/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-zlorly58/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 15:41:09,140 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:46593
2024-04-19 15:41:09,140 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:46593
2024-04-19 15:41:09,140 - distributed.worker - INFO -          dashboard at:          10.201.1.75:45229
2024-04-19 15:41:09,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,140 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,140 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w00hzlzu
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,140 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jzc14phd
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,140 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,140 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zlorly58
2024-04-19 15:41:09,140 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,396 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-54fc6e08-1b53-42e2-82ca-07d5fb227126
2024-04-19 15:41:09,397 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-36ed496c-28d2-4eb8-ae05-dd6e586b3706
2024-04-19 15:41:09,396 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:37669
2024-04-19 15:41:09,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:37669
2024-04-19 15:41:09,397 - distributed.worker - INFO -          dashboard at:         10.201.1.183:45705
2024-04-19 15:41:09,396 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-75d0f7f0-d494-4c52-822a-64400d34b2fe
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:40005
2024-04-19 15:41:09,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:40005
2024-04-19 15:41:09,397 - distributed.worker - INFO -          dashboard at:         10.201.1.183:40677
2024-04-19 15:41:09,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,397 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-348b63c7-3475-4dc2-8519-304266432e62
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:40547
2024-04-19 15:41:09,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:40547
2024-04-19 15:41:09,397 - distributed.worker - INFO -          dashboard at:         10.201.1.183:46289
2024-04-19 15:41:09,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:39331
2024-04-19 15:41:09,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:39331
2024-04-19 15:41:09,397 - distributed.worker - INFO -          dashboard at:         10.201.1.183:33181
2024-04-19 15:41:09,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1j78thhn
2024-04-19 15:41:09,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c3mnvelx
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:09,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m855v74k
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hns0_qep
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:09,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,030 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,030 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,031 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,032 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,032 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,033 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,037 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,037 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,037 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,037 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,037 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,038 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,039 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,039 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,040 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,040 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,040 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,041 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,041 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,041 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:13,042 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:13,042 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 15:41:13,042 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:13,042 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 15:41:21,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:22,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:24,958 - distributed.utils_perf - INFO - full garbage collection released 52.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:25,735 - distributed.utils_perf - INFO - full garbage collection released 0.91 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:26,085 - distributed.utils_perf - INFO - full garbage collection released 833.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:26,528 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,141 - distributed.utils_perf - INFO - full garbage collection released 918.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,078 - distributed.utils_perf - INFO - full garbage collection released 867.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,441 - distributed.utils_perf - INFO - full garbage collection released 486.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:30,740 - distributed.utils_perf - INFO - full garbage collection released 812.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:31,278 - distributed.utils_perf - INFO - full garbage collection released 189.65 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:32,536 - distributed.utils_perf - INFO - full garbage collection released 351.42 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:33,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,485 - distributed.utils_perf - INFO - full garbage collection released 6.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:35,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:35,498 - distributed.utils_perf - INFO - full garbage collection released 41.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:36,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,830 - distributed.utils_perf - INFO - full garbage collection released 1.81 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:37,168 - distributed.utils_perf - INFO - full garbage collection released 640.78 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:37,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,266 - distributed.utils_perf - INFO - full garbage collection released 425.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:37,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,989 - distributed.utils_perf - INFO - full garbage collection released 5.69 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:40,066 - distributed.utils_perf - INFO - full garbage collection released 1.50 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:40,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:50,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:50,685 - distributed.utils_perf - INFO - full garbage collection released 142.31 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:50,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,579 - distributed.utils_perf - INFO - full garbage collection released 149.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:51,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,494 - distributed.utils_perf - INFO - full garbage collection released 1.26 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:54,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:58,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:59,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:00,434 - distributed.utils_perf - INFO - full garbage collection released 257.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:00,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:03,267 - distributed.utils_perf - INFO - full garbage collection released 362.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:03,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:05,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:05,483 - distributed.utils_perf - INFO - full garbage collection released 219.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:06,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:09,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:10,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:12,702 - distributed.utils_perf - INFO - full garbage collection released 1.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:13,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:14,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:15,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:17,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:18,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:18,867 - distributed.utils_perf - INFO - full garbage collection released 350.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:18,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:19,302 - distributed.utils_perf - INFO - full garbage collection released 1.85 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:19,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:21,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:23,140 - distributed.utils_perf - INFO - full garbage collection released 193.91 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:23,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:29,413 - distributed.utils_perf - INFO - full garbage collection released 102.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:32,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:34,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:36,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:36,662 - distributed.utils_perf - INFO - full garbage collection released 175.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:37,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:39,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:43,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:45,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:46,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:48,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:01,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:01,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,772 - distributed.utils_perf - INFO - full garbage collection released 1.04 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:04,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:05,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:07,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:08,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:13,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:15,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:19,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:24,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:26,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:28,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:31,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:34,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:34,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:34,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:38,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:39,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:43,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:43,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:44,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:44,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:48,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:50,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:50,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:53,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:55,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:00,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:03,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:04,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:06,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:08,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:13,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:15,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:18,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:21,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:22,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:23,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:24,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:26,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:31,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:35,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:44,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:45,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:45,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:47,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:48,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:51,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:51,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:53,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:55,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:57,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:00,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:01,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:02,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:07,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:07,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:11,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:14,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:15,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:17,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:18,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:20,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:21,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:24,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:25,098 - distributed.utils_perf - INFO - full garbage collection released 1.92 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:27,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:30,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:33,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:33,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:35,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:41,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:42,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:42,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:43,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:44,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:46,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:50,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:52,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:55,246 - distributed.utils_perf - INFO - full garbage collection released 203.71 MiB from 131 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:57,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:57,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:59,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:01,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:04,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:06,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:12,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:12,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:12,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:13,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:13,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:20,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:20,476 - distributed.utils_perf - INFO - full garbage collection released 299.02 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:24,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:24,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:25,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:30,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:31,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:33,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:36,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:41,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:43,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:48,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:49,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:50,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:52,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:54,400 - distributed.utils_perf - INFO - full garbage collection released 36.77 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:55,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:06,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:08,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:12,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:14,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:14,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:16,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:19,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:20,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:25,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:28,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:29,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:30,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:33,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:37,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:42,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:44,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:45,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:48,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:48,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:56,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:01,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:01,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:04,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:07,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:08,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:10,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:13,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:20,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:22,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:24,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:25,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:25,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:27,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:28,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:32,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:35,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:37,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:42,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:42,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:44,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:44,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:45,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:54,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:55,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:57,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:02,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:04,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:05,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:11,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:11,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:12,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:16,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:18,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:24,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:29,646 - distributed.utils_perf - INFO - full garbage collection released 1.14 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:29,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:30,389 - distributed.utils_perf - INFO - full garbage collection released 700.73 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:30,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:34,912 - distributed.utils_perf - INFO - full garbage collection released 361.79 MiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:34,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:35,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:36,367 - distributed.utils_perf - INFO - full garbage collection released 34.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:36,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:40,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:44,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:47,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:50,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:53,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:54,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:55,042 - distributed.utils_perf - INFO - full garbage collection released 1.17 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:55,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:00,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:03,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:07,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:07,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:16,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:23,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:50:58] task [xgboost.dask-tcp://10.201.1.183:37669]:tcp://10.201.1.183:37669 got new rank 0
[15:50:58] task [xgboost.dask-tcp://10.201.1.183:39331]:tcp://10.201.1.183:39331 got new rank 1
[15:50:58] task [xgboost.dask-tcp://10.201.1.183:40005]:tcp://10.201.1.183:40005 got new rank 2
[15:50:58] task [xgboost.dask-tcp://10.201.1.183:40547]:tcp://10.201.1.183:40547 got new rank 3
[15:50:58] task [xgboost.dask-tcp://10.201.1.75:34861]:tcp://10.201.1.75:34861 got new rank 4
[15:50:58] task [xgboost.dask-tcp://10.201.1.75:38113]:tcp://10.201.1.75:38113 got new rank 5
[15:50:58] task [xgboost.dask-tcp://10.201.1.75:39333]:tcp://10.201.1.75:39333 got new rank 6
[15:50:58] task [xgboost.dask-tcp://10.201.1.75:46593]:tcp://10.201.1.75:46593 got new rank 7
2024-04-19 15:53:03,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:05,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:53:44] task [xgboost.dask-tcp://10.201.1.183:37669]:tcp://10.201.1.183:37669 got new rank 0
[15:53:44] task [xgboost.dask-tcp://10.201.1.183:39331]:tcp://10.201.1.183:39331 got new rank 1
[15:53:44] task [xgboost.dask-tcp://10.201.1.183:40005]:tcp://10.201.1.183:40005 got new rank 2
[15:53:44] task [xgboost.dask-tcp://10.201.1.183:40547]:tcp://10.201.1.183:40547 got new rank 3
[15:53:44] task [xgboost.dask-tcp://10.201.1.75:34861]:tcp://10.201.1.75:34861 got new rank 4
[15:53:44] task [xgboost.dask-tcp://10.201.1.75:38113]:tcp://10.201.1.75:38113 got new rank 5
[15:53:44] task [xgboost.dask-tcp://10.201.1.75:39333]:tcp://10.201.1.75:39333 got new rank 6
[15:53:44] task [xgboost.dask-tcp://10.201.1.75:46593]:tcp://10.201.1.75:46593 got new rank 7
[15:56:41] task [xgboost.dask-tcp://10.201.1.183:37669]:tcp://10.201.1.183:37669 got new rank 0
[15:56:41] task [xgboost.dask-tcp://10.201.1.183:39331]:tcp://10.201.1.183:39331 got new rank 1
[15:56:41] task [xgboost.dask-tcp://10.201.1.183:40005]:tcp://10.201.1.183:40005 got new rank 2
[15:56:41] task [xgboost.dask-tcp://10.201.1.183:40547]:tcp://10.201.1.183:40547 got new rank 3
[15:56:41] task [xgboost.dask-tcp://10.201.1.75:34861]:tcp://10.201.1.75:34861 got new rank 4
[15:56:41] task [xgboost.dask-tcp://10.201.1.75:38113]:tcp://10.201.1.75:38113 got new rank 5
[15:56:41] task [xgboost.dask-tcp://10.201.1.75:39333]:tcp://10.201.1.75:39333 got new rank 6
[15:56:41] task [xgboost.dask-tcp://10.201.1.75:46593]:tcp://10.201.1.75:46593 got new rank 7
2024-04-19 15:59:06,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:07,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:59:49] task [xgboost.dask-tcp://10.201.1.183:37669]:tcp://10.201.1.183:37669 got new rank 0
[15:59:49] task [xgboost.dask-tcp://10.201.1.183:39331]:tcp://10.201.1.183:39331 got new rank 1
[15:59:49] task [xgboost.dask-tcp://10.201.1.183:40005]:tcp://10.201.1.183:40005 got new rank 2
[15:59:49] task [xgboost.dask-tcp://10.201.1.183:40547]:tcp://10.201.1.183:40547 got new rank 3
[15:59:49] task [xgboost.dask-tcp://10.201.1.75:34861]:tcp://10.201.1.75:34861 got new rank 4
[15:59:49] task [xgboost.dask-tcp://10.201.1.75:38113]:tcp://10.201.1.75:38113 got new rank 5
[15:59:49] task [xgboost.dask-tcp://10.201.1.75:39333]:tcp://10.201.1.75:39333 got new rank 6
[15:59:49] task [xgboost.dask-tcp://10.201.1.75:46593]:tcp://10.201.1.75:46593 got new rank 7
2024-04-19 16:04:06,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:04:07,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:04:18,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:04:18,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:04:47] task [xgboost.dask-tcp://10.201.1.183:37669]:tcp://10.201.1.183:37669 got new rank 0
[16:04:47] task [xgboost.dask-tcp://10.201.1.183:39331]:tcp://10.201.1.183:39331 got new rank 1
[16:04:47] task [xgboost.dask-tcp://10.201.1.183:40005]:tcp://10.201.1.183:40005 got new rank 2
[16:04:47] task [xgboost.dask-tcp://10.201.1.183:40547]:tcp://10.201.1.183:40547 got new rank 3
[16:04:47] task [xgboost.dask-tcp://10.201.1.75:34861]:tcp://10.201.1.75:34861 got new rank 4
[16:04:47] task [xgboost.dask-tcp://10.201.1.75:38113]:tcp://10.201.1.75:38113 got new rank 5
[16:04:47] task [xgboost.dask-tcp://10.201.1.75:39333]:tcp://10.201.1.75:39333 got new rank 6
[16:04:47] task [xgboost.dask-tcp://10.201.1.75:46593]:tcp://10.201.1.75:46593 got new rank 7
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:39331. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:46593. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:40005. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:38113. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:37669. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:39333. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:40547. Reason: scheduler-close
2024-04-19 16:07:19,183 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:34861. Reason: scheduler-close
2024-04-19 16:07:19,185 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:36313'. Reason: scheduler-close
2024-04-19 16:07:19,185 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:46559'. Reason: scheduler-close
2024-04-19 16:07:19,185 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:38353'. Reason: scheduler-close
2024-04-19 16:07:19,186 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:32791'. Reason: scheduler-close
2024-04-19 16:07:19,186 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:33333'. Reason: scheduler-close
2024-04-19 16:07:19,184 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:39678 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:39678 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 16:07:19,191 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:46137'. Reason: scheduler-close
2024-04-19 16:07:19,185 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:57522 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:57522 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 16:07:19,185 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:57536 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:57536 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 16:07:19,195 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:41245'. Reason: scheduler-close
2024-04-19 16:07:19,195 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:43769'. Reason: scheduler-close
2024-04-19 16:07:19,298 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,299 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,307 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,307 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,333 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,333 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,363 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,364 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,878 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:41664 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 16:07:19,879 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:07:19,879 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:07:19,879 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:41634 remote=tcp://10.201.3.1:8786>: Stream is closed
/10.201.1.183:41618 remote=tcp://10.201.3.1:8786>: Stream is closed
/10.201.1.183:35852 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 16:07:19,978 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,978 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,988 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,988 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:19,988 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:19,988 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:20,034 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 16:07:20,035 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:21,346 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,370 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,384 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,424 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:38353'. Reason: nanny-close-gracefully
2024-04-19 16:07:21,912 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:21,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:36313'. Reason: nanny-close-gracefully
2024-04-19 16:07:21,925 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:21,930 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:46559'. Reason: nanny-close-gracefully
2024-04-19 16:07:21,930 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:21,981 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,990 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:21,990 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:22,036 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:22,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:46137'. Reason: nanny-close-gracefully
2024-04-19 16:07:22,387 - distributed.dask_worker - INFO - End worker
