2024-04-19 15:41:04,949 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,951 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,950 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,951 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,950 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,951 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,950 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,951 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,223 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,223 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,223 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,224 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,257 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,257 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,257 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,258 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.90:44167'
2024-04-19 15:41:05,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.90:44727'
2024-04-19 15:41:05,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.90:33389'
2024-04-19 15:41:05,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.90:37045'
2024-04-19 15:41:05,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.225:40867'
2024-04-19 15:41:05,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.225:45241'
2024-04-19 15:41:05,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.225:42801'
2024-04-19 15:41:05,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.225:44665'
2024-04-19 15:41:06,248 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,248 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,248 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,248 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,248 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,249 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,249 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,249 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,250 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,250 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,251 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,251 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,251 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,251 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,251 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,252 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,293 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,293 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,294 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,294 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,296 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,296 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,296 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,296 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,361 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,361 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,361 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,362 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,363 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,363 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,364 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,364 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,424 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e65452f8-9f04-4759-a2e3-fbb2dbcc889b
2024-04-19 15:41:08,424 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-cfb93053-ee6a-4a8b-8d18-5300dd571e23
2024-04-19 15:41:08,424 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-adbc3786-6092-4395-835f-e44431f36265
2024-04-19 15:41:08,424 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.90:46495
2024-04-19 15:41:08,425 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.90:46495
2024-04-19 15:41:08,424 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0fde9bb1-ec5b-491a-baff-4cd48f599d96
2024-04-19 15:41:08,424 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.90:37275
2024-04-19 15:41:08,425 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.90:37275
2024-04-19 15:41:08,425 - distributed.worker - INFO -          dashboard at:          10.201.2.90:33587
2024-04-19 15:41:08,425 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,424 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.90:43711
2024-04-19 15:41:08,424 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.90:43711
2024-04-19 15:41:08,424 - distributed.worker - INFO -          dashboard at:          10.201.2.90:40949
2024-04-19 15:41:08,425 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,425 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3oa30bnd
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,424 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.90:40487
2024-04-19 15:41:08,425 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.90:40487
2024-04-19 15:41:08,425 - distributed.worker - INFO -          dashboard at:          10.201.2.90:41301
2024-04-19 15:41:08,425 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,425 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uqao5ra9
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO -          dashboard at:          10.201.2.90:44725
2024-04-19 15:41:08,425 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,425 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sj_9jvxw
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,425 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,425 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4i0w7l_f
2024-04-19 15:41:08,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b3b19c81-e26a-4fbc-a1ee-f0384a67be53
2024-04-19 15:41:08,454 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-309619e7-6d8a-4792-b462-4b0c7df0779c
2024-04-19 15:41:08,454 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-6c112483-3987-4d8d-b6a7-cedc4a2f9870
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.225:40815
2024-04-19 15:41:08,454 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.225:40815
2024-04-19 15:41:08,454 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a7a53990-bb25-4600-b355-c972967e20d1
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.225:37687
2024-04-19 15:41:08,454 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.225:37687
2024-04-19 15:41:08,454 - distributed.worker - INFO -          dashboard at:         10.201.1.225:46391
2024-04-19 15:41:08,454 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.225:36307
2024-04-19 15:41:08,454 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.225:36307
2024-04-19 15:41:08,454 - distributed.worker - INFO -          dashboard at:         10.201.1.225:35291
2024-04-19 15:41:08,454 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,454 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yl04m456
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.225:40311
2024-04-19 15:41:08,454 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.225:40311
2024-04-19 15:41:08,454 - distributed.worker - INFO -          dashboard at:         10.201.1.225:32963
2024-04-19 15:41:08,454 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,454 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,454 - distributed.worker - INFO -          dashboard at:         10.201.1.225:38159
2024-04-19 15:41:08,454 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.52:8786
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,454 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,454 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b327wfir
2024-04-19 15:41:08,454 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,454 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_0of5o5
2024-04-19 15:41:08,454 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ttthmz8g
2024-04-19 15:41:08,455 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,455 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,262 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,262 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,263 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,264 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,264 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,264 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,265 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,267 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,267 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,268 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,268 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,269 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,269 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,270 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,270 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,270 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,271 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,271 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,272 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,272 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,272 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,272 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,273 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:12,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,273 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.52:8786
2024-04-19 15:41:12,273 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,274 - distributed.core - INFO - Starting established connection to tcp://10.201.2.52:8786
2024-04-19 15:41:21,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,338 - distributed.utils_perf - INFO - full garbage collection released 10.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:25,538 - distributed.utils_perf - INFO - full garbage collection released 137.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,110 - distributed.utils_perf - INFO - full garbage collection released 73.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:30,429 - distributed.utils_perf - INFO - full garbage collection released 2.95 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:30,880 - distributed.utils_perf - INFO - full garbage collection released 4.12 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:34,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,223 - distributed.utils_perf - INFO - full garbage collection released 210.84 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:34,450 - distributed.utils_perf - INFO - full garbage collection released 378.09 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:34,845 - distributed.utils_perf - INFO - full garbage collection released 1.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:34,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,998 - distributed.utils_perf - INFO - full garbage collection released 217.67 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:35,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:39,173 - distributed.utils_perf - INFO - full garbage collection released 2.41 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:41,465 - distributed.utils_perf - INFO - full garbage collection released 1.26 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:42,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:42,992 - distributed.utils_perf - INFO - full garbage collection released 121.09 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:43,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:47,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,736 - distributed.utils_perf - INFO - full garbage collection released 765.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:51,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:53,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:56,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:58,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:00,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:00,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,018 - distributed.utils_perf - INFO - full garbage collection released 268.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:02,727 - distributed.utils_perf - INFO - full garbage collection released 63.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:07,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:08,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:08,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:11,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:11,362 - distributed.utils_perf - INFO - full garbage collection released 691.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:12,093 - distributed.utils_perf - INFO - full garbage collection released 146.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:12,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:12,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:13,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:16,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:18,980 - distributed.utils_perf - INFO - full garbage collection released 220.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:19,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:19,992 - distributed.utils_perf - INFO - full garbage collection released 293.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:20,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:21,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:24,130 - distributed.utils_perf - INFO - full garbage collection released 181.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:26,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,017 - distributed.utils_perf - INFO - full garbage collection released 145.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:30,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:31,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:32,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:32,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,073 - distributed.utils_perf - INFO - full garbage collection released 54.06 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:35,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:46,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:48,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:49,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:49,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:54,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:55,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:56,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:58,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:58,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:00,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:00,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:02,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:05,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:09,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:09,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:13,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:13,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,757 - distributed.utils_perf - INFO - full garbage collection released 100.49 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:17,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:25,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:26,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:27,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:31,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:32,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:32,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:36,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:36,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:37,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:39,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:40,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:41,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:43,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:46,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:48,649 - distributed.utils_perf - INFO - full garbage collection released 2.08 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:52,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:53,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:55,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:56,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:59,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:01,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:05,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:05,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:06,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:08,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:12,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:16,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:18,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:21,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:22,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:23,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:25,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:27,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:28,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:29,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:30,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:33,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:34,780 - distributed.utils_perf - INFO - full garbage collection released 1.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:37,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:41,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:41,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:52,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:52,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:54,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:55,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:56,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:59,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:02,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:06,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:06,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:06,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:06,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:08,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:17,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:20,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:22,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:24,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:29,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:30,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:31,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:31,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:34,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:35,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:36,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:37,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:40,028 - distributed.utils_perf - INFO - full garbage collection released 505.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:40,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:41,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:51,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:52,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:57,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:01,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:02,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:02,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:03,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:03,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:05,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:08,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:09,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:15,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:20,588 - distributed.utils_perf - INFO - full garbage collection released 11.83 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:20,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:22,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:22,808 - distributed.utils_perf - INFO - full garbage collection released 362.60 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:24,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:24,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:31,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:32,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:33,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:35,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:35,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:41,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:47,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:48,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:49,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:51,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:55,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:55,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:59,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:59,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:05,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:07,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:07,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:10,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:11,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:11,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:13,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:16,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:19,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:20,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:26,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:29,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:31,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:33,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:37,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:43,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:43,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:46,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:46,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:46,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:48,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:51,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:56,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,435 - distributed.utils_perf - INFO - full garbage collection released 885.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:47:59,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:00,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:02,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:02,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:03,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:07,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:07,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:09,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:09,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:11,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:13,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:21,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:21,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:23,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:24,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:25,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:31,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:32,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:35,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:38,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:38,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:41,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:46,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:46,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:47,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:48,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,414 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:48:49,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,073 - distributed.utils_perf - INFO - full garbage collection released 9.67 MiB from 227 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:51,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,078 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:52,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:00,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:01,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:02,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:02,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:06,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:08,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:10,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:12,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:16,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:17,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:23,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:28,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:31,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:33,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:34,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:36,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:39,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:39,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:42,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:43,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:46,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:47,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:50,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:50,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,844 - distributed.utils_perf - INFO - full garbage collection released 9.75 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:51,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:52,523 - distributed.utils_perf - INFO - full garbage collection released 203.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:53,363 - distributed.utils_perf - INFO - full garbage collection released 246.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:55,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:57,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:58,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:03,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:04,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:05,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:05,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:13,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:17,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:19,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:23,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:33,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:39,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:45,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:46,020 - distributed.utils_perf - INFO - full garbage collection released 41.54 MiB from 190 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:47,413 - distributed.utils_perf - INFO - full garbage collection released 140.12 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:49,072 - distributed.utils_perf - INFO - full garbage collection released 32.99 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:51:03,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:51:24,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:51:57] task [xgboost.dask-tcp://10.201.1.225:36307]:tcp://10.201.1.225:36307 got new rank 0
[15:51:57] task [xgboost.dask-tcp://10.201.1.225:37687]:tcp://10.201.1.225:37687 got new rank 1
[15:51:57] task [xgboost.dask-tcp://10.201.1.225:40311]:tcp://10.201.1.225:40311 got new rank 2
[15:51:57] task [xgboost.dask-tcp://10.201.1.225:40815]:tcp://10.201.1.225:40815 got new rank 3
[15:51:57] task [xgboost.dask-tcp://10.201.2.90:37275]:tcp://10.201.2.90:37275 got new rank 4
[15:51:57] task [xgboost.dask-tcp://10.201.2.90:40487]:tcp://10.201.2.90:40487 got new rank 5
[15:51:57] task [xgboost.dask-tcp://10.201.2.90:43711]:tcp://10.201.2.90:43711 got new rank 6
[15:51:57] task [xgboost.dask-tcp://10.201.2.90:46495]:tcp://10.201.2.90:46495 got new rank 7
2024-04-19 15:54:21,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:21,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:21,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:39,027 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
[15:55:04] task [xgboost.dask-tcp://10.201.1.225:36307]:tcp://10.201.1.225:36307 got new rank 0
[15:55:04] task [xgboost.dask-tcp://10.201.1.225:37687]:tcp://10.201.1.225:37687 got new rank 1
[15:55:04] task [xgboost.dask-tcp://10.201.1.225:40311]:tcp://10.201.1.225:40311 got new rank 2
[15:55:04] task [xgboost.dask-tcp://10.201.1.225:40815]:tcp://10.201.1.225:40815 got new rank 3
[15:55:04] task [xgboost.dask-tcp://10.201.2.90:37275]:tcp://10.201.2.90:37275 got new rank 4
[15:55:04] task [xgboost.dask-tcp://10.201.2.90:40487]:tcp://10.201.2.90:40487 got new rank 5
[15:55:04] task [xgboost.dask-tcp://10.201.2.90:43711]:tcp://10.201.2.90:43711 got new rank 6
[15:55:04] task [xgboost.dask-tcp://10.201.2.90:46495]:tcp://10.201.2.90:46495 got new rank 7
2024-04-19 15:57:21,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:21,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:21,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:26,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:43,281 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
[15:57:58] task [xgboost.dask-tcp://10.201.1.225:36307]:tcp://10.201.1.225:36307 got new rank 0
[15:57:58] task [xgboost.dask-tcp://10.201.1.225:37687]:tcp://10.201.1.225:37687 got new rank 1
[15:57:58] task [xgboost.dask-tcp://10.201.1.225:40311]:tcp://10.201.1.225:40311 got new rank 2
[15:57:58] task [xgboost.dask-tcp://10.201.1.225:40815]:tcp://10.201.1.225:40815 got new rank 3
[15:57:58] task [xgboost.dask-tcp://10.201.2.90:37275]:tcp://10.201.2.90:37275 got new rank 4
[15:57:58] task [xgboost.dask-tcp://10.201.2.90:40487]:tcp://10.201.2.90:40487 got new rank 5
[15:57:58] task [xgboost.dask-tcp://10.201.2.90:43711]:tcp://10.201.2.90:43711 got new rank 6
[15:57:58] task [xgboost.dask-tcp://10.201.2.90:46495]:tcp://10.201.2.90:46495 got new rank 7
2024-04-19 16:00:24,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:24,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:49,456 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
[16:01:06] task [xgboost.dask-tcp://10.201.1.225:36307]:tcp://10.201.1.225:36307 got new rank 0
[16:01:06] task [xgboost.dask-tcp://10.201.1.225:37687]:tcp://10.201.1.225:37687 got new rank 1
[16:01:06] task [xgboost.dask-tcp://10.201.1.225:40311]:tcp://10.201.1.225:40311 got new rank 2
[16:01:06] task [xgboost.dask-tcp://10.201.1.225:40815]:tcp://10.201.1.225:40815 got new rank 3
[16:01:06] task [xgboost.dask-tcp://10.201.2.90:37275]:tcp://10.201.2.90:37275 got new rank 4
[16:01:06] task [xgboost.dask-tcp://10.201.2.90:40487]:tcp://10.201.2.90:40487 got new rank 5
[16:01:06] task [xgboost.dask-tcp://10.201.2.90:43711]:tcp://10.201.2.90:43711 got new rank 6
[16:01:06] task [xgboost.dask-tcp://10.201.2.90:46495]:tcp://10.201.2.90:46495 got new rank 7
2024-04-19 16:03:18,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:32,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:32,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:32,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:32,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:33,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:34,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:34,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:35,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:04:15] task [xgboost.dask-tcp://10.201.1.225:36307]:tcp://10.201.1.225:36307 got new rank 0
[16:04:15] task [xgboost.dask-tcp://10.201.1.225:37687]:tcp://10.201.1.225:37687 got new rank 1
[16:04:15] task [xgboost.dask-tcp://10.201.1.225:40311]:tcp://10.201.1.225:40311 got new rank 2
[16:04:15] task [xgboost.dask-tcp://10.201.1.225:40815]:tcp://10.201.1.225:40815 got new rank 3
[16:04:15] task [xgboost.dask-tcp://10.201.2.90:37275]:tcp://10.201.2.90:37275 got new rank 4
[16:04:15] task [xgboost.dask-tcp://10.201.2.90:40487]:tcp://10.201.2.90:40487 got new rank 5
[16:04:15] task [xgboost.dask-tcp://10.201.2.90:43711]:tcp://10.201.2.90:43711 got new rank 6
[16:04:15] task [xgboost.dask-tcp://10.201.2.90:46495]:tcp://10.201.2.90:46495 got new rank 7
2024-04-19 16:06:32,690 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.90:40487. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.225:37687. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.90:43711. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.225:36307. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.90:46495. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.225:40311. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.90:37275. Reason: scheduler-close
2024-04-19 16:06:51,990 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.225:40815. Reason: scheduler-close
2024-04-19 16:06:51,993 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.90:44727'. Reason: scheduler-close
2024-04-19 16:06:51,993 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.90:33389'. Reason: scheduler-close
2024-04-19 16:06:51,993 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.225:44665'. Reason: scheduler-close
2024-04-19 16:06:51,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.90:59360 remote=tcp://10.201.2.52:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.90:59360 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:51,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.90:59344 remote=tcp://10.201.2.52:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.90:59344 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:51,998 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.90:44167'. Reason: scheduler-close
2024-04-19 16:06:51,999 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.90:37045'. Reason: scheduler-close
2024-04-19 16:06:51,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33058 remote=tcp://10.201.2.52:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33058 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:51,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33036 remote=tcp://10.201.2.52:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33036 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:51,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33062 remote=tcp://10.201.2.52:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.225:33062 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:52,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.225:40867'. Reason: scheduler-close
2024-04-19 16:06:52,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.225:45241'. Reason: scheduler-close
2024-04-19 16:06:52,006 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.225:42801'. Reason: scheduler-close
2024-04-19 16:06:52,786 - distributed.comm.tcp - INFO - Connection from tcp://10.201.1.225:44066 closed before handshake completed
2024-04-19 16:06:52,786 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:52,786 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.225:41288 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:52,786 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:52,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:52,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:52,788 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.225:41280 remote=tcp://10.201.2.52:8786>: Stream is closed
/10.201.2.90:59400 remote=tcp://10.201.2.52:8786>: Stream is closed
/10.201.1.225:54688 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:52,787 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.225:54682 remote=tcp://10.201.2.52:8786>: Stream is closed
/10.201.2.90:60340 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:52,788 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.90:59374 remote=tcp://10.201.2.52:8786>: Stream is closed
/10.201.2.90:60358 remote=tcp://10.201.2.52:8786>: Stream is closed
2024-04-19 16:06:52,837 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,837 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,886 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,886 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,893 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,893 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,893 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,893 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,894 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,894 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,895 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,895 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,922 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,922 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:52,961 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.52:8786; closing.
2024-04-19 16:06:52,962 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:54,840 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,896 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,896 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,896 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,896 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,900 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,924 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:56,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.225:45241'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,184 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,195 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.225:44665'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,195 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.90:33389'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,200 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,239 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.90:37045'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,240 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.225:42801'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,301 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.90:44167'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,311 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.225:40867'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,660 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:56,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.90:44727'. Reason: nanny-close-gracefully
2024-04-19 16:06:56,672 - distributed.dask_worker - INFO - End worker
