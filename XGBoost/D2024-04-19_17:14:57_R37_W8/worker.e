2024-04-19 17:16:14,140 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,141 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,141 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,142 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,453 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,453 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,453 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,454 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:37091'
2024-04-19 17:16:14,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:38759'
2024-04-19 17:16:14,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:38321'
2024-04-19 17:16:14,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:37679'
2024-04-19 17:16:14,692 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,692 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,692 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,692 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,025 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,025 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,025 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,025 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:42709'
2024-04-19 17:16:15,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:42467'
2024-04-19 17:16:15,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:32991'
2024-04-19 17:16:15,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:46155'
2024-04-19 17:16:15,339 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,339 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,339 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,340 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,340 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,340 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,340 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,340 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,368 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,368 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,370 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,371 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,078 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,079 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,080 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,081 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,081 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,127 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,127 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,127 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,127 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,279 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,279 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,279 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,279 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,113 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,113 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,113 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,113 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,396 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-535aa394-0aa1-4173-8bee-42e5aa7e3a3b
2024-04-19 17:16:17,396 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1e9fae5d-5b66-4699-b6cb-9f13287501d1
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:42525
2024-04-19 17:16:17,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:42525
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:46621
2024-04-19 17:16:17,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:46621
2024-04-19 17:16:17,397 - distributed.worker - INFO -          dashboard at:         10.201.3.105:44953
2024-04-19 17:16:17,397 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:17,397 - distributed.worker - INFO -          dashboard at:         10.201.3.105:42187
2024-04-19 17:16:17,397 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wparerij
2024-04-19 17:16:17,397 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5f83efdb-0816-4076-aeec-10a8c402c784
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:41881
2024-04-19 17:16:17,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:41881
2024-04-19 17:16:17,397 - distributed.worker - INFO -          dashboard at:         10.201.3.105:40617
2024-04-19 17:16:17,397 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,396 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-eca29f7c-5a21-40ad-8bd1-5bb6e8f7c7f8
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:40911
2024-04-19 17:16:17,397 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:40911
2024-04-19 17:16:17,397 - distributed.worker - INFO -          dashboard at:         10.201.3.105:41375
2024-04-19 17:16:17,397 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ggolb09l
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ic_v2jq
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,397 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_p_qh0dv
2024-04-19 17:16:17,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,231 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-68ab897e-f0c1-4831-9718-92febfb34b1a
2024-04-19 17:16:18,231 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-bcd60036-ac1d-47e7-8d2b-ca6086de84ac
2024-04-19 17:16:18,231 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c469aec9-a0cd-45d1-b970-1cd448a5cac4
2024-04-19 17:16:18,231 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:40677
2024-04-19 17:16:18,231 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:39251
2024-04-19 17:16:18,231 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:39251
2024-04-19 17:16:18,231 - distributed.worker - INFO -          dashboard at:         10.201.3.107:38851
2024-04-19 17:16:18,231 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:44927
2024-04-19 17:16:18,231 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:44927
2024-04-19 17:16:18,231 - distributed.worker - INFO -          dashboard at:         10.201.3.107:32923
2024-04-19 17:16:18,231 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:18,231 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,231 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,231 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,231 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:40677
2024-04-19 17:16:18,231 - distributed.worker - INFO -          dashboard at:         10.201.3.107:34917
2024-04-19 17:16:18,231 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:18,231 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,231 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,232 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,231 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:18,231 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,232 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,232 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,231 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-82f8c3d7-1f8e-4895-aaba-c5ed7173e69a
2024-04-19 17:16:18,231 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:44255
2024-04-19 17:16:18,232 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:44255
2024-04-19 17:16:18,232 - distributed.worker - INFO -          dashboard at:         10.201.3.107:33169
2024-04-19 17:16:18,231 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-074lx4ls
2024-04-19 17:16:18,232 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6f_xcwd9
2024-04-19 17:16:18,232 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j3fh752d
2024-04-19 17:16:18,232 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,232 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.95:8786
2024-04-19 17:16:18,232 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,232 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,232 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hfx921ay
2024-04-19 17:16:18,232 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,980 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,980 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,980 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,981 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,982 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,984 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,985 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,985 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,986 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,986 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,987 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,988 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,988 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,989 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,989 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,989 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,990 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,990 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,990 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,990 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:20,991 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.95:8786
2024-04-19 17:16:20,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,991 - distributed.core - INFO - Starting established connection to tcp://10.201.3.95:8786
2024-04-19 17:16:30,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,575 - distributed.utils_perf - INFO - full garbage collection released 134.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,297 - distributed.utils_perf - INFO - full garbage collection released 69.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,855 - distributed.utils_perf - INFO - full garbage collection released 164.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,978 - distributed.utils_perf - INFO - full garbage collection released 149.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,143 - distributed.utils_perf - INFO - full garbage collection released 151.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,324 - distributed.utils_perf - INFO - full garbage collection released 200.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:39,287 - distributed.utils_perf - INFO - full garbage collection released 204.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,794 - distributed.utils_perf - INFO - full garbage collection released 619.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:43,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,488 - distributed.utils_perf - INFO - full garbage collection released 1.09 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,842 - distributed.utils_perf - INFO - full garbage collection released 327.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,343 - distributed.utils_perf - INFO - full garbage collection released 7.82 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,246 - distributed.utils_perf - INFO - full garbage collection released 1.68 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,319 - distributed.utils_perf - INFO - full garbage collection released 1.41 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,693 - distributed.utils_perf - INFO - full garbage collection released 1.53 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,412 - distributed.utils_perf - INFO - full garbage collection released 813.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:58,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,329 - distributed.utils_perf - INFO - full garbage collection released 202.84 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:02,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,469 - distributed.utils_perf - INFO - full garbage collection released 173.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:07,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:11,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:11,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,918 - distributed.utils_perf - INFO - full garbage collection released 98.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:18,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,354 - distributed.utils_perf - INFO - full garbage collection released 92.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:24,970 - distributed.utils_perf - INFO - full garbage collection released 37.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:25,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,300 - distributed.utils_perf - INFO - full garbage collection released 914.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:27,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,743 - distributed.utils_perf - INFO - full garbage collection released 543.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:32,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:38,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:41,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,898 - distributed.utils_perf - INFO - full garbage collection released 54.99 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:49,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:58,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:12,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:20,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:41,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:54,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:14,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 59.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,092 - distributed.utils_perf - INFO - full garbage collection released 2.51 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:31,798 - distributed.utils_perf - INFO - full garbage collection released 14.08 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:32,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:39,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:43,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:52,566 - distributed.utils_perf - INFO - full garbage collection released 591.30 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:56,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,619 - distributed.utils_perf - INFO - full garbage collection released 129.44 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:13,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,292 - distributed.utils_perf - INFO - full garbage collection released 7.60 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:41,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:47,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:08,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,622 - distributed.utils_perf - INFO - full garbage collection released 1.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:11,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:11,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:19,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,409 - distributed.utils_perf - INFO - full garbage collection released 2.09 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:15,203 - distributed.utils_perf - INFO - full garbage collection released 644.32 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:16,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,749 - distributed.utils_perf - INFO - full garbage collection released 0.98 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:21,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:30,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,519 - distributed.utils_perf - INFO - full garbage collection released 1.57 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:49,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:22,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:27,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,763 - distributed.utils_perf - INFO - full garbage collection released 13.80 MiB from 151 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:39,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,399 - distributed.utils_perf - INFO - full garbage collection released 1.17 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:47,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,638 - distributed.utils_perf - INFO - full garbage collection released 6.18 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:52,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:03,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,987 - distributed.utils_perf - INFO - full garbage collection released 6.37 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:21,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:49,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:57,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:59,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,356 - distributed.utils_perf - INFO - full garbage collection released 154.77 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:04,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:04,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:05,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:06,542 - distributed.utils_perf - INFO - full garbage collection released 426.06 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:15,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:17,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:20,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:31,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:37,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:41,453 - distributed.utils_perf - INFO - full garbage collection released 13.93 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:53,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:54,657 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:54,861 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:55,104 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:25:55,398 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:25:55,755 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:25:56,185 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:25:56,705 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:57,327 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:58,091 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:59,033 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:26:00,190 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:26:01,621 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:26:03,442 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:26:05,689 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:26:08,466 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:26:16,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:17,737 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 17:26:18,166 - distributed.utils_perf - WARNING - full garbage collections took 37% CPU time recently (threshold: 10%)
2024-04-19 17:26:18,682 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 17:26:19,316 - distributed.utils_perf - WARNING - full garbage collections took 32% CPU time recently (threshold: 10%)
2024-04-19 17:26:20,099 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2024-04-19 17:26:21,059 - distributed.utils_perf - WARNING - full garbage collections took 27% CPU time recently (threshold: 10%)
2024-04-19 17:26:22,238 - distributed.utils_perf - WARNING - full garbage collections took 24% CPU time recently (threshold: 10%)
2024-04-19 17:26:23,699 - distributed.utils_perf - WARNING - full garbage collections took 20% CPU time recently (threshold: 10%)
2024-04-19 17:26:25,540 - distributed.utils_perf - WARNING - full garbage collections took 21% CPU time recently (threshold: 10%)
2024-04-19 17:26:27,822 - distributed.utils_perf - WARNING - full garbage collections took 20% CPU time recently (threshold: 10%)
2024-04-19 17:26:30,639 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
2024-04-19 17:26:38,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:27:10] task [xgboost.dask-tcp://10.201.3.105:40911]:tcp://10.201.3.105:40911 got new rank 0
[17:27:10] task [xgboost.dask-tcp://10.201.3.105:41881]:tcp://10.201.3.105:41881 got new rank 1
[17:27:10] task [xgboost.dask-tcp://10.201.3.105:42525]:tcp://10.201.3.105:42525 got new rank 2
[17:27:10] task [xgboost.dask-tcp://10.201.3.105:46621]:tcp://10.201.3.105:46621 got new rank 3
[17:27:10] task [xgboost.dask-tcp://10.201.3.107:39251]:tcp://10.201.3.107:39251 got new rank 4
[17:27:10] task [xgboost.dask-tcp://10.201.3.107:40677]:tcp://10.201.3.107:40677 got new rank 5
[17:27:10] task [xgboost.dask-tcp://10.201.3.107:44255]:tcp://10.201.3.107:44255 got new rank 6
[17:27:10] task [xgboost.dask-tcp://10.201.3.107:44927]:tcp://10.201.3.107:44927 got new rank 7
2024-04-19 17:29:19,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:21,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:30:02] task [xgboost.dask-tcp://10.201.3.105:40911]:tcp://10.201.3.105:40911 got new rank 0
[17:30:02] task [xgboost.dask-tcp://10.201.3.105:41881]:tcp://10.201.3.105:41881 got new rank 1
[17:30:02] task [xgboost.dask-tcp://10.201.3.105:42525]:tcp://10.201.3.105:42525 got new rank 2
[17:30:02] task [xgboost.dask-tcp://10.201.3.105:46621]:tcp://10.201.3.105:46621 got new rank 3
[17:30:02] task [xgboost.dask-tcp://10.201.3.107:39251]:tcp://10.201.3.107:39251 got new rank 4
[17:30:02] task [xgboost.dask-tcp://10.201.3.107:40677]:tcp://10.201.3.107:40677 got new rank 5
[17:30:02] task [xgboost.dask-tcp://10.201.3.107:44255]:tcp://10.201.3.107:44255 got new rank 6
[17:30:02] task [xgboost.dask-tcp://10.201.3.107:44927]:tcp://10.201.3.107:44927 got new rank 7
2024-04-19 17:32:05,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:05,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:05,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:32:47] task [xgboost.dask-tcp://10.201.3.105:40911]:tcp://10.201.3.105:40911 got new rank 0
[17:32:47] task [xgboost.dask-tcp://10.201.3.105:41881]:tcp://10.201.3.105:41881 got new rank 1
[17:32:47] task [xgboost.dask-tcp://10.201.3.105:42525]:tcp://10.201.3.105:42525 got new rank 2
[17:32:47] task [xgboost.dask-tcp://10.201.3.105:46621]:tcp://10.201.3.105:46621 got new rank 3
[17:32:47] task [xgboost.dask-tcp://10.201.3.107:39251]:tcp://10.201.3.107:39251 got new rank 4
[17:32:47] task [xgboost.dask-tcp://10.201.3.107:40677]:tcp://10.201.3.107:40677 got new rank 5
[17:32:47] task [xgboost.dask-tcp://10.201.3.107:44255]:tcp://10.201.3.107:44255 got new rank 6
[17:32:47] task [xgboost.dask-tcp://10.201.3.107:44927]:tcp://10.201.3.107:44927 got new rank 7
2024-04-19 17:35:20,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:20,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:21,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:21,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:21,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:58] task [xgboost.dask-tcp://10.201.3.105:40911]:tcp://10.201.3.105:40911 got new rank 0
[17:35:58] task [xgboost.dask-tcp://10.201.3.105:41881]:tcp://10.201.3.105:41881 got new rank 1
[17:35:58] task [xgboost.dask-tcp://10.201.3.105:42525]:tcp://10.201.3.105:42525 got new rank 2
[17:35:58] task [xgboost.dask-tcp://10.201.3.105:46621]:tcp://10.201.3.105:46621 got new rank 3
[17:35:58] task [xgboost.dask-tcp://10.201.3.107:39251]:tcp://10.201.3.107:39251 got new rank 4
[17:35:58] task [xgboost.dask-tcp://10.201.3.107:40677]:tcp://10.201.3.107:40677 got new rank 5
[17:35:58] task [xgboost.dask-tcp://10.201.3.107:44255]:tcp://10.201.3.107:44255 got new rank 6
[17:35:58] task [xgboost.dask-tcp://10.201.3.107:44927]:tcp://10.201.3.107:44927 got new rank 7
2024-04-19 17:38:12,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:12,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:12,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:12,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:12,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:26,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:28,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:28,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:28,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:28,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:29,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:30,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:31,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:39:12] task [xgboost.dask-tcp://10.201.3.105:40911]:tcp://10.201.3.105:40911 got new rank 0
[17:39:12] task [xgboost.dask-tcp://10.201.3.105:41881]:tcp://10.201.3.105:41881 got new rank 1
[17:39:12] task [xgboost.dask-tcp://10.201.3.105:42525]:tcp://10.201.3.105:42525 got new rank 2
[17:39:12] task [xgboost.dask-tcp://10.201.3.105:46621]:tcp://10.201.3.105:46621 got new rank 3
[17:39:12] task [xgboost.dask-tcp://10.201.3.107:39251]:tcp://10.201.3.107:39251 got new rank 4
[17:39:12] task [xgboost.dask-tcp://10.201.3.107:40677]:tcp://10.201.3.107:40677 got new rank 5
[17:39:12] task [xgboost.dask-tcp://10.201.3.107:44255]:tcp://10.201.3.107:44255 got new rank 6
[17:39:12] task [xgboost.dask-tcp://10.201.3.107:44927]:tcp://10.201.3.107:44927 got new rank 7
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:42525. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:46621. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:40677. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:41881. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:44927. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:40911. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:39251. Reason: scheduler-close
2024-04-19 17:41:51,218 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:44255. Reason: scheduler-close
2024-04-19 17:41:51,221 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:46155'. Reason: scheduler-close
2024-04-19 17:41:51,221 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:42467'. Reason: scheduler-close
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52158 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52158 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52146 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52146 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52136 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52136 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52144 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:52144 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:38990 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:38990 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:38321'. Reason: scheduler-close
2024-04-19 17:41:51,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:38759'. Reason: scheduler-close
2024-04-19 17:41:51,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:38974 remote=tcp://10.201.3.95:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:38974 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:51,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:37679'. Reason: scheduler-close
2024-04-19 17:41:51,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:37091'. Reason: scheduler-close
2024-04-19 17:41:51,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:32991'. Reason: scheduler-close
2024-04-19 17:41:51,229 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:42709'. Reason: scheduler-close
2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:52,013 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.105:41624 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.105:60578 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.105:41646 remote=tcp://10.201.3.95:8786>: Stream is closed
/10.201.3.105:60586 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.107:36006 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:52,014 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.107:35990 remote=tcp://10.201.3.95:8786>: Stream is closed
/10.201.3.107:49960 remote=tcp://10.201.3.95:8786>: Stream is closed
/10.201.3.107:49982 remote=tcp://10.201.3.95:8786>: Stream is closed
2024-04-19 17:41:52,059 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,059 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,087 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,088 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,101 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,102 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,105 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,106 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,121 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,121 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,131 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,131 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,141 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,142 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:52,161 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.95:8786; closing.
2024-04-19 17:41:52,161 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:54,062 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,091 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,104 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,109 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,123 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,132 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,143 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:54,162 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:55,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:37091'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,407 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,448 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:46155'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,449 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:42467'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,457 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:32991'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,497 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:37679'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,513 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:38321'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,519 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:55,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:38759'. Reason: nanny-close-gracefully
2024-04-19 17:41:55,857 - distributed.dask_worker - INFO - End worker
