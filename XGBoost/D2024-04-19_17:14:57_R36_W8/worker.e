2024-04-19 17:16:13,726 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:13,727 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:13,727 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:13,727 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,051 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,051 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,051 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,051 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.79:37975'
2024-04-19 17:16:14,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.79:43053'
2024-04-19 17:16:14,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.79:34747'
2024-04-19 17:16:14,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.79:42967'
2024-04-19 17:16:14,304 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,304 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,304 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,304 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,619 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,619 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,619 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,619 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.85:41391'
2024-04-19 17:16:14,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.85:43003'
2024-04-19 17:16:14,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.85:39607'
2024-04-19 17:16:14,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.85:34947'
2024-04-19 17:16:14,992 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:14,992 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:14,992 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:14,992 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,992 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:14,992 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,993 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,993 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,017 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,017 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,018 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,018 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,730 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,730 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,730 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,731 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,731 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,731 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,731 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,732 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,778 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,778 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,778 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,778 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,933 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:15,933 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:15,933 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:15,933 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,770 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,770 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,770 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,770 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,055 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d6959b0f-1ab4-4c1f-95cf-349cbf3ed8ba
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.79:33147
2024-04-19 17:16:17,055 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-ac5e5b4f-7667-412a-b1ce-30958adac503
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.79:43565
2024-04-19 17:16:17,055 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-869ab328-060d-4379-8ca5-b448b60c426c
2024-04-19 17:16:17,055 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.79:33147
2024-04-19 17:16:17,055 - distributed.worker - INFO -          dashboard at:          10.201.3.79:42171
2024-04-19 17:16:17,055 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,055 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,055 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,055 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,055 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.79:43565
2024-04-19 17:16:17,055 - distributed.worker - INFO -          dashboard at:          10.201.3.79:38205
2024-04-19 17:16:17,055 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,055 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,055 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,055 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dlf350xv
2024-04-19 17:16:17,055 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,055 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-46a3e34b-39c8-4ba5-82e8-493c5878e120
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.79:46505
2024-04-19 17:16:17,055 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.79:46505
2024-04-19 17:16:17,055 - distributed.worker - INFO -          dashboard at:          10.201.3.79:45505
2024-04-19 17:16:17,055 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.79:39933
2024-04-19 17:16:17,055 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.79:39933
2024-04-19 17:16:17,055 - distributed.worker - INFO -          dashboard at:          10.201.3.79:35933
2024-04-19 17:16:17,055 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,055 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,056 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,056 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j6kjhojz
2024-04-19 17:16:17,055 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,056 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,056 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,056 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m0rkvorw
2024-04-19 17:16:17,056 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-23bnmcu2
2024-04-19 17:16:17,056 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,950 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2c67f449-fcb2-407a-a871-f42b6df71954
2024-04-19 17:16:17,950 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-27a321ea-7f5c-4b98-9cce-c01beaedaccd
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.85:38055
2024-04-19 17:16:17,951 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.85:38055
2024-04-19 17:16:17,950 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5dc0297f-e19c-46ac-b607-b6fb9470822a
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.85:42023
2024-04-19 17:16:17,951 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.85:42023
2024-04-19 17:16:17,951 - distributed.worker - INFO -          dashboard at:          10.201.3.85:39503
2024-04-19 17:16:17,951 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,951 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eunzco8x
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.85:45663
2024-04-19 17:16:17,951 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.85:45663
2024-04-19 17:16:17,951 - distributed.worker - INFO -          dashboard at:          10.201.3.85:46723
2024-04-19 17:16:17,951 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,951 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yjr10t8k
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,950 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0a5906d9-9914-4654-8153-022b971e6282
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.85:33209
2024-04-19 17:16:17,951 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.85:33209
2024-04-19 17:16:17,951 - distributed.worker - INFO -          dashboard at:          10.201.3.85:34809
2024-04-19 17:16:17,951 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,951 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vyexz5hj
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -          dashboard at:          10.201.3.85:35531
2024-04-19 17:16:17,951 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.80:8786
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,951 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,951 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r_fe83gt
2024-04-19 17:16:17,951 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,756 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,757 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,757 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,758 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,759 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,759 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,761 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,762 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,762 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,763 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,763 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,763 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,764 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,765 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,765 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,766 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,766 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,766 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,767 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,767 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,767 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,768 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:20,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,768 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.80:8786
2024-04-19 17:16:20,768 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,769 - distributed.core - INFO - Starting established connection to tcp://10.201.3.80:8786
2024-04-19 17:16:30,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,854 - distributed.utils_perf - INFO - full garbage collection released 415.18 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,512 - distributed.utils_perf - INFO - full garbage collection released 1.72 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,140 - distributed.utils_perf - INFO - full garbage collection released 269.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,500 - distributed.utils_perf - INFO - full garbage collection released 837.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,910 - distributed.utils_perf - INFO - full garbage collection released 49.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:40,627 - distributed.utils_perf - INFO - full garbage collection released 2.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,606 - distributed.utils_perf - INFO - full garbage collection released 573.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,762 - distributed.utils_perf - INFO - full garbage collection released 2.86 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:43,934 - distributed.utils_perf - INFO - full garbage collection released 550.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,321 - distributed.utils_perf - INFO - full garbage collection released 1.01 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,230 - distributed.utils_perf - INFO - full garbage collection released 30.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,330 - distributed.utils_perf - INFO - full garbage collection released 1.27 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:52,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,666 - distributed.utils_perf - INFO - full garbage collection released 278.51 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:03,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,526 - distributed.utils_perf - INFO - full garbage collection released 196.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:06,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,990 - distributed.utils_perf - INFO - full garbage collection released 156.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:15,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,797 - distributed.utils_perf - INFO - full garbage collection released 279.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:33,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:35,572 - distributed.utils_perf - INFO - full garbage collection released 492.42 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:36,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,937 - distributed.utils_perf - INFO - full garbage collection released 46.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:44,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,080 - distributed.utils_perf - INFO - full garbage collection released 361.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:49,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,244 - distributed.utils_perf - INFO - full garbage collection released 5.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:52,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,591 - distributed.utils_perf - INFO - full garbage collection released 127.35 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:01,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,034 - distributed.utils_perf - INFO - full garbage collection released 113.74 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:09,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,557 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:12,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:12,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:16,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,076 - distributed.utils_perf - INFO - full garbage collection released 177.56 MiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:18,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:35,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:41,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:44,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:47,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:57,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:57,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,844 - distributed.utils_perf - INFO - full garbage collection released 2.37 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:30,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:31,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:31,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:39,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,356 - distributed.utils_perf - INFO - full garbage collection released 1.36 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:51,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:52,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:56,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,903 - distributed.utils_perf - INFO - full garbage collection released 203.23 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:19,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,481 - distributed.utils_perf - INFO - full garbage collection released 363.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:20,628 - distributed.utils_perf - INFO - full garbage collection released 1.25 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:20,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,637 - distributed.utils_perf - INFO - full garbage collection released 124.40 MiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:25,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 64.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:47,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,292 - distributed.utils_perf - INFO - full garbage collection released 50.87 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:59,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:12,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:21,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:48,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:00,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:02,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:07,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:18,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:18,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:28,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 54.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:51,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,914 - distributed.utils_perf - INFO - full garbage collection released 1.15 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,154 - distributed.utils_perf - INFO - full garbage collection released 112.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:08,111 - distributed.utils_perf - INFO - full garbage collection released 203.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:08,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:25,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,383 - distributed.utils_perf - WARNING - full garbage collections took 58% CPU time recently (threshold: 10%)
2024-04-19 17:23:31,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,742 - distributed.utils_perf - WARNING - full garbage collections took 58% CPU time recently (threshold: 10%)
2024-04-19 17:23:33,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:35,708 - distributed.utils_perf - WARNING - full garbage collections took 58% CPU time recently (threshold: 10%)
2024-04-19 17:23:35,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:41,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,927 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,564 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,349 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:48,317 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:49,544 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:49,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,047 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:23:52,915 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:23:53,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,231 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:23:55,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,769 - distributed.utils_perf - INFO - full garbage collection released 0.97 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:58,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,055 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:58,304 - distributed.utils_perf - INFO - full garbage collection released 1.15 GiB from 55 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:01,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:13,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:14,595 - distributed.utils_perf - INFO - full garbage collection released 6.38 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:18,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:20,358 - distributed.utils_perf - INFO - full garbage collection released 0.98 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:20,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:54,871 - distributed.utils_perf - INFO - full garbage collection released 140.73 MiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:56,205 - distributed.utils_perf - INFO - full garbage collection released 16.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:01,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:02,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:03,037 - distributed.utils_perf - INFO - full garbage collection released 177.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:04,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:10,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:10,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:13,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:14,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:21,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:27,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:30,153 - distributed.utils_perf - INFO - full garbage collection released 795.83 MiB from 171 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:38,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:08] task [xgboost.dask-tcp://10.201.3.79:33147]:tcp://10.201.3.79:33147 got new rank 0
[17:26:08] task [xgboost.dask-tcp://10.201.3.79:39933]:tcp://10.201.3.79:39933 got new rank 1
[17:26:08] task [xgboost.dask-tcp://10.201.3.79:43565]:tcp://10.201.3.79:43565 got new rank 2
[17:26:08] task [xgboost.dask-tcp://10.201.3.79:46505]:tcp://10.201.3.79:46505 got new rank 3
[17:26:09] task [xgboost.dask-tcp://10.201.3.85:33209]:tcp://10.201.3.85:33209 got new rank 4
[17:26:09] task [xgboost.dask-tcp://10.201.3.85:38055]:tcp://10.201.3.85:38055 got new rank 5
[17:26:09] task [xgboost.dask-tcp://10.201.3.85:42023]:tcp://10.201.3.85:42023 got new rank 6
[17:26:09] task [xgboost.dask-tcp://10.201.3.85:45663]:tcp://10.201.3.85:45663 got new rank 7
2024-04-19 17:28:21,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:21,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:21,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:21,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:21,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:22,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:57] task [xgboost.dask-tcp://10.201.3.79:33147]:tcp://10.201.3.79:33147 got new rank 0
[17:28:57] task [xgboost.dask-tcp://10.201.3.79:39933]:tcp://10.201.3.79:39933 got new rank 1
[17:28:57] task [xgboost.dask-tcp://10.201.3.79:43565]:tcp://10.201.3.79:43565 got new rank 2
[17:28:57] task [xgboost.dask-tcp://10.201.3.79:46505]:tcp://10.201.3.79:46505 got new rank 3
[17:28:57] task [xgboost.dask-tcp://10.201.3.85:33209]:tcp://10.201.3.85:33209 got new rank 4
[17:28:57] task [xgboost.dask-tcp://10.201.3.85:38055]:tcp://10.201.3.85:38055 got new rank 5
[17:28:57] task [xgboost.dask-tcp://10.201.3.85:42023]:tcp://10.201.3.85:42023 got new rank 6
[17:28:57] task [xgboost.dask-tcp://10.201.3.85:45663]:tcp://10.201.3.85:45663 got new rank 7
2024-04-19 17:31:07,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:49] task [xgboost.dask-tcp://10.201.3.79:33147]:tcp://10.201.3.79:33147 got new rank 0
[17:31:49] task [xgboost.dask-tcp://10.201.3.79:39933]:tcp://10.201.3.79:39933 got new rank 1
[17:31:49] task [xgboost.dask-tcp://10.201.3.79:43565]:tcp://10.201.3.79:43565 got new rank 2
[17:31:49] task [xgboost.dask-tcp://10.201.3.79:46505]:tcp://10.201.3.79:46505 got new rank 3
[17:31:49] task [xgboost.dask-tcp://10.201.3.85:33209]:tcp://10.201.3.85:33209 got new rank 4
[17:31:49] task [xgboost.dask-tcp://10.201.3.85:38055]:tcp://10.201.3.85:38055 got new rank 5
[17:31:49] task [xgboost.dask-tcp://10.201.3.85:42023]:tcp://10.201.3.85:42023 got new rank 6
[17:31:49] task [xgboost.dask-tcp://10.201.3.85:45663]:tcp://10.201.3.85:45663 got new rank 7
2024-04-19 17:34:40,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:40,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:40,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:40,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:41,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:41,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:20] task [xgboost.dask-tcp://10.201.3.79:33147]:tcp://10.201.3.79:33147 got new rank 0
[17:35:20] task [xgboost.dask-tcp://10.201.3.79:39933]:tcp://10.201.3.79:39933 got new rank 1
[17:35:20] task [xgboost.dask-tcp://10.201.3.79:43565]:tcp://10.201.3.79:43565 got new rank 2
[17:35:20] task [xgboost.dask-tcp://10.201.3.79:46505]:tcp://10.201.3.79:46505 got new rank 3
[17:35:20] task [xgboost.dask-tcp://10.201.3.85:33209]:tcp://10.201.3.85:33209 got new rank 4
[17:35:20] task [xgboost.dask-tcp://10.201.3.85:38055]:tcp://10.201.3.85:38055 got new rank 5
[17:35:20] task [xgboost.dask-tcp://10.201.3.85:42023]:tcp://10.201.3.85:42023 got new rank 6
[17:35:20] task [xgboost.dask-tcp://10.201.3.85:45663]:tcp://10.201.3.85:45663 got new rank 7
2024-04-19 17:37:29,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:42,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:43,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:43,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:44,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:44,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:45,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:45,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:46,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:38:27] task [xgboost.dask-tcp://10.201.3.79:33147]:tcp://10.201.3.79:33147 got new rank 0
[17:38:27] task [xgboost.dask-tcp://10.201.3.79:39933]:tcp://10.201.3.79:39933 got new rank 1
[17:38:27] task [xgboost.dask-tcp://10.201.3.79:43565]:tcp://10.201.3.79:43565 got new rank 2
[17:38:27] task [xgboost.dask-tcp://10.201.3.79:46505]:tcp://10.201.3.79:46505 got new rank 3
[17:38:27] task [xgboost.dask-tcp://10.201.3.85:33209]:tcp://10.201.3.85:33209 got new rank 4
[17:38:27] task [xgboost.dask-tcp://10.201.3.85:38055]:tcp://10.201.3.85:38055 got new rank 5
[17:38:27] task [xgboost.dask-tcp://10.201.3.85:42023]:tcp://10.201.3.85:42023 got new rank 6
[17:38:27] task [xgboost.dask-tcp://10.201.3.85:45663]:tcp://10.201.3.85:45663 got new rank 7
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.79:33147. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.79:43565. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.79:39933. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.79:46505. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.85:45663. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.85:33209. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.85:38055. Reason: scheduler-close
2024-04-19 17:41:02,432 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.85:42023. Reason: scheduler-close
2024-04-19 17:41:02,434 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.85:43003'. Reason: scheduler-close
2024-04-19 17:41:02,435 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.85:39607'. Reason: scheduler-close
2024-04-19 17:41:02,435 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.85:34947'. Reason: scheduler-close
2024-04-19 17:41:02,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42498 remote=tcp://10.201.3.80:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42498 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:02,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42518 remote=tcp://10.201.3.80:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42518 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:02,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42496 remote=tcp://10.201.3.80:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42496 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:02,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42506 remote=tcp://10.201.3.80:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.79:42506 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:02,440 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.79:34747'. Reason: scheduler-close
2024-04-19 17:41:02,440 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.79:43053'. Reason: scheduler-close
2024-04-19 17:41:02,440 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.79:42967'. Reason: scheduler-close
2024-04-19 17:41:02,434 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.85:38142 remote=tcp://10.201.3.80:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.85:38142 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:02,441 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.79:37975'. Reason: scheduler-close
2024-04-19 17:41:02,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.85:41391'. Reason: scheduler-close
2024-04-19 17:41:03,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.79:56428 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:03,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.85:38182 remote=tcp://10.201.3.80:8786>: Stream is closed
/10.201.3.79:52238 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.79:52240 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.79:56412 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.85:59702 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.85:38174 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.85:59672 remote=tcp://10.201.3.80:8786>: Stream is closed
2024-04-19 17:41:03,288 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,289 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,297 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,298 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,302 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,302 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,319 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,320 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,330 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,330 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,332 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,333 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,352 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,352 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:03,358 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.80:8786; closing.
2024-04-19 17:41:03,358 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:05,290 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,304 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,307 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,321 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,341 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,354 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,358 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:05,359 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:06,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.79:34747'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,111 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.79:43053'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,122 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.79:37975'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,130 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.85:41391'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,226 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.85:34947'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,332 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.85:39607'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,338 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,584 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.79:42967'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,585 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:06,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.85:43003'. Reason: nanny-close-gracefully
2024-04-19 17:41:06,714 - distributed.dask_worker - INFO - End worker
