2024-04-19 17:15:59,719 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,720 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,720 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,720 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,749 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,749 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,749 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,750 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,022 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.173:45103'
2024-04-19 17:16:00,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.173:34415'
2024-04-19 17:16:00,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.173:33555'
2024-04-19 17:16:00,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.173:44459'
2024-04-19 17:16:00,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.174:44069'
2024-04-19 17:16:00,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.174:41449'
2024-04-19 17:16:00,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.174:46249'
2024-04-19 17:16:00,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.174:44147'
2024-04-19 17:16:01,187 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,187 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,187 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,187 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,188 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,188 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,188 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,188 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,199 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,199 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,199 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,200 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,200 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,200 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,201 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,201 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,233 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,234 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,234 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,234 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,246 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,246 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,246 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,247 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:02,236 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,236 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,236 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,236 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,242 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,242 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,242 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,243 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:03,302 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b70b21ad-641e-4a79-a23e-e4af14ac934c
2024-04-19 17:16:03,302 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1ec7244f-029c-4d29-8659-5e4acf560833
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.173:42819
2024-04-19 17:16:03,302 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.173:42819
2024-04-19 17:16:03,302 - distributed.worker - INFO -          dashboard at:         10.201.2.173:36845
2024-04-19 17:16:03,302 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-eb73ef26-86d4-49ea-80bb-d3c9b1991c77
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.173:45251
2024-04-19 17:16:03,302 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.173:45251
2024-04-19 17:16:03,302 - distributed.worker - INFO -          dashboard at:         10.201.2.173:43013
2024-04-19 17:16:03,302 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,302 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-34e0abda-63e3-4555-a702-0855015a703e
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.173:44457
2024-04-19 17:16:03,302 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.173:44457
2024-04-19 17:16:03,302 - distributed.worker - INFO -          dashboard at:         10.201.2.173:46197
2024-04-19 17:16:03,302 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.173:39269
2024-04-19 17:16:03,302 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.173:39269
2024-04-19 17:16:03,302 - distributed.worker - INFO -          dashboard at:         10.201.2.173:35979
2024-04-19 17:16:03,302 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,302 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n28lhly1
2024-04-19 17:16:03,302 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,302 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m8kntytk
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qe1mhf5b
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,302 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,302 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9uj0ba18
2024-04-19 17:16:03,302 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,303 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-35ae43df-8c9c-4e18-b9c5-3a975b58e228
2024-04-19 17:16:03,314 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-471602bc-0de6-4119-8265-eb7b55adf69d
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.174:37923
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.174:34599
2024-04-19 17:16:03,314 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.174:34599
2024-04-19 17:16:03,314 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.174:37923
2024-04-19 17:16:03,314 - distributed.worker - INFO -          dashboard at:         10.201.2.174:44837
2024-04-19 17:16:03,314 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-28b22093-4bd7-458d-9ab4-6a58636bbab8
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.174:37141
2024-04-19 17:16:03,314 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.174:37141
2024-04-19 17:16:03,314 - distributed.worker - INFO -          dashboard at:         10.201.2.174:37635
2024-04-19 17:16:03,314 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,314 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8_fw3woe
2024-04-19 17:16:03,314 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5ef4bfde-f4ce-4165-8ca8-d04004ed4ad0
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.174:44355
2024-04-19 17:16:03,314 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.174:44355
2024-04-19 17:16:03,314 - distributed.worker - INFO -          dashboard at:         10.201.2.174:42453
2024-04-19 17:16:03,314 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,314 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkr9coyw
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO -          dashboard at:         10.201.2.174:37331
2024-04-19 17:16:03,314 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,314 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.176:8786
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,314 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jf0uzp11
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,314 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,314 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7p_qkhw0
2024-04-19 17:16:03,315 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,116 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,116 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,117 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,118 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,118 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,119 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,121 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,121 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,121 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,122 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,122 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,123 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,124 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,124 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,125 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,125 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,125 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,126 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,126 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,126 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,126 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.176:8786
2024-04-19 17:16:07,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,127 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:07,127 - distributed.core - INFO - Starting established connection to tcp://10.201.2.176:8786
2024-04-19 17:16:16,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:20,519 - distributed.utils_perf - INFO - full garbage collection released 391.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:22,881 - distributed.utils_perf - INFO - full garbage collection released 1.26 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,410 - distributed.utils_perf - INFO - full garbage collection released 14.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,502 - distributed.utils_perf - INFO - full garbage collection released 89.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,130 - distributed.utils_perf - INFO - full garbage collection released 430.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,189 - distributed.utils_perf - INFO - full garbage collection released 146.80 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,546 - distributed.utils_perf - INFO - full garbage collection released 442.78 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:25,235 - distributed.utils_perf - INFO - full garbage collection released 1.56 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:25,695 - distributed.utils_perf - INFO - full garbage collection released 5.15 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:26,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,344 - distributed.utils_perf - INFO - full garbage collection released 1.24 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:29,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,287 - distributed.utils_perf - INFO - full garbage collection released 2.55 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,387 - distributed.utils_perf - INFO - full garbage collection released 2.27 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:31,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,263 - distributed.utils_perf - INFO - full garbage collection released 1.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:33,990 - distributed.utils_perf - INFO - full garbage collection released 1.22 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:34,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:37,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:37,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:45,183 - distributed.utils_perf - INFO - full garbage collection released 891.99 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,365 - distributed.utils_perf - INFO - full garbage collection released 1.76 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,198 - distributed.utils_perf - INFO - full garbage collection released 211.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:53,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,218 - distributed.utils_perf - INFO - full garbage collection released 78.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:56,341 - distributed.utils_perf - INFO - full garbage collection released 407.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:56,931 - distributed.utils_perf - INFO - full garbage collection released 253.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:57,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,093 - distributed.utils_perf - INFO - full garbage collection released 649.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:59,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,817 - distributed.utils_perf - INFO - full garbage collection released 744.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:04,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,028 - distributed.utils_perf - INFO - full garbage collection released 205.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:08,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:11,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,430 - distributed.utils_perf - INFO - full garbage collection released 330.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:22,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,006 - distributed.utils_perf - INFO - full garbage collection released 364.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:24,636 - distributed.utils_perf - INFO - full garbage collection released 146.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:24,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,838 - distributed.utils_perf - INFO - full garbage collection released 536.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:30,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:38,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:38,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:40,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:41,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,141 - distributed.utils_perf - INFO - full garbage collection released 100.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:48,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,723 - distributed.utils_perf - INFO - full garbage collection released 0.90 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:03,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,874 - distributed.utils_perf - INFO - full garbage collection released 245.66 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:07,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:16,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:19,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:21,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:49,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:49,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,682 - distributed.utils_perf - INFO - full garbage collection released 770.38 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:52,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,791 - distributed.utils_perf - INFO - full garbage collection released 1.93 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:54,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:01,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:19,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,264 - distributed.utils_perf - INFO - full garbage collection released 2.78 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:24,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,958 - distributed.utils_perf - INFO - full garbage collection released 665.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:32,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,460 - distributed.utils_perf - INFO - full garbage collection released 213.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:37,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:37,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,313 - distributed.utils_perf - INFO - full garbage collection released 174.82 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:54,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:12,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:27,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:27,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:48,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:49,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:50,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:02,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,351 - distributed.utils_perf - INFO - full garbage collection released 296.03 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:17,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:28,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,102 - distributed.utils_perf - INFO - full garbage collection released 3.21 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:46,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:51,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,613 - distributed.utils_perf - INFO - full garbage collection released 3.05 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:19,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:36,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,378 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:02,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:13,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:14,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:31,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,532 - distributed.utils_perf - INFO - full garbage collection released 686.07 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:39,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:42,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:45,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:49,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:54,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:57,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:01,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:02,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:09,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:12,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:23,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:32,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:03] task [xgboost.dask-tcp://10.201.2.173:39269]:tcp://10.201.2.173:39269 got new rank 0
[17:26:03] task [xgboost.dask-tcp://10.201.2.173:42819]:tcp://10.201.2.173:42819 got new rank 1
[17:26:03] task [xgboost.dask-tcp://10.201.2.173:44457]:tcp://10.201.2.173:44457 got new rank 2
[17:26:03] task [xgboost.dask-tcp://10.201.2.173:45251]:tcp://10.201.2.173:45251 got new rank 3
[17:26:03] task [xgboost.dask-tcp://10.201.2.174:34599]:tcp://10.201.2.174:34599 got new rank 4
[17:26:03] task [xgboost.dask-tcp://10.201.2.174:37141]:tcp://10.201.2.174:37141 got new rank 5
[17:26:03] task [xgboost.dask-tcp://10.201.2.174:37923]:tcp://10.201.2.174:37923 got new rank 6
[17:26:03] task [xgboost.dask-tcp://10.201.2.174:44355]:tcp://10.201.2.174:44355 got new rank 7
2024-04-19 17:28:11,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:11,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:11,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:11,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:12,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:12,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:56] task [xgboost.dask-tcp://10.201.2.173:39269]:tcp://10.201.2.173:39269 got new rank 0
[17:28:56] task [xgboost.dask-tcp://10.201.2.173:42819]:tcp://10.201.2.173:42819 got new rank 1
[17:28:56] task [xgboost.dask-tcp://10.201.2.173:44457]:tcp://10.201.2.173:44457 got new rank 2
[17:28:56] task [xgboost.dask-tcp://10.201.2.173:45251]:tcp://10.201.2.173:45251 got new rank 3
[17:28:56] task [xgboost.dask-tcp://10.201.2.174:34599]:tcp://10.201.2.174:34599 got new rank 4
[17:28:56] task [xgboost.dask-tcp://10.201.2.174:37141]:tcp://10.201.2.174:37141 got new rank 5
[17:28:56] task [xgboost.dask-tcp://10.201.2.174:37923]:tcp://10.201.2.174:37923 got new rank 6
[17:28:56] task [xgboost.dask-tcp://10.201.2.174:44355]:tcp://10.201.2.174:44355 got new rank 7
2024-04-19 17:31:13,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:14,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:14,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:50] task [xgboost.dask-tcp://10.201.2.173:39269]:tcp://10.201.2.173:39269 got new rank 0
[17:31:50] task [xgboost.dask-tcp://10.201.2.173:42819]:tcp://10.201.2.173:42819 got new rank 1
[17:31:50] task [xgboost.dask-tcp://10.201.2.173:44457]:tcp://10.201.2.173:44457 got new rank 2
[17:31:50] task [xgboost.dask-tcp://10.201.2.173:45251]:tcp://10.201.2.173:45251 got new rank 3
[17:31:50] task [xgboost.dask-tcp://10.201.2.174:34599]:tcp://10.201.2.174:34599 got new rank 4
[17:31:50] task [xgboost.dask-tcp://10.201.2.174:37141]:tcp://10.201.2.174:37141 got new rank 5
[17:31:50] task [xgboost.dask-tcp://10.201.2.174:37923]:tcp://10.201.2.174:37923 got new rank 6
[17:31:50] task [xgboost.dask-tcp://10.201.2.174:44355]:tcp://10.201.2.174:44355 got new rank 7
[17:34:53] task [xgboost.dask-tcp://10.201.2.173:39269]:tcp://10.201.2.173:39269 got new rank 0
[17:34:53] task [xgboost.dask-tcp://10.201.2.173:42819]:tcp://10.201.2.173:42819 got new rank 1
[17:34:53] task [xgboost.dask-tcp://10.201.2.173:44457]:tcp://10.201.2.173:44457 got new rank 2
[17:34:53] task [xgboost.dask-tcp://10.201.2.173:45251]:tcp://10.201.2.173:45251 got new rank 3
[17:34:53] task [xgboost.dask-tcp://10.201.2.174:34599]:tcp://10.201.2.174:34599 got new rank 4
[17:34:53] task [xgboost.dask-tcp://10.201.2.174:37141]:tcp://10.201.2.174:37141 got new rank 5
[17:34:53] task [xgboost.dask-tcp://10.201.2.174:37923]:tcp://10.201.2.174:37923 got new rank 6
[17:34:53] task [xgboost.dask-tcp://10.201.2.174:44355]:tcp://10.201.2.174:44355 got new rank 7
2024-04-19 17:37:53,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:54,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:55,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:55,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:55,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:56,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:56,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:38:39] task [xgboost.dask-tcp://10.201.2.173:39269]:tcp://10.201.2.173:39269 got new rank 0
[17:38:39] task [xgboost.dask-tcp://10.201.2.173:42819]:tcp://10.201.2.173:42819 got new rank 1
[17:38:39] task [xgboost.dask-tcp://10.201.2.173:44457]:tcp://10.201.2.173:44457 got new rank 2
[17:38:39] task [xgboost.dask-tcp://10.201.2.173:45251]:tcp://10.201.2.173:45251 got new rank 3
[17:38:39] task [xgboost.dask-tcp://10.201.2.174:34599]:tcp://10.201.2.174:34599 got new rank 4
[17:38:39] task [xgboost.dask-tcp://10.201.2.174:37141]:tcp://10.201.2.174:37141 got new rank 5
[17:38:39] task [xgboost.dask-tcp://10.201.2.174:37923]:tcp://10.201.2.174:37923 got new rank 6
[17:38:39] task [xgboost.dask-tcp://10.201.2.174:44355]:tcp://10.201.2.174:44355 got new rank 7
2024-04-19 17:41:11,504 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.173:44457. Reason: scheduler-close
2024-04-19 17:41:11,505 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.174:34599. Reason: scheduler-close
2024-04-19 17:41:11,504 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.173:39269. Reason: scheduler-close
2024-04-19 17:41:11,504 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.173:42819. Reason: scheduler-close
2024-04-19 17:41:11,505 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.174:37923. Reason: scheduler-close
2024-04-19 17:41:11,505 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.173:45251. Reason: scheduler-close
2024-04-19 17:41:11,505 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.174:37141. Reason: scheduler-close
2024-04-19 17:41:11,505 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.174:44355. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.174:44147'. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.174:41449'. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.173:44459'. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.173:34415'. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.173:33555'. Reason: scheduler-close
2024-04-19 17:41:11,507 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.173:45103'. Reason: scheduler-close
2024-04-19 17:41:11,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.174:41350 remote=tcp://10.201.2.176:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.174:41350 remote=tcp://10.201.2.176:8786>: Stream is closed
2024-04-19 17:41:11,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.174:41352 remote=tcp://10.201.2.176:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.174:41352 remote=tcp://10.201.2.176:8786>: Stream is closed
2024-04-19 17:41:11,513 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.174:46249'. Reason: scheduler-close
2024-04-19 17:41:11,513 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.174:44069'. Reason: scheduler-close
2024-04-19 17:41:12,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.174:59262 remote=tcp://10.201.2.176:8786>: Stream is closed
2024-04-19 17:41:12,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.174:41386 remote=tcp://10.201.2.176:8786>: Stream is closed
/10.201.2.174:59240 remote=tcp://10.201.2.176:8786>: Stream is closed
/10.201.2.174:41414 remote=tcp://10.201.2.176:8786>: Stream is closed
2024-04-19 17:41:12,299 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,298 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,298 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,298 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.173:40196 remote=tcp://10.201.2.176:8786>: Stream is closed
/10.201.2.173:40188 remote=tcp://10.201.2.176:8786>: Stream is closed
/10.201.2.173:41570 remote=tcp://10.201.2.176:8786>: Stream is closed
/10.201.2.173:40176 remote=tcp://10.201.2.176:8786>: Stream is closed
2024-04-19 17:41:12,339 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,339 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,394 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,394 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,397 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,398 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,399 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,399 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,402 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,402 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,410 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,411 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,414 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,415 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,433 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.176:8786; closing.
2024-04-19 17:41:12,434 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:14,342 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,396 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,401 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,401 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,412 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,416 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,435 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:15,323 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.174:41449'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,324 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.174:44069'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,430 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,435 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.174:44147'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,436 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,782 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.173:44459'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,782 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,785 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.174:46249'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,786 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,790 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.173:45103'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,791 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.173:34415'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,799 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:16,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.173:33555'. Reason: nanny-close-gracefully
2024-04-19 17:41:16,266 - distributed.dask_worker - INFO - End worker
