2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,135 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,135 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,134 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,309 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,309 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,310 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,310 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.213:35747'
2024-04-19 16:13:13,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.213:34851'
2024-04-19 16:13:13,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.213:43185'
2024-04-19 16:13:13,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.213:43253'
2024-04-19 16:13:13,414 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,414 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,414 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,415 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.225:35559'
2024-04-19 16:13:13,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.225:44871'
2024-04-19 16:13:13,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.225:35219'
2024-04-19 16:13:13,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.225:39361'
2024-04-19 16:13:14,307 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,307 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,307 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,307 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,308 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,308 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,308 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,309 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,352 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,352 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,352 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,352 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,402 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,402 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,402 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,402 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,403 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,403 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,403 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,403 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,446 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,447 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,447 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,447 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:15,313 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,313 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,313 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,313 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,407 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,407 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,407 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,407 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:16,315 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-82a577a0-0232-4ad6-b81f-d942e7e0e361
2024-04-19 16:13:16,315 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-26f4295e-72a1-4040-b145-1ca7809d3d4d
2024-04-19 16:13:16,316 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.213:38967
2024-04-19 16:13:16,316 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.213:38967
2024-04-19 16:13:16,316 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.213:41017
2024-04-19 16:13:16,316 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.213:41017
2024-04-19 16:13:16,316 - distributed.worker - INFO -          dashboard at:         10.201.3.213:45097
2024-04-19 16:13:16,316 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,316 - distributed.worker - INFO -          dashboard at:         10.201.3.213:39287
2024-04-19 16:13:16,316 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,316 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,316 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,316 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,316 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,316 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,316 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,316 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vfzxeeev
2024-04-19 16:13:16,316 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,316 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-myqirt2e
2024-04-19 16:13:16,316 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,398 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9e0da040-e5cd-4861-9e77-699fef736751
2024-04-19 16:13:16,398 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.213:36179
2024-04-19 16:13:16,398 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.213:36179
2024-04-19 16:13:16,398 - distributed.worker - INFO -          dashboard at:         10.201.3.213:32939
2024-04-19 16:13:16,398 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,398 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,398 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,398 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-irgjy9bs
2024-04-19 16:13:16,399 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,402 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-fe7c9615-58d6-41f6-aef6-54d668b2f049
2024-04-19 16:13:16,402 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.213:46169
2024-04-19 16:13:16,402 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.213:46169
2024-04-19 16:13:16,402 - distributed.worker - INFO -          dashboard at:         10.201.3.213:41579
2024-04-19 16:13:16,402 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,402 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,402 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,403 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,403 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n1alpps0
2024-04-19 16:13:16,403 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,506 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-55bb6e7b-6d37-41a7-89e8-f721dd44d4ff
2024-04-19 16:13:16,506 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c1215c4e-2fa8-4bd7-bda1-7f47feafc3d9
2024-04-19 16:13:16,506 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.225:46561
2024-04-19 16:13:16,506 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.225:46561
2024-04-19 16:13:16,506 - distributed.worker - INFO -          dashboard at:         10.201.3.225:46881
2024-04-19 16:13:16,506 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,506 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,506 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,506 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,506 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.225:42281
2024-04-19 16:13:16,506 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.225:42281
2024-04-19 16:13:16,507 - distributed.worker - INFO -          dashboard at:         10.201.3.225:46463
2024-04-19 16:13:16,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9na3ndd3
2024-04-19 16:13:16,506 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4b0d0f9b-0ec6-4065-808d-1e00c690f09c
2024-04-19 16:13:16,507 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.225:39929
2024-04-19 16:13:16,507 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,507 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.225:39929
2024-04-19 16:13:16,507 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4fn_d5o0
2024-04-19 16:13:16,507 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5c380630-7c15-4372-906f-695f02e34508
2024-04-19 16:13:16,507 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.225:35345
2024-04-19 16:13:16,507 - distributed.worker - INFO -          dashboard at:         10.201.3.225:38131
2024-04-19 16:13:16,507 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.225:35345
2024-04-19 16:13:16,507 - distributed.worker - INFO -          dashboard at:         10.201.3.225:39455
2024-04-19 16:13:16,507 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.217:8786
2024-04-19 16:13:16,507 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9gg1ay0p
2024-04-19 16:13:16,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,507 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,508 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zegr9o6b
2024-04-19 16:13:16,508 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,681 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,681 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,681 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,682 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,682 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,683 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,683 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,683 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,684 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,684 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,684 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,685 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,753 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,753 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,754 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,754 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,754 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,754 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,755 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,755 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,755 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,756 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:18,756 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.217:8786
2024-04-19 16:13:18,756 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,757 - distributed.core - INFO - Starting established connection to tcp://10.201.3.217:8786
2024-04-19 16:13:22,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:25,970 - distributed.utils_perf - INFO - full garbage collection released 532.94 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:27,000 - distributed.utils_perf - INFO - full garbage collection released 208.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:28,528 - distributed.utils_perf - INFO - full garbage collection released 430.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:30,141 - distributed.utils_perf - INFO - full garbage collection released 1.04 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:30,639 - distributed.utils_perf - INFO - full garbage collection released 150.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:32,649 - distributed.utils_perf - INFO - full garbage collection released 2.48 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:32,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:33,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:34,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:36,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:36,610 - distributed.utils_perf - INFO - full garbage collection released 5.45 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:39,008 - distributed.utils_perf - INFO - full garbage collection released 58.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:39,188 - distributed.utils_perf - INFO - full garbage collection released 26.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:39,377 - distributed.utils_perf - INFO - full garbage collection released 222.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:40,076 - distributed.utils_perf - INFO - full garbage collection released 1.38 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:40,489 - distributed.utils_perf - INFO - full garbage collection released 452.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:40,624 - distributed.utils_perf - INFO - full garbage collection released 358.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:41,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:42,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:43,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:45,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:45,403 - distributed.utils_perf - INFO - full garbage collection released 23.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:46,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:47,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:47,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:47,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:52,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:55,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:58,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:58,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:58,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:59,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:59,830 - distributed.utils_perf - INFO - full garbage collection released 315.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:00,953 - distributed.utils_perf - INFO - full garbage collection released 359.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:01,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:02,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:03,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:03,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:03,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:11,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:11,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:14,090 - distributed.utils_perf - INFO - full garbage collection released 9.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:14,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:15,068 - distributed.utils_perf - INFO - full garbage collection released 512.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:16,244 - distributed.utils_perf - INFO - full garbage collection released 361.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:16,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:16,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:20,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:21,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:21,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:21,686 - distributed.utils_perf - INFO - full garbage collection released 48.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:23,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:24,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:24,026 - distributed.utils_perf - INFO - full garbage collection released 411.80 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:24,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:32,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:34,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:37,514 - distributed.utils_perf - INFO - full garbage collection released 91.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:41,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:43,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:46,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:47,496 - distributed.utils_perf - INFO - full garbage collection released 195.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:50,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:51,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:52,290 - distributed.utils_perf - INFO - full garbage collection released 1.05 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:53,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:54,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:59,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:01,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:01,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:02,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:03,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:05,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:07,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:08,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:09,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:09,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:12,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:16,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:19,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:20,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:24,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:25,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:27,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:28,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:30,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:32,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:33,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:35,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:36,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:37,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:39,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:41,504 - distributed.utils_perf - INFO - full garbage collection released 97.99 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:15:42,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:43,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:44,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:48,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:50,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:52,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:52,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:53,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:59,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:00,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:00,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:06,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:07,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:07,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:10,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:10,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:14,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:14,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:15,343 - distributed.utils_perf - INFO - full garbage collection released 1.36 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:19,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:24,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:24,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:24,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:25,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:31,204 - distributed.utils_perf - INFO - full garbage collection released 1.57 GiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:33,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:40,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:42,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:45,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:46,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:47,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:48,145 - distributed.utils_perf - INFO - full garbage collection released 115.27 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:48,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:51,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:53,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:53,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:54,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:54,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:02,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:02,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:03,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:53,734 - distributed.utils_perf - INFO - full garbage collection released 3.01 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:17:03,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:09,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:13,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:15,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:18,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:19,622 - distributed.utils_perf - INFO - full garbage collection released 1.39 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:17:22,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:23,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:23,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:25,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:25,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:27,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:29,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:30,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:34,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:36,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:36,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:44,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:45,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:47,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:49,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:50,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:52,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:53,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:00,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:01,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:04,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:04,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:05,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:05,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:07,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:18,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:19,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:20,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:21,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:24,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:29,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:30,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:30,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:33,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:34,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:34,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:34,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:36,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:43,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:43,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:44,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:46,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:48,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:50,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:50,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:52,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:53,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:54,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:57,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:59,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:59,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:03,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:05,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:07,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:07,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:08,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:14,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:18,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:23,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:24,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:25,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:26,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:27,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:27,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:31,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:32,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:33,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:35,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:37,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:38,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:38,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:39,834 - distributed.utils_perf - INFO - full garbage collection released 1.10 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:19:40,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:42,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:44,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:45,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:46,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:47,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:48,896 - distributed.utils_perf - INFO - full garbage collection released 34.71 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:19:50,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:52,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:57,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:58,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:59,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:59,775 - distributed.utils_perf - INFO - full garbage collection released 419.36 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:20:01,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:02,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:03,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:07,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:08,949 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 16:20:11,674 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 16:20:14,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:20,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:21,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:27,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:28,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:28,444 - distributed.utils_perf - INFO - full garbage collection released 1.55 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:20:32,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:32,760 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 16:20:33,865 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 16:20:34,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:34,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:35,239 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 16:20:36,971 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 16:20:39,125 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 16:20:39,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:39,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:42,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:44,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:46,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:49,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:50,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:53,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:00,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:01,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:04,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:06,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:09,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:10,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:16,698 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 16:21:19,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:20,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:24,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:24,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:27,094 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:27,971 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:29,059 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:29,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:30,399 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:30,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:32,088 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:32,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:34,185 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 16:21:42,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:45,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:52,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:55,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:56,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:58,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:00,884 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 16:22:02,534 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 16:22:04,591 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 16:22:06,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:08,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:11,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:13,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:20,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:26,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:27,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:31,621 - distributed.utils_perf - INFO - full garbage collection released 10.79 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:22:34,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:36,877 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:22:39,518 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:22:39,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:42,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:42,809 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:22:45,740 - distributed.utils_perf - INFO - full garbage collection released 449.04 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:22:46,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:48,850 - distributed.utils_perf - INFO - full garbage collection released 11.84 MiB from 152 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:22:49,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:50,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:55,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:01,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:02,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:22,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:30,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:43,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:51,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:57,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:24:03,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:24:19,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:24:27,691 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:24:29,792 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 16:24:32,396 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 16:24:39,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:25:01,418 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
[16:25:11] task [xgboost.dask-tcp://10.201.3.213:36179]:tcp://10.201.3.213:36179 got new rank 0
[16:25:11] task [xgboost.dask-tcp://10.201.3.213:38967]:tcp://10.201.3.213:38967 got new rank 1
[16:25:11] task [xgboost.dask-tcp://10.201.3.213:41017]:tcp://10.201.3.213:41017 got new rank 2
[16:25:11] task [xgboost.dask-tcp://10.201.3.213:46169]:tcp://10.201.3.213:46169 got new rank 3
[16:25:11] task [xgboost.dask-tcp://10.201.3.225:35345]:tcp://10.201.3.225:35345 got new rank 4
[16:25:11] task [xgboost.dask-tcp://10.201.3.225:39929]:tcp://10.201.3.225:39929 got new rank 5
[16:25:11] task [xgboost.dask-tcp://10.201.3.225:42281]:tcp://10.201.3.225:42281 got new rank 6
[16:25:11] task [xgboost.dask-tcp://10.201.3.225:46561]:tcp://10.201.3.225:46561 got new rank 7
2024-04-19 16:27:12,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:27:12,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:27:13,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:27:28,985 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
[16:27:57] task [xgboost.dask-tcp://10.201.3.213:36179]:tcp://10.201.3.213:36179 got new rank 0
[16:27:57] task [xgboost.dask-tcp://10.201.3.213:38967]:tcp://10.201.3.213:38967 got new rank 1
[16:27:57] task [xgboost.dask-tcp://10.201.3.213:41017]:tcp://10.201.3.213:41017 got new rank 2
[16:27:58] task [xgboost.dask-tcp://10.201.3.213:46169]:tcp://10.201.3.213:46169 got new rank 3
[16:27:58] task [xgboost.dask-tcp://10.201.3.225:35345]:tcp://10.201.3.225:35345 got new rank 4
[16:27:58] task [xgboost.dask-tcp://10.201.3.225:39929]:tcp://10.201.3.225:39929 got new rank 5
[16:27:58] task [xgboost.dask-tcp://10.201.3.225:42281]:tcp://10.201.3.225:42281 got new rank 6
[16:27:58] task [xgboost.dask-tcp://10.201.3.225:46561]:tcp://10.201.3.225:46561 got new rank 7
2024-04-19 16:30:32,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:30:32,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:30:36,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:30:55,594 - distributed.utils_perf - WARNING - full garbage collections took 25% CPU time recently (threshold: 10%)
[16:31:13] task [xgboost.dask-tcp://10.201.3.213:36179]:tcp://10.201.3.213:36179 got new rank 0
[16:31:13] task [xgboost.dask-tcp://10.201.3.213:38967]:tcp://10.201.3.213:38967 got new rank 1
[16:31:13] task [xgboost.dask-tcp://10.201.3.213:41017]:tcp://10.201.3.213:41017 got new rank 2
[16:31:13] task [xgboost.dask-tcp://10.201.3.213:46169]:tcp://10.201.3.213:46169 got new rank 3
[16:31:13] task [xgboost.dask-tcp://10.201.3.225:35345]:tcp://10.201.3.225:35345 got new rank 4
[16:31:13] task [xgboost.dask-tcp://10.201.3.225:39929]:tcp://10.201.3.225:39929 got new rank 5
[16:31:13] task [xgboost.dask-tcp://10.201.3.225:42281]:tcp://10.201.3.225:42281 got new rank 6
[16:31:13] task [xgboost.dask-tcp://10.201.3.225:46561]:tcp://10.201.3.225:46561 got new rank 7
2024-04-19 16:33:43,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:33:45,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:34:07,186 - distributed.utils_perf - WARNING - full garbage collections took 21% CPU time recently (threshold: 10%)
[16:34:30] task [xgboost.dask-tcp://10.201.3.213:36179]:tcp://10.201.3.213:36179 got new rank 0
[16:34:30] task [xgboost.dask-tcp://10.201.3.213:38967]:tcp://10.201.3.213:38967 got new rank 1
[16:34:30] task [xgboost.dask-tcp://10.201.3.213:41017]:tcp://10.201.3.213:41017 got new rank 2
[16:34:30] task [xgboost.dask-tcp://10.201.3.213:46169]:tcp://10.201.3.213:46169 got new rank 3
[16:34:30] task [xgboost.dask-tcp://10.201.3.225:35345]:tcp://10.201.3.225:35345 got new rank 4
[16:34:30] task [xgboost.dask-tcp://10.201.3.225:39929]:tcp://10.201.3.225:39929 got new rank 5
[16:34:30] task [xgboost.dask-tcp://10.201.3.225:42281]:tcp://10.201.3.225:42281 got new rank 6
[16:34:30] task [xgboost.dask-tcp://10.201.3.225:46561]:tcp://10.201.3.225:46561 got new rank 7
2024-04-19 16:37:08,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:08,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:08,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:09,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:23,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:23,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:23,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:24,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:24,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:25,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:37:26,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:38:08] task [xgboost.dask-tcp://10.201.3.213:36179]:tcp://10.201.3.213:36179 got new rank 0
[16:38:08] task [xgboost.dask-tcp://10.201.3.213:38967]:tcp://10.201.3.213:38967 got new rank 1
[16:38:08] task [xgboost.dask-tcp://10.201.3.213:41017]:tcp://10.201.3.213:41017 got new rank 2
[16:38:08] task [xgboost.dask-tcp://10.201.3.213:46169]:tcp://10.201.3.213:46169 got new rank 3
[16:38:08] task [xgboost.dask-tcp://10.201.3.225:35345]:tcp://10.201.3.225:35345 got new rank 4
[16:38:08] task [xgboost.dask-tcp://10.201.3.225:39929]:tcp://10.201.3.225:39929 got new rank 5
[16:38:08] task [xgboost.dask-tcp://10.201.3.225:42281]:tcp://10.201.3.225:42281 got new rank 6
[16:38:08] task [xgboost.dask-tcp://10.201.3.225:46561]:tcp://10.201.3.225:46561 got new rank 7
2024-04-19 16:39:49,893 - distributed.utils_perf - WARNING - full garbage collections took 21% CPU time recently (threshold: 10%)
2024-04-19 16:40:33,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:41:10,521 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.225:39929. Reason: scheduler-close
2024-04-19 16:41:10,521 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.225:35345. Reason: scheduler-close
2024-04-19 16:41:10,520 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.213:41017. Reason: scheduler-close
2024-04-19 16:41:10,521 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.225:42281. Reason: scheduler-close
2024-04-19 16:41:10,520 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.213:38967. Reason: scheduler-close
2024-04-19 16:41:10,521 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.225:46561. Reason: scheduler-close
2024-04-19 16:41:10,520 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.213:46169. Reason: scheduler-close
2024-04-19 16:41:10,520 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.213:36179. Reason: scheduler-close
2024-04-19 16:41:10,523 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.213:43253'. Reason: scheduler-close
2024-04-19 16:41:10,523 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.213:35747'. Reason: scheduler-close
2024-04-19 16:41:10,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40600 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40600 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40584 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40584 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40570 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40570 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40560 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.225:40560 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,525 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.225:35219'. Reason: scheduler-close
2024-04-19 16:41:10,525 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.225:35559'. Reason: scheduler-close
2024-04-19 16:41:10,525 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.225:44871'. Reason: scheduler-close
2024-04-19 16:41:10,526 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.225:39361'. Reason: scheduler-close
2024-04-19 16:41:10,522 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.213:57762 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.213:57762 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,522 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.213:57744 remote=tcp://10.201.3.217:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.213:57744 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:10,531 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.213:34851'. Reason: scheduler-close
2024-04-19 16:41:10,531 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.213:43185'. Reason: scheduler-close
2024-04-19 16:41:11,326 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.225:47322 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:11,327 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.225:58544 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:11,327 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:41:11,327 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.225:47332 remote=tcp://10.201.3.217:8786>: Stream is closed
/10.201.3.225:58520 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:11,330 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,330 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,333 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,333 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,333 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:41:11,336 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
/10.201.3.213:56940 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:11,336 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,334 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:41:11,334 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:41:11,334 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.213:57784 remote=tcp://10.201.3.217:8786>: Stream is closed
/10.201.3.213:57800 remote=tcp://10.201.3.217:8786>: Stream is closed
/10.201.3.213:57790 remote=tcp://10.201.3.217:8786>: Stream is closed
2024-04-19 16:41:11,342 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,343 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,428 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,429 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,453 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,453 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,465 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,465 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:11,471 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.217:8786; closing.
2024-04-19 16:41:11,472 - distributed.nanny - INFO - Worker closed
2024-04-19 16:41:13,406 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:13,408 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:13,432 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:13,442 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:13,455 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:13,467 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:41:14,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.225:35559'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,103 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.225:44871'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,142 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.225:39361'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,150 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,601 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.225:35219'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,602 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.213:43185'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,806 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.213:43253'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,814 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:14,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.213:35747'. Reason: nanny-close-gracefully
2024-04-19 16:41:14,820 - distributed.dask_worker - INFO - End worker
2024-04-19 16:41:15,281 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.213:34851'. Reason: nanny-close-gracefully
2024-04-19 16:41:15,282 - distributed.dask_worker - INFO - End worker
