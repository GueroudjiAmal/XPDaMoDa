2024-04-19 17:15:59,376 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,377 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,377 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,377 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,382 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,382 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,382 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,382 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:40813'
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:37353'
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:45083'
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:34879'
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:37167'
2024-04-19 17:15:59,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:40969'
2024-04-19 17:15:59,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:42921'
2024-04-19 17:15:59,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:37631'
2024-04-19 17:16:00,788 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,789 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,789 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,789 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,789 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,790 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,790 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,790 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,792 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,793 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,793 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,793 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,794 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,835 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,835 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,837 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,837 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,850 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,850 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,850 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,850 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,856 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,856 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,856 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,856 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,898 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2f3bd6d9-1918-45b1-a694-847a13fca28a
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:39457
2024-04-19 17:16:02,898 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4400394b-2298-4c8e-bfe7-878b3208f69e
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:41723
2024-04-19 17:16:02,898 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:41723
2024-04-19 17:16:02,898 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0eeaf98e-d359-4f72-9873-328a1637ef2d
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:36191
2024-04-19 17:16:02,898 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:36191
2024-04-19 17:16:02,898 - distributed.worker - INFO -          dashboard at:         10.201.2.159:46647
2024-04-19 17:16:02,898 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,898 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-638d6e89-c430-4b77-9f7a-65e831e545e4
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:37297
2024-04-19 17:16:02,898 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:37297
2024-04-19 17:16:02,898 - distributed.worker - INFO -          dashboard at:         10.201.2.159:46445
2024-04-19 17:16:02,898 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,898 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:39457
2024-04-19 17:16:02,898 - distributed.worker - INFO -          dashboard at:         10.201.2.159:37827
2024-04-19 17:16:02,898 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,898 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fpozy3ko
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -          dashboard at:         10.201.2.159:45283
2024-04-19 17:16:02,898 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,898 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p15wf3f2
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,898 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uvp5px_l
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,898 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fpzbhlvg
2024-04-19 17:16:02,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,941 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-94a56f7e-0b2e-45fd-b8b9-2be271f9d0fb
2024-04-19 17:16:02,942 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b24d9a46-0578-4bca-b142-b14efcd39041
2024-04-19 17:16:02,942 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0fe3b4cd-b09e-43ec-a83a-d029484d420d
2024-04-19 17:16:02,942 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2e037b80-b16c-4f08-af6a-600849ad9aff
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:41095
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:41105
2024-04-19 17:16:02,942 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:41105
2024-04-19 17:16:02,942 - distributed.worker - INFO -          dashboard at:         10.201.2.163:41867
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:33329
2024-04-19 17:16:02,942 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:33329
2024-04-19 17:16:02,942 - distributed.worker - INFO -          dashboard at:         10.201.2.163:40403
2024-04-19 17:16:02,942 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:35999
2024-04-19 17:16:02,942 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:35999
2024-04-19 17:16:02,942 - distributed.worker - INFO -          dashboard at:         10.201.2.163:37765
2024-04-19 17:16:02,942 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:41095
2024-04-19 17:16:02,942 - distributed.worker - INFO -          dashboard at:         10.201.2.163:40811
2024-04-19 17:16:02,942 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.145:8786
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,942 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0qh82mya
2024-04-19 17:16:02,942 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xea87k5h
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,942 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1dcwr_09
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,942 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o1uqv4z5
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,942 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,765 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,765 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,766 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,767 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,767 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,768 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,770 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,771 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,771 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,771 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,771 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,772 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,772 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,773 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,773 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,774 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,774 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,775 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,775 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,775 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,775 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,776 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,776 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,776 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:06,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:06,777 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.145:8786
2024-04-19 17:16:06,777 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:06,777 - distributed.core - INFO - Starting established connection to tcp://10.201.2.145:8786
2024-04-19 17:16:16,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,165 - distributed.utils_perf - INFO - full garbage collection released 74.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:20,143 - distributed.utils_perf - INFO - full garbage collection released 1.65 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:20,551 - distributed.utils_perf - INFO - full garbage collection released 271.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,362 - distributed.utils_perf - INFO - full garbage collection released 746.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,520 - distributed.utils_perf - INFO - full garbage collection released 235.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:27,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,367 - distributed.utils_perf - INFO - full garbage collection released 2.69 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:28,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,603 - distributed.utils_perf - INFO - full garbage collection released 1.52 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:29,832 - distributed.utils_perf - INFO - full garbage collection released 814.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,343 - distributed.utils_perf - INFO - full garbage collection released 4.74 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:33,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:33,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,369 - distributed.utils_perf - INFO - full garbage collection released 2.67 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:36,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,612 - distributed.utils_perf - INFO - full garbage collection released 23.60 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,895 - distributed.utils_perf - INFO - full garbage collection released 1.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:45,920 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,407 - distributed.utils_perf - INFO - full garbage collection released 88.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:53,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,247 - distributed.utils_perf - INFO - full garbage collection released 37.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:00,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,467 - distributed.utils_perf - INFO - full garbage collection released 102.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:08,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,283 - distributed.utils_perf - INFO - full garbage collection released 264.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:09,210 - distributed.utils_perf - INFO - full garbage collection released 132.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:13,027 - distributed.utils_perf - INFO - full garbage collection released 269.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:19,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,880 - distributed.utils_perf - INFO - full garbage collection released 307.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:25,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,015 - distributed.utils_perf - INFO - full garbage collection released 698.18 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:35,097 - distributed.utils_perf - INFO - full garbage collection released 228.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:35,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:38,742 - distributed.utils_perf - INFO - full garbage collection released 354.78 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:43,922 - distributed.utils_perf - INFO - full garbage collection released 254.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:45,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:16,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:16,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:20,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:32,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:35,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:35,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:42,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:56,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:01,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:08,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:09,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:09,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:20,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,409 - distributed.utils_perf - INFO - full garbage collection released 782.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:46,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,348 - distributed.utils_perf - INFO - full garbage collection released 3.43 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:54,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,113 - distributed.utils_perf - INFO - full garbage collection released 173.21 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:11,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,821 - distributed.utils_perf - INFO - full garbage collection released 576.21 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:19,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:39,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:59,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:12,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:12,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:14,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:19,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:21,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,141 - distributed.utils_perf - INFO - full garbage collection released 1.99 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:33,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,512 - distributed.utils_perf - INFO - full garbage collection released 767.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:00,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:03,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,642 - distributed.utils_perf - INFO - full garbage collection released 199.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:11,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:28,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:44,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,439 - distributed.utils_perf - INFO - full garbage collection released 39.62 MiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:21,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:22,574 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:23:24,189 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:23:26,174 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:23:26,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,593 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:23:31,579 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:23:31,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:41,903 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:42,582 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:42,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,419 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,449 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,740 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,370 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:47,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,371 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:51,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,838 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:52,006 - distributed.utils_perf - INFO - full garbage collection released 1.45 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:52,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,916 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:56,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:03,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:17,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:20,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,662 - distributed.utils_perf - INFO - full garbage collection released 14.77 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:29,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:33,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:43,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,698 - distributed.utils_perf - INFO - full garbage collection released 1.62 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:59,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:13,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:20,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:27,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:02] task [xgboost.dask-tcp://10.201.2.159:36191]:tcp://10.201.2.159:36191 got new rank 0
[17:26:02] task [xgboost.dask-tcp://10.201.2.159:37297]:tcp://10.201.2.159:37297 got new rank 1
[17:26:02] task [xgboost.dask-tcp://10.201.2.159:39457]:tcp://10.201.2.159:39457 got new rank 2
[17:26:02] task [xgboost.dask-tcp://10.201.2.159:41723]:tcp://10.201.2.159:41723 got new rank 3
[17:26:02] task [xgboost.dask-tcp://10.201.2.163:33329]:tcp://10.201.2.163:33329 got new rank 4
[17:26:02] task [xgboost.dask-tcp://10.201.2.163:35999]:tcp://10.201.2.163:35999 got new rank 5
[17:26:02] task [xgboost.dask-tcp://10.201.2.163:41095]:tcp://10.201.2.163:41095 got new rank 6
[17:26:02] task [xgboost.dask-tcp://10.201.2.163:41105]:tcp://10.201.2.163:41105 got new rank 7
2024-04-19 17:28:06,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:06,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:06,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:06,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:06,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:07,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:46] task [xgboost.dask-tcp://10.201.2.159:36191]:tcp://10.201.2.159:36191 got new rank 0
[17:28:46] task [xgboost.dask-tcp://10.201.2.159:37297]:tcp://10.201.2.159:37297 got new rank 1
[17:28:46] task [xgboost.dask-tcp://10.201.2.159:39457]:tcp://10.201.2.159:39457 got new rank 2
[17:28:46] task [xgboost.dask-tcp://10.201.2.159:41723]:tcp://10.201.2.159:41723 got new rank 3
[17:28:46] task [xgboost.dask-tcp://10.201.2.163:33329]:tcp://10.201.2.163:33329 got new rank 4
[17:28:46] task [xgboost.dask-tcp://10.201.2.163:35999]:tcp://10.201.2.163:35999 got new rank 5
[17:28:46] task [xgboost.dask-tcp://10.201.2.163:41095]:tcp://10.201.2.163:41095 got new rank 6
[17:28:46] task [xgboost.dask-tcp://10.201.2.163:41105]:tcp://10.201.2.163:41105 got new rank 7
2024-04-19 17:31:03,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:36] task [xgboost.dask-tcp://10.201.2.159:36191]:tcp://10.201.2.159:36191 got new rank 0
[17:31:36] task [xgboost.dask-tcp://10.201.2.159:37297]:tcp://10.201.2.159:37297 got new rank 1
[17:31:36] task [xgboost.dask-tcp://10.201.2.159:39457]:tcp://10.201.2.159:39457 got new rank 2
[17:31:36] task [xgboost.dask-tcp://10.201.2.159:41723]:tcp://10.201.2.159:41723 got new rank 3
[17:31:36] task [xgboost.dask-tcp://10.201.2.163:33329]:tcp://10.201.2.163:33329 got new rank 4
[17:31:36] task [xgboost.dask-tcp://10.201.2.163:35999]:tcp://10.201.2.163:35999 got new rank 5
[17:31:36] task [xgboost.dask-tcp://10.201.2.163:41095]:tcp://10.201.2.163:41095 got new rank 6
[17:31:36] task [xgboost.dask-tcp://10.201.2.163:41105]:tcp://10.201.2.163:41105 got new rank 7
2024-04-19 17:33:44,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:34:23] task [xgboost.dask-tcp://10.201.2.159:36191]:tcp://10.201.2.159:36191 got new rank 0
[17:34:23] task [xgboost.dask-tcp://10.201.2.159:37297]:tcp://10.201.2.159:37297 got new rank 1
[17:34:23] task [xgboost.dask-tcp://10.201.2.159:39457]:tcp://10.201.2.159:39457 got new rank 2
[17:34:23] task [xgboost.dask-tcp://10.201.2.159:41723]:tcp://10.201.2.159:41723 got new rank 3
[17:34:23] task [xgboost.dask-tcp://10.201.2.163:33329]:tcp://10.201.2.163:33329 got new rank 4
[17:34:23] task [xgboost.dask-tcp://10.201.2.163:35999]:tcp://10.201.2.163:35999 got new rank 5
[17:34:23] task [xgboost.dask-tcp://10.201.2.163:41095]:tcp://10.201.2.163:41095 got new rank 6
[17:34:23] task [xgboost.dask-tcp://10.201.2.163:41105]:tcp://10.201.2.163:41105 got new rank 7
2024-04-19 17:36:45,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:45,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:45,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:45,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:45,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:46,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:47,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:47,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:37:28] task [xgboost.dask-tcp://10.201.2.159:36191]:tcp://10.201.2.159:36191 got new rank 0
[17:37:28] task [xgboost.dask-tcp://10.201.2.159:37297]:tcp://10.201.2.159:37297 got new rank 1
[17:37:28] task [xgboost.dask-tcp://10.201.2.159:39457]:tcp://10.201.2.159:39457 got new rank 2
[17:37:28] task [xgboost.dask-tcp://10.201.2.159:41723]:tcp://10.201.2.159:41723 got new rank 3
[17:37:28] task [xgboost.dask-tcp://10.201.2.163:33329]:tcp://10.201.2.163:33329 got new rank 4
[17:37:28] task [xgboost.dask-tcp://10.201.2.163:35999]:tcp://10.201.2.163:35999 got new rank 5
[17:37:28] task [xgboost.dask-tcp://10.201.2.163:41095]:tcp://10.201.2.163:41095 got new rank 6
[17:37:28] task [xgboost.dask-tcp://10.201.2.163:41105]:tcp://10.201.2.163:41105 got new rank 7
2024-04-19 17:39:27,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:39:30,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:39:30,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:33329. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:41105. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:35999. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:41095. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:41723. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:39457. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:36191. Reason: scheduler-close
2024-04-19 17:39:58,663 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:37297. Reason: scheduler-close
2024-04-19 17:39:58,666 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:40969'. Reason: scheduler-close
2024-04-19 17:39:58,666 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:37631'. Reason: scheduler-close
2024-04-19 17:39:58,666 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:37167'. Reason: scheduler-close
2024-04-19 17:39:58,666 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:42921'. Reason: scheduler-close
2024-04-19 17:39:58,664 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53198 remote=tcp://10.201.2.145:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53198 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:58,664 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53176 remote=tcp://10.201.2.145:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53176 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:58,664 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53186 remote=tcp://10.201.2.145:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53186 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:58,664 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53178 remote=tcp://10.201.2.145:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:53178 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:58,671 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:40813'. Reason: scheduler-close
2024-04-19 17:39:58,671 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:37353'. Reason: scheduler-close
2024-04-19 17:39:58,671 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:45083'. Reason: scheduler-close
2024-04-19 17:39:58,671 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:34879'. Reason: scheduler-close
2024-04-19 17:39:59,448 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.159:34808 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,448 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.159:46592 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,449 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.159:34814 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,450 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.159:34796 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,452 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:37292 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,451 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:39:59,452 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:37310 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,452 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:37302 remote=tcp://10.201.2.145:8786>: Stream is closed
/10.201.2.163:52030 remote=tcp://10.201.2.145:8786>: Stream is closed
2024-04-19 17:39:59,471 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,471 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,489 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,489 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,495 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,495 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,520 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,520 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,523 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,524 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,526 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,526 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,549 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,549 - distributed.nanny - INFO - Worker closed
2024-04-19 17:39:59,589 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.145:8786; closing.
2024-04-19 17:39:59,590 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:01,501 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,508 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,526 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,528 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,551 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,591 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:01,612 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:02,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:45083'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,335 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,343 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:37353'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,343 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,349 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:34879'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,349 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:40969'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,734 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:37631'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,741 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:37167'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,781 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:02,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:40813'. Reason: nanny-close-gracefully
2024-04-19 17:40:02,802 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:03,212 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:42921'. Reason: nanny-close-gracefully
2024-04-19 17:40:03,213 - distributed.dask_worker - INFO - End worker
