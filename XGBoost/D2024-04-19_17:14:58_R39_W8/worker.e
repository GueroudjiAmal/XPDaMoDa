2024-04-19 17:16:15,103 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,103 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,103 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,104 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,162 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,162 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,163 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,163 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,415 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,415 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,416 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,415 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.114:39335'
2024-04-19 17:16:15,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.114:43967'
2024-04-19 17:16:15,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.114:44315'
2024-04-19 17:16:15,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.114:38693'
2024-04-19 17:16:15,493 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,493 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,493 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,497 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.113:45709'
2024-04-19 17:16:15,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.113:38195'
2024-04-19 17:16:15,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.113:33907'
2024-04-19 17:16:15,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.113:42021'
2024-04-19 17:16:16,496 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,496 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,496 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,496 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,497 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,512 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,512 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,513 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,520 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,521 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,521 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,521 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,521 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,521 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,521 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,521 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,541 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,557 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,557 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,557 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,565 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,565 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,566 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,566 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,511 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,511 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,511 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,512 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,540 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,540 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,540 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,540 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:18,596 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-816980d8-b099-4b7d-9f7b-c13fd14470a6
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.114:41657
2024-04-19 17:16:18,596 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.114:41657
2024-04-19 17:16:18,596 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4aad417c-9d4f-4da5-adea-058a054fb136
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.114:39249
2024-04-19 17:16:18,596 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3a678da4-6ac8-48ad-bafc-bdc3730b8bb1
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.114:41533
2024-04-19 17:16:18,596 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.114:41533
2024-04-19 17:16:18,596 - distributed.worker - INFO -          dashboard at:         10.201.3.114:39789
2024-04-19 17:16:18,596 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,596 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,596 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9b1ada98-c146-4cf3-b555-81338bdda3cb
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.114:41459
2024-04-19 17:16:18,596 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.114:41459
2024-04-19 17:16:18,596 - distributed.worker - INFO -          dashboard at:         10.201.3.114:44969
2024-04-19 17:16:18,596 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,596 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,596 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,596 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9nkuei2o
2024-04-19 17:16:18,596 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,596 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.114:39249
2024-04-19 17:16:18,596 - distributed.worker - INFO -          dashboard at:         10.201.3.114:40539
2024-04-19 17:16:18,596 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,596 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,597 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,596 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,596 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wn3ypa__
2024-04-19 17:16:18,597 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,596 - distributed.worker - INFO -          dashboard at:         10.201.3.114:34907
2024-04-19 17:16:18,596 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,597 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,597 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,597 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-12ik1frm
2024-04-19 17:16:18,597 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-scwl2zuy
2024-04-19 17:16:18,597 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,597 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e2aaef4b-ec73-448f-b9c5-ea5f0931ca3e
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.113:33743
2024-04-19 17:16:18,650 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.113:33743
2024-04-19 17:16:18,650 - distributed.worker - INFO -          dashboard at:         10.201.3.113:33909
2024-04-19 17:16:18,650 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-fd78de4c-0e3c-4896-9521-aadfb70ff5aa
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.113:33199
2024-04-19 17:16:18,650 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.113:33199
2024-04-19 17:16:18,650 - distributed.worker - INFO -          dashboard at:         10.201.3.113:46321
2024-04-19 17:16:18,650 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5bdcc948-6950-4c37-bddc-8e084b5d91d5
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.113:41409
2024-04-19 17:16:18,650 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.113:41409
2024-04-19 17:16:18,650 - distributed.worker - INFO -          dashboard at:         10.201.3.113:40439
2024-04-19 17:16:18,650 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,650 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,650 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c7f80d1a-ff2e-4ad9-a1a3-0bf0e7090b98
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.113:40637
2024-04-19 17:16:18,650 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.113:40637
2024-04-19 17:16:18,650 - distributed.worker - INFO -          dashboard at:         10.201.3.113:41899
2024-04-19 17:16:18,650 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,650 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,650 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4iwov42t
2024-04-19 17:16:18,650 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,650 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,650 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1m8bmoa1
2024-04-19 17:16:18,650 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.122:8786
2024-04-19 17:16:18,650 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,650 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-chau6qlw
2024-04-19 17:16:18,651 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,650 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l6jf0yhi
2024-04-19 17:16:18,651 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,651 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,487 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,488 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,488 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,489 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,489 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,490 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,493 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,493 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,494 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,494 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,494 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,495 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,496 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,496 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,496 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,497 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,497 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,497 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,497 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,497 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,497 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,498 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,498 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:22,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,499 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.122:8786
2024-04-19 17:16:22,499 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,500 - distributed.core - INFO - Starting established connection to tcp://10.201.3.122:8786
2024-04-19 17:16:30,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,674 - distributed.utils_perf - INFO - full garbage collection released 47.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,970 - distributed.utils_perf - INFO - full garbage collection released 456.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,362 - distributed.utils_perf - INFO - full garbage collection released 446.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:42,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,142 - distributed.utils_perf - INFO - full garbage collection released 292.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:43,589 - distributed.utils_perf - INFO - full garbage collection released 1.45 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,136 - distributed.utils_perf - INFO - full garbage collection released 138.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,980 - distributed.utils_perf - INFO - full garbage collection released 565.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,891 - distributed.utils_perf - INFO - full garbage collection released 9.69 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,624 - distributed.utils_perf - INFO - full garbage collection released 73.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,351 - distributed.utils_perf - INFO - full garbage collection released 3.63 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:53,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:56,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:56,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,452 - distributed.utils_perf - INFO - full garbage collection released 85.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:58,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,645 - distributed.utils_perf - INFO - full garbage collection released 149.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:12,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,449 - distributed.utils_perf - INFO - full garbage collection released 170.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:20,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,757 - distributed.utils_perf - INFO - full garbage collection released 358.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:25,003 - distributed.utils_perf - INFO - full garbage collection released 181.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:29,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:35,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:35,221 - distributed.utils_perf - INFO - full garbage collection released 515.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:35,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:35,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,111 - distributed.utils_perf - INFO - full garbage collection released 255.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:40,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:41,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,759 - distributed.utils_perf - INFO - full garbage collection released 91.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:46,707 - distributed.utils_perf - INFO - full garbage collection released 70.60 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:48,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,726 - distributed.utils_perf - INFO - full garbage collection released 268.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:50,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:04,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:04,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,770 - distributed.utils_perf - INFO - full garbage collection released 296.69 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:08,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,438 - distributed.utils_perf - INFO - full garbage collection released 138.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:10,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:12,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:42,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:52,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:54,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:57,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:57,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,121 - distributed.utils_perf - INFO - full garbage collection released 1.25 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:26,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:39,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:41,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,376 - distributed.utils_perf - INFO - full garbage collection released 849.33 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:47,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,154 - distributed.utils_perf - INFO - full garbage collection released 1.42 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:54,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,613 - distributed.utils_perf - INFO - full garbage collection released 89.69 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:31,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,352 - distributed.utils_perf - INFO - full garbage collection released 361.55 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:42,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:47,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:50,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:03,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:11,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,979 - distributed.utils_perf - INFO - full garbage collection released 352.93 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:38,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:42,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:43,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:50,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,213 - distributed.comm.tcp - INFO - Connection from tcp://10.201.3.113:35678 closed before handshake completed
2024-04-19 17:21:57,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:00,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:02,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:06,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:07,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:37,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,484 - distributed.utils_perf - INFO - full garbage collection released 1.21 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:15,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:22,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:27,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:35,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:39,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,556 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:24:07,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,819 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:24:08,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,565 - distributed.utils_perf - INFO - full garbage collection released 11.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:11,395 - distributed.utils_perf - INFO - full garbage collection released 280.17 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:15,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:43,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,447 - distributed.utils_perf - INFO - full garbage collection released 15.83 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:49,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,103 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:50,939 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:51,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,969 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:53,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,227 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:54,820 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:55,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,824 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:24:57,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:59,301 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:25:00,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:02,318 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:25:03,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:03,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:05,669 - distributed.utils_perf - INFO - full garbage collection released 1.09 GiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:10,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:11,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:12,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:16,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:17,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:23,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:33,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:37,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:07] task [xgboost.dask-tcp://10.201.3.113:33199]:tcp://10.201.3.113:33199 got new rank 0
[17:26:07] task [xgboost.dask-tcp://10.201.3.113:33743]:tcp://10.201.3.113:33743 got new rank 1
[17:26:07] task [xgboost.dask-tcp://10.201.3.113:40637]:tcp://10.201.3.113:40637 got new rank 2
[17:26:07] task [xgboost.dask-tcp://10.201.3.113:41409]:tcp://10.201.3.113:41409 got new rank 3
[17:26:07] task [xgboost.dask-tcp://10.201.3.114:39249]:tcp://10.201.3.114:39249 got new rank 4
[17:26:08] task [xgboost.dask-tcp://10.201.3.114:41459]:tcp://10.201.3.114:41459 got new rank 5
[17:26:08] task [xgboost.dask-tcp://10.201.3.114:41533]:tcp://10.201.3.114:41533 got new rank 6
[17:26:08] task [xgboost.dask-tcp://10.201.3.114:41657]:tcp://10.201.3.114:41657 got new rank 7
2024-04-19 17:28:09,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:09,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:10,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:52] task [xgboost.dask-tcp://10.201.3.113:33199]:tcp://10.201.3.113:33199 got new rank 0
[17:28:53] task [xgboost.dask-tcp://10.201.3.113:33743]:tcp://10.201.3.113:33743 got new rank 1
[17:28:53] task [xgboost.dask-tcp://10.201.3.113:40637]:tcp://10.201.3.113:40637 got new rank 2
[17:28:53] task [xgboost.dask-tcp://10.201.3.113:41409]:tcp://10.201.3.113:41409 got new rank 3
[17:28:53] task [xgboost.dask-tcp://10.201.3.114:39249]:tcp://10.201.3.114:39249 got new rank 4
[17:28:53] task [xgboost.dask-tcp://10.201.3.114:41459]:tcp://10.201.3.114:41459 got new rank 5
[17:28:53] task [xgboost.dask-tcp://10.201.3.114:41533]:tcp://10.201.3.114:41533 got new rank 6
[17:28:53] task [xgboost.dask-tcp://10.201.3.114:41657]:tcp://10.201.3.114:41657 got new rank 7
[17:31:51] task [xgboost.dask-tcp://10.201.3.113:33199]:tcp://10.201.3.113:33199 got new rank 0
[17:31:51] task [xgboost.dask-tcp://10.201.3.113:33743]:tcp://10.201.3.113:33743 got new rank 1
[17:31:51] task [xgboost.dask-tcp://10.201.3.113:40637]:tcp://10.201.3.113:40637 got new rank 2
[17:31:51] task [xgboost.dask-tcp://10.201.3.113:41409]:tcp://10.201.3.113:41409 got new rank 3
[17:31:51] task [xgboost.dask-tcp://10.201.3.114:39249]:tcp://10.201.3.114:39249 got new rank 4
[17:31:51] task [xgboost.dask-tcp://10.201.3.114:41459]:tcp://10.201.3.114:41459 got new rank 5
[17:31:51] task [xgboost.dask-tcp://10.201.3.114:41533]:tcp://10.201.3.114:41533 got new rank 6
[17:31:51] task [xgboost.dask-tcp://10.201.3.114:41657]:tcp://10.201.3.114:41657 got new rank 7
[17:34:51] task [xgboost.dask-tcp://10.201.3.113:33199]:tcp://10.201.3.113:33199 got new rank 0
[17:34:51] task [xgboost.dask-tcp://10.201.3.113:33743]:tcp://10.201.3.113:33743 got new rank 1
[17:34:51] task [xgboost.dask-tcp://10.201.3.113:40637]:tcp://10.201.3.113:40637 got new rank 2
[17:34:51] task [xgboost.dask-tcp://10.201.3.113:41409]:tcp://10.201.3.113:41409 got new rank 3
[17:34:51] task [xgboost.dask-tcp://10.201.3.114:39249]:tcp://10.201.3.114:39249 got new rank 4
[17:34:51] task [xgboost.dask-tcp://10.201.3.114:41459]:tcp://10.201.3.114:41459 got new rank 5
[17:34:51] task [xgboost.dask-tcp://10.201.3.114:41533]:tcp://10.201.3.114:41533 got new rank 6
[17:34:51] task [xgboost.dask-tcp://10.201.3.114:41657]:tcp://10.201.3.114:41657 got new rank 7
2024-04-19 17:36:58,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:58,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:58,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:59,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:14,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:15,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:16,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:37:58] task [xgboost.dask-tcp://10.201.3.113:33199]:tcp://10.201.3.113:33199 got new rank 0
[17:37:58] task [xgboost.dask-tcp://10.201.3.113:33743]:tcp://10.201.3.113:33743 got new rank 1
[17:37:58] task [xgboost.dask-tcp://10.201.3.113:40637]:tcp://10.201.3.113:40637 got new rank 2
[17:37:58] task [xgboost.dask-tcp://10.201.3.113:41409]:tcp://10.201.3.113:41409 got new rank 3
[17:37:58] task [xgboost.dask-tcp://10.201.3.114:39249]:tcp://10.201.3.114:39249 got new rank 4
[17:37:58] task [xgboost.dask-tcp://10.201.3.114:41459]:tcp://10.201.3.114:41459 got new rank 5
[17:37:58] task [xgboost.dask-tcp://10.201.3.114:41533]:tcp://10.201.3.114:41533 got new rank 6
[17:37:58] task [xgboost.dask-tcp://10.201.3.114:41657]:tcp://10.201.3.114:41657 got new rank 7
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.114:39249. Reason: scheduler-close
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.114:41459. Reason: scheduler-close
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.114:41657. Reason: scheduler-close
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.114:41533. Reason: scheduler-close
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.113:40637. Reason: scheduler-close
2024-04-19 17:40:29,855 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.113:33743. Reason: scheduler-close
2024-04-19 17:40:29,856 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.113:33199. Reason: scheduler-close
2024-04-19 17:40:29,856 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.113:41409. Reason: scheduler-close
2024-04-19 17:40:29,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.113:42021'. Reason: scheduler-close
2024-04-19 17:40:29,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.113:38195'. Reason: scheduler-close
2024-04-19 17:40:29,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.114:44315'. Reason: scheduler-close
2024-04-19 17:40:29,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.114:39335'. Reason: scheduler-close
2024-04-19 17:40:29,856 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.113:38582 remote=tcp://10.201.3.122:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.113:38582 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:29,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.113:38580 remote=tcp://10.201.3.122:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.113:38580 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:29,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.113:33907'. Reason: scheduler-close
2024-04-19 17:40:29,864 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.113:45709'. Reason: scheduler-close
2024-04-19 17:40:29,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.114:54770 remote=tcp://10.201.3.122:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.114:54770 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:29,857 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.114:54784 remote=tcp://10.201.3.122:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.114:54784 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:29,864 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.114:38693'. Reason: scheduler-close
2024-04-19 17:40:29,865 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.114:43967'. Reason: scheduler-close
2024-04-19 17:40:30,639 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.113:57476 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,639 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.113:57468 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,640 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.113:57452 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,640 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.113:32824 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,642 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,643 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,645 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:30,644 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:30,644 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.114:36526 remote=tcp://10.201.3.122:8786>: Stream is closed
/10.201.3.114:36540 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,645 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.114:36552 remote=tcp://10.201.3.122:8786>: Stream is closed
/10.201.3.114:36548 remote=tcp://10.201.3.122:8786>: Stream is closed
2024-04-19 17:40:30,648 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,648 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,659 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,659 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,685 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,686 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,714 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,714 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,751 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,752 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,753 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,753 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:30,755 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.122:8786; closing.
2024-04-19 17:40:30,756 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:32,717 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,719 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,728 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,739 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,740 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,753 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:32,754 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:33,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.113:33907'. Reason: nanny-close-gracefully
2024-04-19 17:40:33,528 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:33,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.113:45709'. Reason: nanny-close-gracefully
2024-04-19 17:40:33,569 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:33,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.113:38195'. Reason: nanny-close-gracefully
2024-04-19 17:40:33,575 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:33,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.113:42021'. Reason: nanny-close-gracefully
2024-04-19 17:40:33,990 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:33,999 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.114:39335'. Reason: nanny-close-gracefully
2024-04-19 17:40:33,999 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:34,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.114:43967'. Reason: nanny-close-gracefully
2024-04-19 17:40:34,039 - distributed.dask_worker - INFO - End worker
