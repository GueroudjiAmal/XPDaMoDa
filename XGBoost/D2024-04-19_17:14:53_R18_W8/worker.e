2024-04-19 17:15:31,230 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,230 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,231 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,231 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,245 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,245 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,245 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,245 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,486 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.215:43457'
2024-04-19 17:15:31,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.215:35635'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.215:44303'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.11:44049'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.11:42337'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.11:39985'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.11:38115'
2024-04-19 17:15:31,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.215:44529'
2024-04-19 17:15:32,484 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,485 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,485 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,485 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,486 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,486 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,486 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,486 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,505 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,505 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,505 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,506 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,506 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,506 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,506 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,507 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,530 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,530 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,530 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,531 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,550 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,550 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,550 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,551 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:33,480 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,480 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,480 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,480 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,505 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,505 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,505 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,505 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:34,530 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9fe320e1-b426-4d1b-bc9c-143f230fdb38
2024-04-19 17:15:34,530 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e10950d2-a4e3-402b-8198-0482f33f06a0
2024-04-19 17:15:34,530 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7c6ca514-4e41-4313-99af-7d28e2599e9f
2024-04-19 17:15:34,530 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.215:46521
2024-04-19 17:15:34,531 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.215:46521
2024-04-19 17:15:34,531 - distributed.worker - INFO -          dashboard at:         10.201.2.215:32959
2024-04-19 17:15:34,530 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.215:34935
2024-04-19 17:15:34,530 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.215:34935
2024-04-19 17:15:34,531 - distributed.worker - INFO -          dashboard at:         10.201.2.215:33627
2024-04-19 17:15:34,531 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,530 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5e06a6e0-580f-4867-864a-11d958baed03
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.215:38761
2024-04-19 17:15:34,531 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.215:38761
2024-04-19 17:15:34,531 - distributed.worker - INFO -          dashboard at:         10.201.2.215:41731
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.215:38653
2024-04-19 17:15:34,531 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.215:38653
2024-04-19 17:15:34,531 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,531 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mzr7orxe
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,531 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bzn99m95
2024-04-19 17:15:34,531 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,531 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vnlnv45h
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO -          dashboard at:         10.201.2.215:42631
2024-04-19 17:15:34,531 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,531 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,531 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ad6dvua
2024-04-19 17:15:34,531 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-dc31988c-7919-4656-a07c-2fee2766d901
2024-04-19 17:15:34,568 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a8a0d9b9-915e-4694-8d85-ae3a5f781123
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.11:41205
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.11:34023
2024-04-19 17:15:34,568 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.11:34023
2024-04-19 17:15:34,568 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.11:41205
2024-04-19 17:15:34,568 - distributed.worker - INFO -          dashboard at:          10.201.3.11:36151
2024-04-19 17:15:34,568 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,568 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f8cdbf1e-f4aa-4222-bf40-0eb4bb78369c
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.11:36217
2024-04-19 17:15:34,568 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.11:36217
2024-04-19 17:15:34,568 - distributed.worker - INFO -          dashboard at:          10.201.3.11:45009
2024-04-19 17:15:34,568 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,568 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0755dabe-8d5e-4d0e-af05-86576c85c201
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.11:41821
2024-04-19 17:15:34,568 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.11:41821
2024-04-19 17:15:34,568 - distributed.worker - INFO -          dashboard at:          10.201.3.11:33291
2024-04-19 17:15:34,568 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,568 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,568 - distributed.worker - INFO -          dashboard at:          10.201.3.11:44119
2024-04-19 17:15:34,568 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.245:8786
2024-04-19 17:15:34,568 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,568 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ug35vsn
2024-04-19 17:15:34,569 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,568 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mu4iuldr
2024-04-19 17:15:34,568 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,568 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,569 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,569 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-obkw0joa
2024-04-19 17:15:34,569 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,569 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wx_41iwb
2024-04-19 17:15:34,569 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,419 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,419 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,419 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,420 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,421 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,421 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,423 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,423 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,424 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,425 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,425 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,426 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,427 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,427 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,427 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,428 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,428 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,428 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,429 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,429 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,429 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:38,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:38,430 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.245:8786
2024-04-19 17:15:38,430 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:38,430 - distributed.core - INFO - Starting established connection to tcp://10.201.2.245:8786
2024-04-19 17:15:47,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:54,682 - distributed.utils_perf - INFO - full garbage collection released 255.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,743 - distributed.utils_perf - INFO - full garbage collection released 676.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,960 - distributed.utils_perf - INFO - full garbage collection released 500.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:55,704 - distributed.utils_perf - INFO - full garbage collection released 423.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:58,884 - distributed.utils_perf - INFO - full garbage collection released 0.93 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:59,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:00,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:00,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:02,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:04,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:05,887 - distributed.utils_perf - INFO - full garbage collection released 801.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:06,343 - distributed.utils_perf - INFO - full garbage collection released 1.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:06,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:11,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:12,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,475 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:16,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:17,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,763 - distributed.utils_perf - INFO - full garbage collection released 300.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:18,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:20,840 - distributed.utils_perf - INFO - full garbage collection released 0.94 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:21,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:22,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,444 - distributed.utils_perf - INFO - full garbage collection released 72.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,177 - distributed.utils_perf - INFO - full garbage collection released 35.53 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,584 - distributed.utils_perf - INFO - full garbage collection released 0.93 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:32,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:36,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,752 - distributed.utils_perf - INFO - full garbage collection released 160.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,759 - distributed.utils_perf - INFO - full garbage collection released 1.27 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:49,854 - distributed.utils_perf - INFO - full garbage collection released 366.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:51,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:51,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,900 - distributed.utils_perf - INFO - full garbage collection released 231.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:54,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:16,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:17,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,605 - distributed.utils_perf - INFO - full garbage collection released 466.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:40,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:47,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:12,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:20,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,510 - distributed.utils_perf - INFO - full garbage collection released 655.53 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:27,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,297 - distributed.utils_perf - INFO - full garbage collection released 6.30 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:31,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:52,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:57,659 - distributed.utils_perf - INFO - full garbage collection released 523.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:58,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:09,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:14,236 - distributed.utils_perf - INFO - full garbage collection released 405.33 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:17,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:20,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,805 - distributed.utils_perf - INFO - full garbage collection released 866.71 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:22,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:23,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:25,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:37,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:39,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,642 - distributed.utils_perf - INFO - full garbage collection released 150.07 MiB from 55 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:40,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:56,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,670 - distributed.utils_perf - INFO - full garbage collection released 383.51 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:07,235 - distributed.utils_perf - INFO - full garbage collection released 654.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:08,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:37,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:39,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:39,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,414 - distributed.utils_perf - INFO - full garbage collection released 546.24 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:47,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:57,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,371 - distributed.utils_perf - INFO - full garbage collection released 352.56 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:05,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,264 - distributed.utils_perf - INFO - full garbage collection released 1.81 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:15,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:19,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:31,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:47,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:47,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:49,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:00,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:00,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:03,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:07,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:37,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,223 - distributed.utils_perf - INFO - full garbage collection released 15.81 MiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:09,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,707 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:19,957 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:20,260 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:20,622 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:20,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,057 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:21,580 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:22,254 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:23,076 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:23,206 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:24,088 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:25,312 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:26,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,877 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:28,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,788 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:30,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,147 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:31,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,762 - distributed.utils_perf - INFO - full garbage collection released 1.62 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:40,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:41,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,945 - distributed.utils_perf - INFO - full garbage collection released 1.36 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:55,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:03,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,194 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:24:19,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:31,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:25:30] task [xgboost.dask-tcp://10.201.2.215:34935]:tcp://10.201.2.215:34935 got new rank 0
[17:25:30] task [xgboost.dask-tcp://10.201.2.215:38653]:tcp://10.201.2.215:38653 got new rank 1
[17:25:30] task [xgboost.dask-tcp://10.201.2.215:38761]:tcp://10.201.2.215:38761 got new rank 2
[17:25:30] task [xgboost.dask-tcp://10.201.2.215:46521]:tcp://10.201.2.215:46521 got new rank 3
[17:25:30] task [xgboost.dask-tcp://10.201.3.11:34023]:tcp://10.201.3.11:34023 got new rank 4
[17:25:30] task [xgboost.dask-tcp://10.201.3.11:36217]:tcp://10.201.3.11:36217 got new rank 5
[17:25:30] task [xgboost.dask-tcp://10.201.3.11:41205]:tcp://10.201.3.11:41205 got new rank 6
[17:25:30] task [xgboost.dask-tcp://10.201.3.11:41821]:tcp://10.201.3.11:41821 got new rank 7
2024-04-19 17:27:43,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:43,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:43,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:43,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:25] task [xgboost.dask-tcp://10.201.2.215:34935]:tcp://10.201.2.215:34935 got new rank 0
[17:28:25] task [xgboost.dask-tcp://10.201.2.215:38653]:tcp://10.201.2.215:38653 got new rank 1
[17:28:25] task [xgboost.dask-tcp://10.201.2.215:38761]:tcp://10.201.2.215:38761 got new rank 2
[17:28:25] task [xgboost.dask-tcp://10.201.2.215:46521]:tcp://10.201.2.215:46521 got new rank 3
[17:28:25] task [xgboost.dask-tcp://10.201.3.11:34023]:tcp://10.201.3.11:34023 got new rank 4
[17:28:25] task [xgboost.dask-tcp://10.201.3.11:36217]:tcp://10.201.3.11:36217 got new rank 5
[17:28:25] task [xgboost.dask-tcp://10.201.3.11:41205]:tcp://10.201.3.11:41205 got new rank 6
[17:28:25] task [xgboost.dask-tcp://10.201.3.11:41821]:tcp://10.201.3.11:41821 got new rank 7
2024-04-19 17:28:50,552 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:30:59,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:59,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:59,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:00,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:33] task [xgboost.dask-tcp://10.201.2.215:34935]:tcp://10.201.2.215:34935 got new rank 0
[17:31:33] task [xgboost.dask-tcp://10.201.2.215:38653]:tcp://10.201.2.215:38653 got new rank 1
[17:31:33] task [xgboost.dask-tcp://10.201.2.215:38761]:tcp://10.201.2.215:38761 got new rank 2
[17:31:33] task [xgboost.dask-tcp://10.201.2.215:46521]:tcp://10.201.2.215:46521 got new rank 3
[17:31:33] task [xgboost.dask-tcp://10.201.3.11:34023]:tcp://10.201.3.11:34023 got new rank 4
[17:31:33] task [xgboost.dask-tcp://10.201.3.11:36217]:tcp://10.201.3.11:36217 got new rank 5
[17:31:33] task [xgboost.dask-tcp://10.201.3.11:41205]:tcp://10.201.3.11:41205 got new rank 6
[17:31:33] task [xgboost.dask-tcp://10.201.3.11:41821]:tcp://10.201.3.11:41821 got new rank 7
2024-04-19 17:33:39,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:44,071 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
[17:34:23] task [xgboost.dask-tcp://10.201.2.215:34935]:tcp://10.201.2.215:34935 got new rank 0
[17:34:23] task [xgboost.dask-tcp://10.201.2.215:38653]:tcp://10.201.2.215:38653 got new rank 1
[17:34:23] task [xgboost.dask-tcp://10.201.2.215:38761]:tcp://10.201.2.215:38761 got new rank 2
[17:34:23] task [xgboost.dask-tcp://10.201.2.215:46521]:tcp://10.201.2.215:46521 got new rank 3
[17:34:23] task [xgboost.dask-tcp://10.201.3.11:34023]:tcp://10.201.3.11:34023 got new rank 4
[17:34:23] task [xgboost.dask-tcp://10.201.3.11:36217]:tcp://10.201.3.11:36217 got new rank 5
[17:34:23] task [xgboost.dask-tcp://10.201.3.11:41205]:tcp://10.201.3.11:41205 got new rank 6
[17:34:23] task [xgboost.dask-tcp://10.201.3.11:41821]:tcp://10.201.3.11:41821 got new rank 7
2024-04-19 17:36:54,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:54,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:54,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:07,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:08,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:08,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:10,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:11,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:11,699 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
[17:37:56] task [xgboost.dask-tcp://10.201.2.215:34935]:tcp://10.201.2.215:34935 got new rank 0
[17:37:56] task [xgboost.dask-tcp://10.201.2.215:38653]:tcp://10.201.2.215:38653 got new rank 1
[17:37:56] task [xgboost.dask-tcp://10.201.2.215:38761]:tcp://10.201.2.215:38761 got new rank 2
[17:37:56] task [xgboost.dask-tcp://10.201.2.215:46521]:tcp://10.201.2.215:46521 got new rank 3
[17:37:56] task [xgboost.dask-tcp://10.201.3.11:34023]:tcp://10.201.3.11:34023 got new rank 4
[17:37:56] task [xgboost.dask-tcp://10.201.3.11:36217]:tcp://10.201.3.11:36217 got new rank 5
[17:37:56] task [xgboost.dask-tcp://10.201.3.11:41205]:tcp://10.201.3.11:41205 got new rank 6
[17:37:56] task [xgboost.dask-tcp://10.201.3.11:41821]:tcp://10.201.3.11:41821 got new rank 7
2024-04-19 17:38:18,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:18,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:18,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:18,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.215:38761. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.215:38653. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.11:36217. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.11:34023. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.215:46521. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.11:41205. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.11:41821. Reason: scheduler-close
2024-04-19 17:41:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.215:34935. Reason: scheduler-close
2024-04-19 17:41:16,676 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.215:44529'. Reason: scheduler-close
2024-04-19 17:41:16,676 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.215:35635'. Reason: scheduler-close
2024-04-19 17:41:16,676 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.215:43457'. Reason: scheduler-close
2024-04-19 17:41:16,676 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.215:44303'. Reason: scheduler-close
2024-04-19 17:41:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53940 remote=tcp://10.201.2.245:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53940 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53924 remote=tcp://10.201.2.245:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53924 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53950 remote=tcp://10.201.2.245:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53950 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53962 remote=tcp://10.201.2.245:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.11:53962 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.11:44049'. Reason: scheduler-close
2024-04-19 17:41:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.11:42337'. Reason: scheduler-close
2024-04-19 17:41:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.11:38115'. Reason: scheduler-close
2024-04-19 17:41:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.11:39985'. Reason: scheduler-close
2024-04-19 17:41:17,462 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:17,462 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.215:40726 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,462 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.215:40710 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,462 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.215:40698 remote=tcp://10.201.2.245:8786>: Stream is closed
/10.201.2.215:40692 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,543 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,543 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,549 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,549 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,572 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,572 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,597 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,598 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:17,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.11:33264 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.11:39442 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.11:33252 remote=tcp://10.201.2.245:8786>: Stream is closed
/10.201.3.11:39468 remote=tcp://10.201.2.245:8786>: Stream is closed
2024-04-19 17:41:17,659 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,659 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,669 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,669 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,719 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,720 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:17,725 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.245:8786; closing.
2024-04-19 17:41:17,725 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:19,546 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,550 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,574 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,599 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,701 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,739 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,744 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:19,755 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:20,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.215:44529'. Reason: nanny-close-gracefully
2024-04-19 17:41:20,798 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:20,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.215:35635'. Reason: nanny-close-gracefully
2024-04-19 17:41:20,806 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:20,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.215:43457'. Reason: nanny-close-gracefully
2024-04-19 17:41:20,912 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:21,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.11:44049'. Reason: nanny-close-gracefully
2024-04-19 17:41:21,175 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:21,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.11:39985'. Reason: nanny-close-gracefully
2024-04-19 17:41:21,183 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:21,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.11:42337'. Reason: nanny-close-gracefully
2024-04-19 17:41:21,190 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:21,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.215:44303'. Reason: nanny-close-gracefully
2024-04-19 17:41:21,279 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:21,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.11:38115'. Reason: nanny-close-gracefully
2024-04-19 17:41:21,644 - distributed.dask_worker - INFO - End worker
