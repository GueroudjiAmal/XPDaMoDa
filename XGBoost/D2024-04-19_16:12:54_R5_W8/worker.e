2024-04-19 16:13:12,001 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,001 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,002 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,002 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,067 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,067 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,068 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,068 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:36745'
2024-04-19 16:13:12,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:33381'
2024-04-19 16:13:12,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:40087'
2024-04-19 16:13:12,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:43767'
2024-04-19 16:13:12,832 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:12,834 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,846 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:12,846 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:12,847 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,847 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,848 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:12,848 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:12,875 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,890 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,891 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:12,891 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,327 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,327 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,328 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,328 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:13,525 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:13,525 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:13,525 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:13,525 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:13,554 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,554 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,554 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,555 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:13,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:41447'
2024-04-19 16:13:13,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:40783'
2024-04-19 16:13:13,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:33711'
2024-04-19 16:13:13,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:42199'
2024-04-19 16:13:14,529 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-82da23f3-2bf3-4cb5-8821-446f3deb82a8
2024-04-19 16:13:14,529 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-04089964-1f71-442a-bab4-bac80680908e
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:37685
2024-04-19 16:13:14,529 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1f3f8793-125f-4828-ae7f-847cbb1ac61f
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:33819
2024-04-19 16:13:14,529 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:33819
2024-04-19 16:13:14,529 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-72815a45-1cc0-414f-ba42-78cee10297a7
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:38515
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:41637
2024-04-19 16:13:14,529 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:41637
2024-04-19 16:13:14,529 - distributed.worker - INFO -          dashboard at:         10.201.2.237:34531
2024-04-19 16:13:14,529 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:14,529 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,529 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:14,529 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:37685
2024-04-19 16:13:14,529 - distributed.worker - INFO -          dashboard at:         10.201.2.237:36121
2024-04-19 16:13:14,529 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:14,529 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,529 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:14,529 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:14,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ofi_1xl
2024-04-19 16:13:14,529 - distributed.worker - INFO -          dashboard at:         10.201.2.237:46229
2024-04-19 16:13:14,529 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:14,529 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,529 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:14,529 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fg18mw95
2024-04-19 16:13:14,530 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,529 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:38515
2024-04-19 16:13:14,529 - distributed.worker - INFO -          dashboard at:         10.201.2.237:36001
2024-04-19 16:13:14,529 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:14,529 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,530 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:14,530 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:14,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lk2hffgq
2024-04-19 16:13:14,530 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,529 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:14,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lbs36x5n
2024-04-19 16:13:14,530 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,530 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:14,558 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,558 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,558 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,558 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,559 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 16:13:14,559 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,559 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,559 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 16:13:14,603 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,604 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,604 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:14,604 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 16:13:15,562 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,562 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,562 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,562 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 16:13:15,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:15,853 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:15,854 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:15,854 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:15,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:15,855 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:15,855 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:15,856 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:15,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:15,856 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:15,856 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:15,857 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:15,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:15,858 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:15,858 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:15,858 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:16,517 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8606c5f8-eeb4-42b1-825e-469e3cd40703
2024-04-19 16:13:16,517 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7fb73a0d-d5ad-4b05-bc98-e79741f3696a
2024-04-19 16:13:16,517 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:44717
2024-04-19 16:13:16,517 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:44717
2024-04-19 16:13:16,517 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1e4b570e-b7cc-446c-84cd-2e171ed736a4
2024-04-19 16:13:16,517 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:41379
2024-04-19 16:13:16,517 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:41379
2024-04-19 16:13:16,518 - distributed.worker - INFO -          dashboard at:         10.201.2.137:46623
2024-04-19 16:13:16,518 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,517 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:38901
2024-04-19 16:13:16,517 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:38901
2024-04-19 16:13:16,517 - distributed.worker - INFO -          dashboard at:         10.201.2.137:44961
2024-04-19 16:13:16,517 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,518 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,518 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xov_j6yl
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,517 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1370e3b8-4cf4-4002-bb82-bfc95387f681
2024-04-19 16:13:16,518 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:33635
2024-04-19 16:13:16,518 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:33635
2024-04-19 16:13:16,518 - distributed.worker - INFO -          dashboard at:         10.201.2.137:43127
2024-04-19 16:13:16,518 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,518 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,518 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2k7v457r
2024-04-19 16:13:16,518 - distributed.worker - INFO -          dashboard at:         10.201.2.137:42505
2024-04-19 16:13:16,518 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.150:8786
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,518 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,518 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vgo2aske
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,518 - distributed.worker - INFO -               Threads:                          8
2024-04-19 16:13:16,518 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 16:13:16,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fboldcmo
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:16,518 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,272 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:18,272 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,273 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:18,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,274 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:18,274 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,274 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:18,274 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:18,275 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,275 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:18,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 16:13:18,276 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.150:8786
2024-04-19 16:13:18,276 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 16:13:18,276 - distributed.core - INFO - Starting established connection to tcp://10.201.2.150:8786
2024-04-19 16:13:20,231 - distributed.utils_perf - INFO - full garbage collection released 466.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:20,770 - distributed.utils_perf - INFO - full garbage collection released 200.94 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:22,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:22,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:25,179 - distributed.utils_perf - INFO - full garbage collection released 294.65 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:26,568 - distributed.utils_perf - INFO - full garbage collection released 35.92 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:26,818 - distributed.utils_perf - INFO - full garbage collection released 61.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:27,669 - distributed.utils_perf - INFO - full garbage collection released 791.65 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:28,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:30,859 - distributed.utils_perf - INFO - full garbage collection released 14.11 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:31,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:32,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:32,073 - distributed.utils_perf - INFO - full garbage collection released 1.51 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:32,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:32,973 - distributed.utils_perf - INFO - full garbage collection released 861.47 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:34,961 - distributed.utils_perf - INFO - full garbage collection released 76.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:35,325 - distributed.utils_perf - INFO - full garbage collection released 1.06 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:35,564 - distributed.utils_perf - INFO - full garbage collection released 342.65 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:35,619 - distributed.utils_perf - INFO - full garbage collection released 443.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:36,988 - distributed.utils_perf - INFO - full garbage collection released 2.80 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:37,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:37,886 - distributed.utils_perf - INFO - full garbage collection released 215.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:38,379 - distributed.utils_perf - INFO - full garbage collection released 377.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:38,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:41,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:42,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:44,114 - distributed.utils_perf - INFO - full garbage collection released 71.15 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:44,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:44,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:46,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:48,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:50,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:52,939 - distributed.utils_perf - INFO - full garbage collection released 546.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:55,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:55,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:57,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:57,766 - distributed.utils_perf - INFO - full garbage collection released 58.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:13:57,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:58,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:13:59,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:00,624 - distributed.utils_perf - INFO - full garbage collection released 130.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:01,374 - distributed.utils_perf - INFO - full garbage collection released 202.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:04,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:07,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:10,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:11,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:12,171 - distributed.utils_perf - INFO - full garbage collection released 429.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:14,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:17,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:18,232 - distributed.utils_perf - INFO - full garbage collection released 107.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:19,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:21,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:23,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:25,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:28,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:29,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:31,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:33,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:42,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:42,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:42,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:44,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:46,694 - distributed.utils_perf - INFO - full garbage collection released 120.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:47,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:48,147 - distributed.utils_perf - INFO - full garbage collection released 1.22 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:14:49,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:50,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:50,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:53,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:54,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:55,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:56,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:57,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:58,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:14:59,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:03,584 - distributed.utils_perf - INFO - full garbage collection released 547.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:15:10,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:11,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:11,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:11,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:15,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:17,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:17,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:23,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:24,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:24,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:26,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:26,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:27,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:30,137 - distributed.utils_perf - INFO - full garbage collection released 83.80 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:15:30,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:32,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:35,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:37,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:38,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:38,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:38,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:41,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:42,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:47,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:49,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:50,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:51,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:52,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:54,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:15:59,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:01,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:02,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:05,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:05,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:07,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:10,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:13,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:15,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:19,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:22,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:23,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:24,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:24,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:31,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:34,949 - distributed.utils_perf - INFO - full garbage collection released 464.97 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:35,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:35,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:36,995 - distributed.utils_perf - INFO - full garbage collection released 543.73 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:39,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:42,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:43,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:47,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:47,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:49,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:49,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:49,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:53,565 - distributed.utils_perf - INFO - full garbage collection released 2.57 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:16:53,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:54,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:54,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:57,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:16:59,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:00,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:02,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:02,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:03,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:05,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:06,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:12,785 - distributed.utils_perf - INFO - full garbage collection released 1.40 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:17:13,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:14,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:19,177 - distributed.utils_perf - INFO - full garbage collection released 84.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:17:19,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:20,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:21,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:22,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:22,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:24,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:27,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:27,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:31,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:34,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:35,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:38,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:40,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:41,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:43,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:44,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:46,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:53,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:55,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:55,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:17:56,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:01,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:02,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:04,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:05,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:10,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:11,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:13,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:13,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:13,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:15,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:23,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:24,068 - distributed.utils_perf - INFO - full garbage collection released 504.85 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:18:25,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:26,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:30,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:31,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:31,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:31,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:33,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:34,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:38,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:39,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:40,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:41,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:43,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:50,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:51,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:52,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:54,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:54,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:56,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:18:57,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:04,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:05,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:07,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:07,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:07,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:08,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:09,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:11,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:12,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:18,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:19,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:20,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:25,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:27,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:28,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:28,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:29,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:30,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:31,825 - distributed.utils_perf - INFO - full garbage collection released 540.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:19:32,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:33,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:34,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:42,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:42,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:46,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:47,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:49,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:54,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:55,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:56,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:19:57,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:00,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:01,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:03,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:04,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:08,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:11,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:11,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:11,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:14,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:18,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:20,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:22,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:22,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:24,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:25,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:27,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:31,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:35,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:37,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:39,276 - distributed.utils_perf - INFO - full garbage collection released 2.15 GiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:20:39,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:40,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:44,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:44,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:45,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:47,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:47,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:48,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:50,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:54,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:55,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:20:57,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:00,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:01,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:06,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:07,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:13,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:16,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:19,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:24,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:24,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:31,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:32,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:33,817 - distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:21:37,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:37,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:43,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:43,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:49,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:49,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:50,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:50,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:57,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:58,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:59,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:21:59,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:00,394 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:22:04,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:04,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:08,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:09,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:11,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:13,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:20,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:22,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:24,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:24,470 - distributed.utils_perf - INFO - full garbage collection released 34.54 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:22:24,660 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 16:22:25,086 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 16:22:25,848 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:22:26,772 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:22:27,911 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:22:29,332 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:22:30,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:30,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:31,137 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 16:22:31,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:33,347 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 16:22:39,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:40,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:44,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:51,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:51,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:22:58,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:06,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:23:08,169 - distributed.utils_perf - INFO - full garbage collection released 12.84 MiB from 152 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:23:13,167 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:23:14,682 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:23:16,566 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 16:23:18,872 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 16:23:26,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:24:00] task [xgboost.dask-tcp://10.201.2.137:33635]:tcp://10.201.2.137:33635 got new rank 0
[16:24:00] task [xgboost.dask-tcp://10.201.2.137:38901]:tcp://10.201.2.137:38901 got new rank 1
[16:24:00] task [xgboost.dask-tcp://10.201.2.137:41379]:tcp://10.201.2.137:41379 got new rank 2
[16:24:00] task [xgboost.dask-tcp://10.201.2.137:44717]:tcp://10.201.2.137:44717 got new rank 3
[16:24:00] task [xgboost.dask-tcp://10.201.2.237:33819]:tcp://10.201.2.237:33819 got new rank 4
[16:24:00] task [xgboost.dask-tcp://10.201.2.237:37685]:tcp://10.201.2.237:37685 got new rank 5
[16:24:00] task [xgboost.dask-tcp://10.201.2.237:38515]:tcp://10.201.2.237:38515 got new rank 6
[16:24:00] task [xgboost.dask-tcp://10.201.2.237:41637]:tcp://10.201.2.237:41637 got new rank 7
2024-04-19 16:26:16,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:26:17,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:26:18,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:26:56] task [xgboost.dask-tcp://10.201.2.137:33635]:tcp://10.201.2.137:33635 got new rank 0
[16:26:56] task [xgboost.dask-tcp://10.201.2.137:38901]:tcp://10.201.2.137:38901 got new rank 1
[16:26:56] task [xgboost.dask-tcp://10.201.2.137:41379]:tcp://10.201.2.137:41379 got new rank 2
[16:26:56] task [xgboost.dask-tcp://10.201.2.137:44717]:tcp://10.201.2.137:44717 got new rank 3
[16:26:56] task [xgboost.dask-tcp://10.201.2.237:33819]:tcp://10.201.2.237:33819 got new rank 4
[16:26:56] task [xgboost.dask-tcp://10.201.2.237:37685]:tcp://10.201.2.237:37685 got new rank 5
[16:26:56] task [xgboost.dask-tcp://10.201.2.237:38515]:tcp://10.201.2.237:38515 got new rank 6
[16:26:56] task [xgboost.dask-tcp://10.201.2.237:41637]:tcp://10.201.2.237:41637 got new rank 7
2024-04-19 16:29:25,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:29:25,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:29:25,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:30:02] task [xgboost.dask-tcp://10.201.2.137:33635]:tcp://10.201.2.137:33635 got new rank 0
[16:30:02] task [xgboost.dask-tcp://10.201.2.137:38901]:tcp://10.201.2.137:38901 got new rank 1
[16:30:02] task [xgboost.dask-tcp://10.201.2.137:41379]:tcp://10.201.2.137:41379 got new rank 2
[16:30:02] task [xgboost.dask-tcp://10.201.2.137:44717]:tcp://10.201.2.137:44717 got new rank 3
[16:30:02] task [xgboost.dask-tcp://10.201.2.237:33819]:tcp://10.201.2.237:33819 got new rank 4
[16:30:02] task [xgboost.dask-tcp://10.201.2.237:37685]:tcp://10.201.2.237:37685 got new rank 5
[16:30:02] task [xgboost.dask-tcp://10.201.2.237:38515]:tcp://10.201.2.237:38515 got new rank 6
[16:30:02] task [xgboost.dask-tcp://10.201.2.237:41637]:tcp://10.201.2.237:41637 got new rank 7
2024-04-19 16:33:07,842 - distributed.utils_perf - INFO - full garbage collection released 72.45 MiB from 133 reference cycles (threshold: 9.54 MiB)
2024-04-19 16:33:10,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:33:11,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:33:11,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:33:21,619 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
2024-04-19 16:33:21,619 - distributed.utils_perf - INFO - full garbage collection released 632.80 MiB from 247 reference cycles (threshold: 9.54 MiB)
[16:33:48] task [xgboost.dask-tcp://10.201.2.137:33635]:tcp://10.201.2.137:33635 got new rank 0
[16:33:48] task [xgboost.dask-tcp://10.201.2.137:38901]:tcp://10.201.2.137:38901 got new rank 1
[16:33:48] task [xgboost.dask-tcp://10.201.2.137:41379]:tcp://10.201.2.137:41379 got new rank 2
[16:33:48] task [xgboost.dask-tcp://10.201.2.137:44717]:tcp://10.201.2.137:44717 got new rank 3
[16:33:48] task [xgboost.dask-tcp://10.201.2.237:33819]:tcp://10.201.2.237:33819 got new rank 4
[16:33:48] task [xgboost.dask-tcp://10.201.2.237:37685]:tcp://10.201.2.237:37685 got new rank 5
[16:33:48] task [xgboost.dask-tcp://10.201.2.237:38515]:tcp://10.201.2.237:38515 got new rank 6
[16:33:48] task [xgboost.dask-tcp://10.201.2.237:41637]:tcp://10.201.2.237:41637 got new rank 7
2024-04-19 16:36:09,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:09,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:10,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:10,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:23,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:23,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:25,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:25,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:25,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:25,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:26,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:26,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:36:45,902 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)
[16:37:09] task [xgboost.dask-tcp://10.201.2.137:33635]:tcp://10.201.2.137:33635 got new rank 0
[16:37:09] task [xgboost.dask-tcp://10.201.2.137:38901]:tcp://10.201.2.137:38901 got new rank 1
[16:37:09] task [xgboost.dask-tcp://10.201.2.137:41379]:tcp://10.201.2.137:41379 got new rank 2
[16:37:09] task [xgboost.dask-tcp://10.201.2.137:44717]:tcp://10.201.2.137:44717 got new rank 3
[16:37:09] task [xgboost.dask-tcp://10.201.2.237:33819]:tcp://10.201.2.237:33819 got new rank 4
[16:37:09] task [xgboost.dask-tcp://10.201.2.237:37685]:tcp://10.201.2.237:37685 got new rank 5
[16:37:09] task [xgboost.dask-tcp://10.201.2.237:38515]:tcp://10.201.2.237:38515 got new rank 6
[16:37:09] task [xgboost.dask-tcp://10.201.2.237:41637]:tcp://10.201.2.237:41637 got new rank 7
2024-04-19 16:39:19,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:41637. Reason: scheduler-close
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:37685. Reason: scheduler-close
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:33819. Reason: scheduler-close
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:38515. Reason: scheduler-close
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:44717. Reason: scheduler-close
2024-04-19 16:39:52,464 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:41379. Reason: scheduler-close
2024-04-19 16:39:52,465 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:38901. Reason: scheduler-close
2024-04-19 16:39:52,465 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:33635. Reason: scheduler-close
2024-04-19 16:39:52,467 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:40087'. Reason: scheduler-close
2024-04-19 16:39:52,467 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:33711'. Reason: scheduler-close
2024-04-19 16:39:52,467 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:41447'. Reason: scheduler-close
2024-04-19 16:39:52,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:41110 remote=tcp://10.201.2.150:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:41110 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:52,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:41140 remote=tcp://10.201.2.150:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:41140 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:52,469 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:42199'. Reason: scheduler-close
2024-04-19 16:39:52,470 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:40783'. Reason: scheduler-close
2024-04-19 16:39:52,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59848 remote=tcp://10.201.2.150:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59848 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:52,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59832 remote=tcp://10.201.2.150:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59832 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:52,466 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59842 remote=tcp://10.201.2.150:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:59842 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:52,475 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:36745'. Reason: scheduler-close
2024-04-19 16:39:52,475 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:43767'. Reason: scheduler-close
2024-04-19 16:39:52,475 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:33381'. Reason: scheduler-close
2024-04-19 16:39:53,252 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:39:53,252 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:39:53,252 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:35428 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:53,252 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:35450 remote=tcp://10.201.2.150:8786>: Stream is closed
/10.201.2.137:39866 remote=tcp://10.201.2.150:8786>: Stream is closed
/10.201.2.137:35444 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:53,260 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:39:53,260 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.237:33106 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:53,260 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:39:53,260 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.237:59864 remote=tcp://10.201.2.150:8786>: Stream is closed
/10.201.2.237:43274 remote=tcp://10.201.2.150:8786>: Stream is closed
/10.201.2.237:59874 remote=tcp://10.201.2.150:8786>: Stream is closed
2024-04-19 16:39:53,320 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,321 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,344 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,344 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,365 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,366 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,366 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,367 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,379 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,379 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,387 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,387 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,409 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,409 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:53,413 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.150:8786; closing.
2024-04-19 16:39:53,414 - distributed.nanny - INFO - Worker closed
2024-04-19 16:39:55,323 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:55,347 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:55,367 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:55,368 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:55,388 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:55,411 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:39:56,281 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:33711'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,282 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,290 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:40783'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,291 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,396 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:42199'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,397 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,693 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:33381'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,694 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:36745'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,702 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:43767'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,708 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:56,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:41447'. Reason: nanny-close-gracefully
2024-04-19 16:39:56,755 - distributed.dask_worker - INFO - End worker
2024-04-19 16:39:57,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:40087'. Reason: nanny-close-gracefully
2024-04-19 16:39:57,177 - distributed.dask_worker - INFO - End worker
