2024-04-19 15:41:05,335 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,335 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,336 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,336 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,342 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,342 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,342 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,342 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,741 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,741 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,741 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,741 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,742 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,742 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,742 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,742 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.222:39273'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.247:35315'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.222:36755'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.247:37601'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.222:34957'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.222:38181'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.247:43035'
2024-04-19 15:41:05,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.247:34715'
2024-04-19 15:41:06,781 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,781 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,781 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,781 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,782 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,782 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,782 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,782 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,792 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,793 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,793 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,793 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,794 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,794 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,825 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,825 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,825 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,826 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,838 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,838 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,839 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,830 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,830 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,830 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,831 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,843 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,843 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,843 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,843 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,929 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-20bf4633-b93a-4348-b7d0-04f47052aba9
2024-04-19 15:41:08,929 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7d85065c-2c0a-419c-bc20-559f3b3dc5a6
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.222:33419
2024-04-19 15:41:08,929 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.222:33419
2024-04-19 15:41:08,929 - distributed.worker - INFO -          dashboard at:         10.201.2.222:40387
2024-04-19 15:41:08,929 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-81218c72-12f2-43c2-8b1f-ecf31d43ec32
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.222:40015
2024-04-19 15:41:08,929 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.222:40015
2024-04-19 15:41:08,929 - distributed.worker - INFO -          dashboard at:         10.201.2.222:33499
2024-04-19 15:41:08,929 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.222:38281
2024-04-19 15:41:08,929 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.222:38281
2024-04-19 15:41:08,929 - distributed.worker - INFO -          dashboard at:         10.201.2.222:36487
2024-04-19 15:41:08,929 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,929 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jums4l7i
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,929 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gxum4bi3
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b5cae48b-3c06-4673-a8b5-02bf415eb0ce
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.222:42637
2024-04-19 15:41:08,929 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.222:42637
2024-04-19 15:41:08,929 - distributed.worker - INFO -          dashboard at:         10.201.2.222:36807
2024-04-19 15:41:08,929 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,929 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-889tk5kr
2024-04-19 15:41:08,929 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,929 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,929 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r5d599tg
2024-04-19 15:41:08,930 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,949 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-23056bb5-efaa-4157-9d7e-4a33cfa6deff
2024-04-19 15:41:08,949 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-10bd5e67-f8d8-425a-bb0f-ce08e761e594
2024-04-19 15:41:08,949 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.247:46401
2024-04-19 15:41:08,949 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.247:46401
2024-04-19 15:41:08,949 - distributed.worker - INFO -          dashboard at:         10.201.1.247:37697
2024-04-19 15:41:08,949 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.247:46303
2024-04-19 15:41:08,949 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.247:46303
2024-04-19 15:41:08,949 - distributed.worker - INFO -          dashboard at:         10.201.1.247:44937
2024-04-19 15:41:08,949 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,950 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,950 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9nl0x0j6
2024-04-19 15:41:08,949 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7e137784-83bc-4d69-844e-77f289609449
2024-04-19 15:41:08,949 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.247:33991
2024-04-19 15:41:08,949 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.247:33991
2024-04-19 15:41:08,950 - distributed.worker - INFO -          dashboard at:         10.201.1.247:33773
2024-04-19 15:41:08,950 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,949 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,950 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,950 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w3mizql1
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,949 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c1ebb8b1-146e-4638-9387-c4de067c7020
2024-04-19 15:41:08,949 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.247:32837
2024-04-19 15:41:08,949 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.247:32837
2024-04-19 15:41:08,949 - distributed.worker - INFO -          dashboard at:         10.201.1.247:46545
2024-04-19 15:41:08,950 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.142:8786
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,950 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,950 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4gh3t866
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,950 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,950 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9g10yaif
2024-04-19 15:41:08,950 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,709 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,709 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,709 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,710 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,711 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,711 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,711 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,713 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,714 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,714 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,716 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,716 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,716 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,717 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,717 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,718 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,718 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,718 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,719 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,719 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,719 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,719 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,719 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:12,720 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.142:8786
2024-04-19 15:41:12,720 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,720 - distributed.core - INFO - Starting established connection to tcp://10.201.2.142:8786
2024-04-19 15:41:21,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,417 - distributed.utils_perf - INFO - full garbage collection released 306.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:25,945 - distributed.utils_perf - INFO - full garbage collection released 59.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,380 - distributed.utils_perf - INFO - full garbage collection released 195.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,406 - distributed.utils_perf - INFO - full garbage collection released 43.37 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,175 - distributed.utils_perf - INFO - full garbage collection released 23.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:30,891 - distributed.utils_perf - INFO - full garbage collection released 2.39 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:31,209 - distributed.utils_perf - INFO - full garbage collection released 3.06 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:31,724 - distributed.utils_perf - INFO - full garbage collection released 1.77 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:32,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,542 - distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:33,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,219 - distributed.utils_perf - INFO - full garbage collection released 101.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:37,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,431 - distributed.utils_perf - INFO - full garbage collection released 5.52 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:38,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:39,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:41,484 - distributed.utils_perf - INFO - full garbage collection released 2.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:42,126 - distributed.utils_perf - INFO - full garbage collection released 1.87 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:42,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:50,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:53,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:54,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:57,877 - distributed.utils_perf - INFO - full garbage collection released 358.74 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:59,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:04,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:07,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:09,080 - distributed.utils_perf - INFO - full garbage collection released 3.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:09,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:13,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:16,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:17,037 - distributed.utils_perf - INFO - full garbage collection released 100.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:20,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:20,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:20,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:20,998 - distributed.utils_perf - INFO - full garbage collection released 688.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:22,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:23,202 - distributed.utils_perf - INFO - full garbage collection released 166.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:23,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:29,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,443 - distributed.utils_perf - INFO - full garbage collection released 300.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:30,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:31,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:32,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,560 - distributed.utils_perf - INFO - full garbage collection released 147.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:34,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:34,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:35,096 - distributed.utils_perf - INFO - full garbage collection released 148.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:35,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:35,223 - distributed.utils_perf - INFO - full garbage collection released 295.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:35,876 - distributed.utils_perf - INFO - full garbage collection released 301.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:38,369 - distributed.utils_perf - INFO - full garbage collection released 23.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:38,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:44,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:45,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:51,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:51,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:52,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:56,648 - distributed.utils_perf - INFO - full garbage collection released 229.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:57,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:58,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:58,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:05,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:07,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:09,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:15,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:25,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:28,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:30,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:34,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:36,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:38,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:41,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:41,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:41,322 - distributed.utils_perf - INFO - full garbage collection released 1.24 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:41,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:44,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:46,426 - distributed.utils_perf - INFO - full garbage collection released 98.18 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:47,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:50,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:54,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:58,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:59,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:59,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:02,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:02,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:05,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:07,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:08,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:12,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:13,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:14,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:15,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:21,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:23,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:25,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:25,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:26,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:27,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:30,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:35,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:40,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:42,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:43,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:47,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:48,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:48,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:52,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:53,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:55,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:59,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:07,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:08,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:12,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:12,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:14,208 - distributed.utils_perf - INFO - full garbage collection released 24.70 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:15,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:16,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:19,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:20,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:21,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:24,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:25,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:32,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:33,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:33,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:34,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:35,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:39,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:40,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:41,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:41,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:44,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:46,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:49,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:51,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:52,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:54,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:56,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:57,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:59,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:00,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:02,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:03,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:03,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:08,739 - distributed.utils_perf - INFO - full garbage collection released 265.19 MiB from 74 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:09,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:10,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:14,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:15,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:17,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:20,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:21,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:24,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:28,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:31,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:32,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:35,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:44,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:49,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:50,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:51,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:55,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:57,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:58,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:58,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:00,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:02,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:11,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:13,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:16,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:20,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:20,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:26,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:31,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:33,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:35,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:37,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:37,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:41,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:42,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:45,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:49,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:52,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:53,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:54,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:54,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:01,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:02,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:04,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,424 - distributed.utils_perf - INFO - full garbage collection released 0.94 GiB from 171 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:09,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:09,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:13,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:16,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:20,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:22,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:31,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:36,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:37,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:40,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:40,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:41,392 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:48:41,846 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:42,398 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:42,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,075 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:43,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,891 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:44,891 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:46,102 - distributed.utils_perf - INFO - full garbage collection released 9.70 MiB from 228 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:46,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:46,141 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:47,732 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:48,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,683 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:48:50,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:51,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,106 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:48:53,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:54,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:55,064 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:48:59,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:01,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:01,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:01,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:03,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:04,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:06,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:10,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:15,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:16,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:18,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:20,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:25,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:28,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:30,019 - distributed.utils_perf - INFO - full garbage collection released 1.57 GiB from 74 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:32,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:37,574 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:49:38,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:40,639 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:49:41,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:42,117 - distributed.utils_perf - INFO - full garbage collection released 12.67 MiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:43,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:44,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:45,240 - distributed.utils_perf - INFO - full garbage collection released 7.49 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:46,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:47,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:48,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:49,440 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:50,187 - distributed.utils_perf - INFO - full garbage collection released 32.89 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:50,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,049 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:51,102 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:51,783 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:52,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:52,674 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:53,125 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:53,786 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:54,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:55,165 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:55,638 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:56,897 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:57,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:58,738 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:59,047 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:59,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:01,695 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:50:03,460 - distributed.utils_perf - INFO - full garbage collection released 21.73 MiB from 190 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:05,001 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:50:05,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:06,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:08,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:10,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:12,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:14,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:16,016 - distributed.utils_perf - INFO - full garbage collection released 167.19 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:26,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:34,826 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 15:50:36,791 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 15:50:44,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:51:14] task [xgboost.dask-tcp://10.201.1.247:32837]:tcp://10.201.1.247:32837 got new rank 0
[15:51:14] task [xgboost.dask-tcp://10.201.1.247:33991]:tcp://10.201.1.247:33991 got new rank 1
[15:51:14] task [xgboost.dask-tcp://10.201.1.247:46303]:tcp://10.201.1.247:46303 got new rank 2
[15:51:14] task [xgboost.dask-tcp://10.201.1.247:46401]:tcp://10.201.1.247:46401 got new rank 3
[15:51:14] task [xgboost.dask-tcp://10.201.2.222:33419]:tcp://10.201.2.222:33419 got new rank 4
[15:51:14] task [xgboost.dask-tcp://10.201.2.222:38281]:tcp://10.201.2.222:38281 got new rank 5
[15:51:15] task [xgboost.dask-tcp://10.201.2.222:40015]:tcp://10.201.2.222:40015 got new rank 6
[15:51:15] task [xgboost.dask-tcp://10.201.2.222:42637]:tcp://10.201.2.222:42637 got new rank 7
2024-04-19 15:53:33,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:34,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:54:13] task [xgboost.dask-tcp://10.201.1.247:32837]:tcp://10.201.1.247:32837 got new rank 0
[15:54:13] task [xgboost.dask-tcp://10.201.1.247:33991]:tcp://10.201.1.247:33991 got new rank 1
[15:54:13] task [xgboost.dask-tcp://10.201.1.247:46303]:tcp://10.201.1.247:46303 got new rank 2
[15:54:13] task [xgboost.dask-tcp://10.201.1.247:46401]:tcp://10.201.1.247:46401 got new rank 3
[15:54:13] task [xgboost.dask-tcp://10.201.2.222:33419]:tcp://10.201.2.222:33419 got new rank 4
[15:54:13] task [xgboost.dask-tcp://10.201.2.222:38281]:tcp://10.201.2.222:38281 got new rank 5
[15:54:13] task [xgboost.dask-tcp://10.201.2.222:40015]:tcp://10.201.2.222:40015 got new rank 6
[15:54:13] task [xgboost.dask-tcp://10.201.2.222:42637]:tcp://10.201.2.222:42637 got new rank 7
[15:57:18] task [xgboost.dask-tcp://10.201.1.247:32837]:tcp://10.201.1.247:32837 got new rank 0
[15:57:18] task [xgboost.dask-tcp://10.201.1.247:33991]:tcp://10.201.1.247:33991 got new rank 1
[15:57:18] task [xgboost.dask-tcp://10.201.1.247:46303]:tcp://10.201.1.247:46303 got new rank 2
[15:57:18] task [xgboost.dask-tcp://10.201.1.247:46401]:tcp://10.201.1.247:46401 got new rank 3
[15:57:18] task [xgboost.dask-tcp://10.201.2.222:33419]:tcp://10.201.2.222:33419 got new rank 4
[15:57:18] task [xgboost.dask-tcp://10.201.2.222:38281]:tcp://10.201.2.222:38281 got new rank 5
[15:57:18] task [xgboost.dask-tcp://10.201.2.222:40015]:tcp://10.201.2.222:40015 got new rank 6
[15:57:18] task [xgboost.dask-tcp://10.201.2.222:42637]:tcp://10.201.2.222:42637 got new rank 7
2024-04-19 15:59:34,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:35,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:35,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:35,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:36,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:00:15] task [xgboost.dask-tcp://10.201.1.247:32837]:tcp://10.201.1.247:32837 got new rank 0
[16:00:15] task [xgboost.dask-tcp://10.201.1.247:33991]:tcp://10.201.1.247:33991 got new rank 1
[16:00:15] task [xgboost.dask-tcp://10.201.1.247:46303]:tcp://10.201.1.247:46303 got new rank 2
[16:00:15] task [xgboost.dask-tcp://10.201.1.247:46401]:tcp://10.201.1.247:46401 got new rank 3
[16:00:15] task [xgboost.dask-tcp://10.201.2.222:33419]:tcp://10.201.2.222:33419 got new rank 4
[16:00:15] task [xgboost.dask-tcp://10.201.2.222:38281]:tcp://10.201.2.222:38281 got new rank 5
[16:00:15] task [xgboost.dask-tcp://10.201.2.222:40015]:tcp://10.201.2.222:40015 got new rank 6
[16:00:15] task [xgboost.dask-tcp://10.201.2.222:42637]:tcp://10.201.2.222:42637 got new rank 7
2024-04-19 16:02:26,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:41,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:41,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:42,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:42,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:43,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:43,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:44,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:44,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:03:27] task [xgboost.dask-tcp://10.201.1.247:32837]:tcp://10.201.1.247:32837 got new rank 0
[16:03:27] task [xgboost.dask-tcp://10.201.1.247:33991]:tcp://10.201.1.247:33991 got new rank 1
[16:03:27] task [xgboost.dask-tcp://10.201.1.247:46303]:tcp://10.201.1.247:46303 got new rank 2
[16:03:27] task [xgboost.dask-tcp://10.201.1.247:46401]:tcp://10.201.1.247:46401 got new rank 3
[16:03:27] task [xgboost.dask-tcp://10.201.2.222:33419]:tcp://10.201.2.222:33419 got new rank 4
[16:03:27] task [xgboost.dask-tcp://10.201.2.222:38281]:tcp://10.201.2.222:38281 got new rank 5
[16:03:27] task [xgboost.dask-tcp://10.201.2.222:40015]:tcp://10.201.2.222:40015 got new rank 6
[16:03:27] task [xgboost.dask-tcp://10.201.2.222:42637]:tcp://10.201.2.222:42637 got new rank 7
2024-04-19 16:05:56,987 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.222:38281. Reason: scheduler-close
2024-04-19 16:05:56,987 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.247:46303. Reason: scheduler-close
2024-04-19 16:05:56,987 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.222:33419. Reason: scheduler-close
2024-04-19 16:05:56,987 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.247:33991. Reason: scheduler-close
2024-04-19 16:05:56,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.247:46401. Reason: scheduler-close
2024-04-19 16:05:56,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.222:40015. Reason: scheduler-close
2024-04-19 16:05:56,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.222:42637. Reason: scheduler-close
2024-04-19 16:05:56,987 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.247:32837. Reason: scheduler-close
2024-04-19 16:05:56,989 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.222:38181'. Reason: scheduler-close
2024-04-19 16:05:56,989 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.222:34957'. Reason: scheduler-close
2024-04-19 16:05:56,990 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.247:43035'. Reason: scheduler-close
2024-04-19 16:05:56,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.222:50740 remote=tcp://10.201.2.142:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.222:50740 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:56,988 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.222:50730 remote=tcp://10.201.2.142:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.222:50730 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:56,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.222:36755'. Reason: scheduler-close
2024-04-19 16:05:56,996 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.222:39273'. Reason: scheduler-close
2024-04-19 16:05:56,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59096 remote=tcp://10.201.2.142:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59096 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:56,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59098 remote=tcp://10.201.2.142:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59098 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:56,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59082 remote=tcp://10.201.2.142:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.247:59082 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:56,998 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.247:35315'. Reason: scheduler-close
2024-04-19 16:05:56,998 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.247:34715'. Reason: scheduler-close
2024-04-19 16:05:56,998 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.247:37601'. Reason: scheduler-close
2024-04-19 16:05:57,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.222:50842 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,768 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.222:50838 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,768 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.222:50754 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,768 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.222:50752 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.247:43304 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:05:57,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.247:43290 remote=tcp://10.201.2.142:8786>: Stream is closed
/10.201.1.247:59150 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.247:59118 remote=tcp://10.201.2.142:8786>: Stream is closed
2024-04-19 16:05:57,816 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,816 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,817 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,818 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,824 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,824 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,840 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,840 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,864 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,864 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,868 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,869 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,876 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,877 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:57,894 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.142:8786; closing.
2024-04-19 16:05:57,894 - distributed.nanny - INFO - Worker closed
2024-04-19 16:05:59,840 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,842 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,843 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,869 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,872 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:05:59,896 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:00,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.222:36755'. Reason: nanny-close-gracefully
2024-04-19 16:06:00,643 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:00,749 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.222:38181'. Reason: nanny-close-gracefully
2024-04-19 16:06:00,749 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:00,755 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.222:34957'. Reason: nanny-close-gracefully
2024-04-19 16:06:00,756 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:01,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.222:39273'. Reason: nanny-close-gracefully
2024-04-19 16:06:01,104 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:01,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.247:35315'. Reason: nanny-close-gracefully
2024-04-19 16:06:01,126 - distributed.dask_worker - INFO - End worker
