2024-04-19 17:16:15,580 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,600 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,581 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,589 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,599 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,604 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,608 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,608 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,893 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,895 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,895 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,895 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,917 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,917 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,917 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,917 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.41:38807'
2024-04-19 17:16:15,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.41:38227'
2024-04-19 17:16:15,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.41:44829'
2024-04-19 17:16:15,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.41:38815'
2024-04-19 17:16:15,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.29:35061'
2024-04-19 17:16:15,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.29:33945'
2024-04-19 17:16:15,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.29:46413'
2024-04-19 17:16:15,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.29:37609'
2024-04-19 17:16:16,916 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,916 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,916 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,917 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,917 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,917 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,917 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,917 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,957 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,957 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,957 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,957 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,957 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,957 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,957 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,958 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,960 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,960 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,960 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,961 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,001 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,001 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:17,918 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,918 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,918 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,918 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,970 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,970 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,970 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,970 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:18,981 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d6ffa7d7-c35b-498d-9e8b-4cd37ec8800c
2024-04-19 17:16:18,981 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b74906ce-3cbe-45fa-942e-628dc62c037d
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.41:42623
2024-04-19 17:16:18,981 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.41:42623
2024-04-19 17:16:18,981 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-217dc00c-e82a-4c06-82bd-622eb32a694e
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.41:35923
2024-04-19 17:16:18,981 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-77fdc87c-3e6d-4948-b49c-21473840ff0f
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.41:42369
2024-04-19 17:16:18,981 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.41:42369
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.41:39731
2024-04-19 17:16:18,981 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.41:39731
2024-04-19 17:16:18,981 - distributed.worker - INFO -          dashboard at:          10.201.3.41:39881
2024-04-19 17:16:18,981 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qhiudp0d
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -          dashboard at:          10.201.3.41:45863
2024-04-19 17:16:18,981 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ek4w31gi
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.41:35923
2024-04-19 17:16:18,981 - distributed.worker - INFO -          dashboard at:          10.201.3.41:44835
2024-04-19 17:16:18,981 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -          dashboard at:          10.201.3.41:33365
2024-04-19 17:16:18,981 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8udlmia7
2024-04-19 17:16:18,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,982 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9o5y712b
2024-04-19 17:16:18,982 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-93904a1e-de68-4b5d-b81d-ae5c39c88400
2024-04-19 17:16:19,057 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d8677679-f265-4879-a877-8b6e5502f167
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.29:36351
2024-04-19 17:16:19,057 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0b3955ad-762e-4d8b-802c-a4bf8eb874a2
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.29:46315
2024-04-19 17:16:19,057 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.29:46315
2024-04-19 17:16:19,057 - distributed.worker - INFO -          dashboard at:          10.201.3.29:44145
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.29:45165
2024-04-19 17:16:19,057 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.29:45165
2024-04-19 17:16:19,057 - distributed.worker - INFO -          dashboard at:          10.201.3.29:39539
2024-04-19 17:16:19,057 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:19,057 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b1b61dbe-f6f3-47e1-bdc3-0fd02b8f67e3
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Start worker at:    tcp://10.201.3.29:46749
2024-04-19 17:16:19,057 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.29:46749
2024-04-19 17:16:19,057 - distributed.worker - INFO -          dashboard at:          10.201.3.29:33125
2024-04-19 17:16:19,057 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:19,057 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:19,057 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ok8zlztm
2024-04-19 17:16:19,057 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO -          Listening to:    tcp://10.201.3.29:36351
2024-04-19 17:16:19,057 - distributed.worker - INFO -          dashboard at:          10.201.3.29:38823
2024-04-19 17:16:19,057 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:19,057 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:19,057 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h7a6y4in
2024-04-19 17:16:19,058 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.48:8786
2024-04-19 17:16:19,057 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:19,057 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:19,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wrms2os9
2024-04-19 17:16:19,058 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,057 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:19,058 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:19,058 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:19,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-juqjb9pn
2024-04-19 17:16:19,058 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,863 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,863 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,864 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,865 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,865 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,866 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,868 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,868 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,868 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,869 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,870 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,870 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,871 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,871 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,871 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,872 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,872 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,873 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,873 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,873 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,873 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:22,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:22,874 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.48:8786
2024-04-19 17:16:22,874 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:22,875 - distributed.core - INFO - Starting established connection to tcp://10.201.3.48:8786
2024-04-19 17:16:31,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:36,201 - distributed.utils_perf - INFO - full garbage collection released 713.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,329 - distributed.utils_perf - INFO - full garbage collection released 196.53 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:39,927 - distributed.utils_perf - INFO - full garbage collection released 494.32 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:40,871 - distributed.utils_perf - INFO - full garbage collection released 3.51 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,373 - distributed.utils_perf - INFO - full garbage collection released 3.49 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,067 - distributed.utils_perf - INFO - full garbage collection released 1.80 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,077 - distributed.utils_perf - INFO - full garbage collection released 2.70 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:45,422 - distributed.utils_perf - INFO - full garbage collection released 2.47 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,129 - distributed.utils_perf - INFO - full garbage collection released 1.64 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,984 - distributed.utils_perf - INFO - full garbage collection released 1.76 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:51,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:56,016 - distributed.utils_perf - INFO - full garbage collection released 21.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:59,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,129 - distributed.utils_perf - INFO - full garbage collection released 281.81 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:59,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:08,540 - distributed.utils_perf - INFO - full garbage collection released 856.59 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:08,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:11,383 - distributed.utils_perf - INFO - full garbage collection released 399.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:12,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,325 - distributed.utils_perf - INFO - full garbage collection released 32.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:15,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,546 - distributed.utils_perf - INFO - full garbage collection released 81.56 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:21,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,950 - distributed.utils_perf - INFO - full garbage collection released 113.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:33,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:40,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:40,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:40,610 - distributed.utils_perf - INFO - full garbage collection released 251.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:40,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,770 - distributed.utils_perf - INFO - full garbage collection released 135.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:48,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,925 - distributed.utils_perf - INFO - full garbage collection released 1.12 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:58,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:58,362 - distributed.utils_perf - INFO - full garbage collection released 234.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:58,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:58,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:19,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:19,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:21,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:41,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:42,658 - distributed.utils_perf - INFO - full garbage collection released 200.46 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:43,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:44,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,755 - distributed.utils_perf - INFO - full garbage collection released 147.39 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:46,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:52,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:54,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:56,954 - distributed.utils_perf - INFO - full garbage collection released 620.34 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:57,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:08,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:25,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,756 - distributed.utils_perf - INFO - full garbage collection released 87.28 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:29,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:52,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:58,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,027 - distributed.utils_perf - INFO - full garbage collection released 1.76 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:41,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:47,311 - distributed.utils_perf - INFO - full garbage collection released 34.89 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:48,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:04,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:12,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:43,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:43,873 - distributed.comm.tcp - INFO - Connection from tcp://10.201.3.41:37586 closed before handshake completed
2024-04-19 17:21:44,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:47,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:48,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:53,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:53,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:07,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:30,920 - distributed.utils_perf - INFO - full garbage collection released 700.30 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:31,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:44,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:44,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:51,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:04,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,248 - distributed.utils_perf - INFO - full garbage collection released 1.12 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:13,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:27,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:59,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,172 - distributed.utils_perf - INFO - full garbage collection released 12.89 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:05,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:20,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:33,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:33,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:36,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:40,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:43,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:54,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:57,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:58,221 - distributed.utils_perf - INFO - full garbage collection released 1.49 GiB from 132 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:59,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:59,620 - distributed.utils_perf - INFO - full garbage collection released 13.84 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:59,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,494 - distributed.utils_perf - INFO - full garbage collection released 379.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:02,284 - distributed.utils_perf - INFO - full garbage collection released 601.46 MiB from 133 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:02,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:05,282 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:25:06,843 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:25:07,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,767 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:25:10,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:12,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:13,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:14,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:17,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:31,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:41,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:42,248 - distributed.utils_perf - INFO - full garbage collection released 129.84 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:48,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:20] task [xgboost.dask-tcp://10.201.3.29:36351]:tcp://10.201.3.29:36351 got new rank 0
[17:26:20] task [xgboost.dask-tcp://10.201.3.29:45165]:tcp://10.201.3.29:45165 got new rank 1
[17:26:20] task [xgboost.dask-tcp://10.201.3.29:46315]:tcp://10.201.3.29:46315 got new rank 2
[17:26:20] task [xgboost.dask-tcp://10.201.3.29:46749]:tcp://10.201.3.29:46749 got new rank 3
[17:26:20] task [xgboost.dask-tcp://10.201.3.41:35923]:tcp://10.201.3.41:35923 got new rank 4
[17:26:21] task [xgboost.dask-tcp://10.201.3.41:39731]:tcp://10.201.3.41:39731 got new rank 5
[17:26:21] task [xgboost.dask-tcp://10.201.3.41:42369]:tcp://10.201.3.41:42369 got new rank 6
[17:26:21] task [xgboost.dask-tcp://10.201.3.41:42623]:tcp://10.201.3.41:42623 got new rank 7
[17:29:06] task [xgboost.dask-tcp://10.201.3.29:36351]:tcp://10.201.3.29:36351 got new rank 0
[17:29:06] task [xgboost.dask-tcp://10.201.3.29:45165]:tcp://10.201.3.29:45165 got new rank 1
[17:29:06] task [xgboost.dask-tcp://10.201.3.29:46315]:tcp://10.201.3.29:46315 got new rank 2
[17:29:06] task [xgboost.dask-tcp://10.201.3.29:46749]:tcp://10.201.3.29:46749 got new rank 3
[17:29:06] task [xgboost.dask-tcp://10.201.3.41:35923]:tcp://10.201.3.41:35923 got new rank 4
[17:29:06] task [xgboost.dask-tcp://10.201.3.41:39731]:tcp://10.201.3.41:39731 got new rank 5
[17:29:06] task [xgboost.dask-tcp://10.201.3.41:42369]:tcp://10.201.3.41:42369 got new rank 6
[17:29:06] task [xgboost.dask-tcp://10.201.3.41:42623]:tcp://10.201.3.41:42623 got new rank 7
2024-04-19 17:31:13,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:13,888 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:31:15,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:51] task [xgboost.dask-tcp://10.201.3.29:36351]:tcp://10.201.3.29:36351 got new rank 0
[17:31:51] task [xgboost.dask-tcp://10.201.3.29:45165]:tcp://10.201.3.29:45165 got new rank 1
[17:31:51] task [xgboost.dask-tcp://10.201.3.29:46315]:tcp://10.201.3.29:46315 got new rank 2
[17:31:51] task [xgboost.dask-tcp://10.201.3.29:46749]:tcp://10.201.3.29:46749 got new rank 3
[17:31:51] task [xgboost.dask-tcp://10.201.3.41:35923]:tcp://10.201.3.41:35923 got new rank 4
[17:31:51] task [xgboost.dask-tcp://10.201.3.41:39731]:tcp://10.201.3.41:39731 got new rank 5
[17:31:51] task [xgboost.dask-tcp://10.201.3.41:42369]:tcp://10.201.3.41:42369 got new rank 6
[17:31:51] task [xgboost.dask-tcp://10.201.3.41:42623]:tcp://10.201.3.41:42623 got new rank 7
2024-04-19 17:33:54,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:54,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:54,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:55,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:55,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:19,009 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
[17:34:39] task [xgboost.dask-tcp://10.201.3.29:36351]:tcp://10.201.3.29:36351 got new rank 0
[17:34:39] task [xgboost.dask-tcp://10.201.3.29:45165]:tcp://10.201.3.29:45165 got new rank 1
[17:34:39] task [xgboost.dask-tcp://10.201.3.29:46315]:tcp://10.201.3.29:46315 got new rank 2
[17:34:39] task [xgboost.dask-tcp://10.201.3.29:46749]:tcp://10.201.3.29:46749 got new rank 3
[17:34:39] task [xgboost.dask-tcp://10.201.3.41:35923]:tcp://10.201.3.41:35923 got new rank 4
[17:34:39] task [xgboost.dask-tcp://10.201.3.41:39731]:tcp://10.201.3.41:39731 got new rank 5
[17:34:39] task [xgboost.dask-tcp://10.201.3.41:42369]:tcp://10.201.3.41:42369 got new rank 6
[17:34:39] task [xgboost.dask-tcp://10.201.3.41:42623]:tcp://10.201.3.41:42623 got new rank 7
2024-04-19 17:36:37,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:38,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:51,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:51,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:52,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:52,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:52,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:53,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:53,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:36:54,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:37:38] task [xgboost.dask-tcp://10.201.3.29:36351]:tcp://10.201.3.29:36351 got new rank 0
[17:37:38] task [xgboost.dask-tcp://10.201.3.29:45165]:tcp://10.201.3.29:45165 got new rank 1
[17:37:38] task [xgboost.dask-tcp://10.201.3.29:46315]:tcp://10.201.3.29:46315 got new rank 2
[17:37:38] task [xgboost.dask-tcp://10.201.3.29:46749]:tcp://10.201.3.29:46749 got new rank 3
[17:37:38] task [xgboost.dask-tcp://10.201.3.41:35923]:tcp://10.201.3.41:35923 got new rank 4
[17:37:38] task [xgboost.dask-tcp://10.201.3.41:39731]:tcp://10.201.3.41:39731 got new rank 5
[17:37:38] task [xgboost.dask-tcp://10.201.3.41:42369]:tcp://10.201.3.41:42369 got new rank 6
[17:37:38] task [xgboost.dask-tcp://10.201.3.41:42623]:tcp://10.201.3.41:42623 got new rank 7
2024-04-19 17:39:31,173 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:40:07,902 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.41:39731. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.41:42623. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.41:42369. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.41:35923. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.29:36351. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.29:46315. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.29:45165. Reason: scheduler-close
2024-04-19 17:40:07,903 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.29:46749. Reason: scheduler-close
2024-04-19 17:40:07,904 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.29:33945'. Reason: scheduler-close
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38240 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38240 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,906 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.29:37609'. Reason: scheduler-close
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38228 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38228 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38210 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.29:38210 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,907 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.29:46413'. Reason: scheduler-close
2024-04-19 17:40:07,907 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.29:35061'. Reason: scheduler-close
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51740 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51740 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51716 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51716 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51728 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51728 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51746 remote=tcp://10.201.3.48:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.41:51746 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:07,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.41:38807'. Reason: scheduler-close
2024-04-19 17:40:07,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.41:44829'. Reason: scheduler-close
2024-04-19 17:40:07,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.41:38815'. Reason: scheduler-close
2024-04-19 17:40:07,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.41:38227'. Reason: scheduler-close
2024-04-19 17:40:08,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:08,676 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.29:38256 remote=tcp://10.201.3.48:8786>: Stream is closed
/10.201.3.29:39730 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.29:38266 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.29:38272 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,682 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:08,682 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.41:42836 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,682 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.41:42838 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,682 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.41:52864 remote=tcp://10.201.3.48:8786>: Stream is closed
/10.201.3.41:52882 remote=tcp://10.201.3.48:8786>: Stream is closed
2024-04-19 17:40:08,749 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,749 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,754 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,754 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,764 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,764 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,773 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,773 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,774 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,774 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,781 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,782 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,785 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,786 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:08,810 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.48:8786; closing.
2024-04-19 17:40:08,810 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:10,752 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,767 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,775 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,776 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,783 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:10,811 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:11,693 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.29:37609'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,694 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:11,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.29:46413'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,701 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:11,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.29:33945'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,736 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:11,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.41:38227'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,863 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:11,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.41:44829'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,870 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:11,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.41:38815'. Reason: nanny-close-gracefully
2024-04-19 17:40:11,876 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:12,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.29:35061'. Reason: nanny-close-gracefully
2024-04-19 17:40:12,135 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:12,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.41:38807'. Reason: nanny-close-gracefully
2024-04-19 17:40:12,311 - distributed.dask_worker - INFO - End worker
