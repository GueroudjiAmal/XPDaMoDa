2024-04-19 17:40:29,207 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:29,207 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:29,207 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:29,207 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:29,259 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:29,260 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:29,260 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:29,260 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:29,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:44761'
2024-04-19 17:40:29,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:42225'
2024-04-19 17:40:29,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:36349'
2024-04-19 17:40:29,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.159:38745'
2024-04-19 17:40:30,139 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:30,140 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:30,140 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:30,140 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:30,140 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,140 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,140 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,141 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,183 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,183 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,183 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,184 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,299 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,299 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,299 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,299 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:30,446 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,446 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,446 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,446 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:30,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:42655'
2024-04-19 17:40:30,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:39609'
2024-04-19 17:40:30,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:42091'
2024-04-19 17:40:30,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.163:41161'
2024-04-19 17:40:30,785 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:30,785 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:30,785 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:30,786 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:31,334 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:31,335 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:31,336 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:31,336 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:31,336 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:31,336 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:31,337 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:31,337 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:31,377 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:31,379 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:31,379 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:31,379 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:31,785 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-eebce8ba-0ed5-4693-83b6-dfa0a4698f58
2024-04-19 17:40:31,785 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-24066c67-086b-47f2-9894-3cc3ffa6db4d
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:42465
2024-04-19 17:40:31,786 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:42465
2024-04-19 17:40:31,785 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-ef366387-80d0-490c-8b4f-b44bcfbc7912
2024-04-19 17:40:31,785 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:43545
2024-04-19 17:40:31,786 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:43545
2024-04-19 17:40:31,786 - distributed.worker - INFO -          dashboard at:         10.201.2.159:38853
2024-04-19 17:40:31,786 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:39377
2024-04-19 17:40:31,786 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:39377
2024-04-19 17:40:31,786 - distributed.worker - INFO -          dashboard at:         10.201.2.159:38499
2024-04-19 17:40:31,786 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:31,786 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g4y1ecxw
2024-04-19 17:40:31,786 - distributed.worker - INFO -          dashboard at:         10.201.2.159:46723
2024-04-19 17:40:31,786 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:31,785 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-cde269a4-aad2-4dbb-994d-2fbfb1d109dd
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.159:43279
2024-04-19 17:40:31,786 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.159:43279
2024-04-19 17:40:31,786 - distributed.worker - INFO -          dashboard at:         10.201.2.159:33301
2024-04-19 17:40:31,786 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:31,786 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vl9xag1n
2024-04-19 17:40:31,786 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wzgtmrl3
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:31,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibbqjihb
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,786 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,204 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:32,204 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:32,204 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:32,204 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:33,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:33,157 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,157 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,158 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:33,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:33,159 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,159 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,159 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:33,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:33,160 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,160 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,161 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:33,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:33,162 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,162 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,162 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:33,246 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b0ed8adb-eae2-4dfe-b3d1-7419af4159b8
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:44435
2024-04-19 17:40:33,247 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b3f78944-72fb-4129-a315-e3945aefea25
2024-04-19 17:40:33,247 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8c0d1fd4-e4a2-4ce8-b921-ac7f0a5e83d5
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:41235
2024-04-19 17:40:33,247 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:41235
2024-04-19 17:40:33,247 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-28a40139-7c16-4873-a034-548bf18ba09b
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:44615
2024-04-19 17:40:33,247 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:44615
2024-04-19 17:40:33,247 - distributed.worker - INFO -          dashboard at:         10.201.2.163:44859
2024-04-19 17:40:33,247 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:44435
2024-04-19 17:40:33,247 - distributed.worker - INFO -          dashboard at:         10.201.2.163:43839
2024-04-19 17:40:33,247 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:33,247 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wxfi3ogw
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.163:44671
2024-04-19 17:40:33,247 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.163:44671
2024-04-19 17:40:33,247 - distributed.worker - INFO -          dashboard at:         10.201.2.163:34569
2024-04-19 17:40:33,247 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:33,247 - distributed.worker - INFO -          dashboard at:         10.201.2.163:34105
2024-04-19 17:40:33,247 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:33,247 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xplcbenl
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.154:8786
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:33,247 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r8uukf2m
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:33,247 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:33,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wn1ugt5g
2024-04-19 17:40:33,247 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:34,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:34,434 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:34,434 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:34,435 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:34,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:34,436 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:34,436 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:34,436 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:34,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:34,437 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:34,437 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:34,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:34,438 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:34,438 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.154:8786
2024-04-19 17:40:34,438 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:34,438 - distributed.core - INFO - Starting established connection to tcp://10.201.2.154:8786
2024-04-19 17:40:37,680 - distributed.utils_perf - INFO - full garbage collection released 260.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:37,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:37,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:37,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:37,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:37,932 - distributed.utils_perf - INFO - full garbage collection released 439.18 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:41,972 - distributed.utils_perf - INFO - full garbage collection released 1.08 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:42,665 - distributed.utils_perf - INFO - full garbage collection released 371.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:47,521 - distributed.utils_perf - INFO - full garbage collection released 375.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:50,457 - distributed.utils_perf - INFO - full garbage collection released 0.91 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:51,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:51,401 - distributed.utils_perf - INFO - full garbage collection released 158.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:51,596 - distributed.utils_perf - INFO - full garbage collection released 401.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:53,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:54,631 - distributed.utils_perf - INFO - full garbage collection released 4.78 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:54,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:55,381 - distributed.utils_perf - INFO - full garbage collection released 319.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:56,086 - distributed.utils_perf - INFO - full garbage collection released 2.73 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:56,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:58,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:00,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:01,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:02,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:03,848 - distributed.utils_perf - INFO - full garbage collection released 285.15 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:04,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:09,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:09,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:10,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:10,847 - distributed.utils_perf - INFO - full garbage collection released 136.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:12,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:13,093 - distributed.utils_perf - INFO - full garbage collection released 477.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:14,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:16,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:17,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:23,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:23,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:23,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:23,514 - distributed.utils_perf - INFO - full garbage collection released 193.01 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:26,229 - distributed.utils_perf - INFO - full garbage collection released 296.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:26,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:26,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:32,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:33,885 - distributed.utils_perf - INFO - full garbage collection released 158.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:34,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:34,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:34,877 - distributed.utils_perf - INFO - full garbage collection released 278.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:34,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:35,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:36,946 - distributed.utils_perf - INFO - full garbage collection released 243.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:39,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:42,508 - distributed.utils_perf - INFO - full garbage collection released 216.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:42,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:43,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:43,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:43,167 - distributed.utils_perf - INFO - full garbage collection released 4.56 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:43,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:48,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:56,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:56,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:57,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:59,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:59,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:00,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:02,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:05,101 - distributed.utils_perf - INFO - full garbage collection released 0.95 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:07,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:09,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:09,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:10,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:11,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:13,549 - distributed.utils_perf - INFO - full garbage collection released 159.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:15,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:19,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:23,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:26,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:28,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:31,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:31,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:32,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:36,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:38,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:39,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:40,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:44,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:47,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:48,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:53,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:55,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:56,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:56,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:56,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:02,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:02,606 - distributed.utils_perf - INFO - full garbage collection released 72.86 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:03,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:05,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:05,476 - distributed.utils_perf - INFO - full garbage collection released 423.32 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:07,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:10,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:12,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:15,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:16,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:16,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:23,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:24,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:24,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:27,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:28,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:29,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:31,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:32,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:33,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:34,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:36,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:37,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:41,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:42,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:48,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:49,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:52,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:53,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:56,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:56,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:58,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:01,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:02,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:04,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:06,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:07,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:10,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:14,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:15,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:16,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:18,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:21,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:21,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:21,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:28,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:29,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:34,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:36,840 - distributed.utils_perf - INFO - full garbage collection released 1.39 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:37,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:38,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:38,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:39,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:43,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:43,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:43,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:47,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:48,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:48,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:54,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:56,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:00,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:02,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:04,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:06,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:06,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:08,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:09,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:12,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:16,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:17,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:20,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:20,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:22,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:30,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:30,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:32,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:33,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:35,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:35,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:39,265 - distributed.utils_perf - INFO - full garbage collection released 305.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:45:39,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:41,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:46,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:47,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:47,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:50,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:54,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:54,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:57,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:58,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:58,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:00,673 - distributed.utils_perf - INFO - full garbage collection released 291.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:46:04,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:05,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:08,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:08,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:11,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:13,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:13,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:17,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:18,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:28,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:28,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:29,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:30,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:33,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:36,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:36,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:36,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:40,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:41,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:41,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:43,776 - distributed.utils_perf - INFO - full garbage collection released 0.98 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:46:44,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:44,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:46,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:47,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:49,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:49,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:54,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:55,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:00,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:02,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:04,386 - distributed.utils_perf - INFO - full garbage collection released 12.80 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:10,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:11,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:14,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:17,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:21,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:22,606 - distributed.utils_perf - INFO - full garbage collection released 149.71 MiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:24,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:24,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:25,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:25,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:29,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:31,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:35,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:35,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:36,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:38,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:38,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:43,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:43,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:44,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:45,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:48,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:50,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:53,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:53,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:54,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:58,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:59,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:03,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:03,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:09,139 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:48:09,499 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:48:09,937 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:10,465 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:11,116 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:11,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:11,919 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:12,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:12,900 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:14,115 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:48:15,614 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:48:17,525 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:48:17,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:19,885 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:48:22,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:23,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:25,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:25,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:31,220 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:48:32,868 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:48:34,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:34,898 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:48:35,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:36,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:37,417 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:48:40,498 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:48:42,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:43,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:47,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:48,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:48,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:01,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:07,083 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:49:07,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:10,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:12,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:13,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:19,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:21,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:27,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:29,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:31,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:35,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:37,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:40,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:42,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:45,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:51,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:51,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:53,017 - distributed.utils_perf - INFO - full garbage collection released 1.48 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:49:56,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:57,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:59,563 - distributed.utils_perf - INFO - full garbage collection released 1.51 GiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:50:04,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:12,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:15,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:16,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:18,572 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:50:18,906 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:50:19,304 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:50:19,787 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:20,369 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:21,092 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:21,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:21,975 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:22,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:23,102 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:24,473 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:50:26,152 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:50:26,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:28,252 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:50:30,834 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:50:32,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:34,013 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:50:39,389 - distributed.utils_perf - INFO - full garbage collection released 83.72 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:50:41,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:44,487 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:50:45,557 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:50:46,876 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:50:47,232 - distributed.utils_perf - WARNING - full garbage collections took 57% CPU time recently (threshold: 10%)
2024-04-19 17:50:48,502 - distributed.utils_perf - WARNING - full garbage collections took 37% CPU time recently (threshold: 10%)
2024-04-19 17:50:50,513 - distributed.utils_perf - WARNING - full garbage collections took 33% CPU time recently (threshold: 10%)
2024-04-19 17:50:53,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:58,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:51:30] task [xgboost.dask-tcp://10.201.2.159:39377]:tcp://10.201.2.159:39377 got new rank 0
[17:51:30] task [xgboost.dask-tcp://10.201.2.159:42465]:tcp://10.201.2.159:42465 got new rank 1
[17:51:30] task [xgboost.dask-tcp://10.201.2.159:43279]:tcp://10.201.2.159:43279 got new rank 2
[17:51:30] task [xgboost.dask-tcp://10.201.2.159:43545]:tcp://10.201.2.159:43545 got new rank 3
[17:51:30] task [xgboost.dask-tcp://10.201.2.163:41235]:tcp://10.201.2.163:41235 got new rank 4
[17:51:30] task [xgboost.dask-tcp://10.201.2.163:44435]:tcp://10.201.2.163:44435 got new rank 5
[17:51:30] task [xgboost.dask-tcp://10.201.2.163:44615]:tcp://10.201.2.163:44615 got new rank 6
[17:51:30] task [xgboost.dask-tcp://10.201.2.163:44671]:tcp://10.201.2.163:44671 got new rank 7
2024-04-19 17:53:52,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:53:52,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:53:52,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:54:35] task [xgboost.dask-tcp://10.201.2.159:39377]:tcp://10.201.2.159:39377 got new rank 0
[17:54:35] task [xgboost.dask-tcp://10.201.2.159:42465]:tcp://10.201.2.159:42465 got new rank 1
[17:54:35] task [xgboost.dask-tcp://10.201.2.159:43279]:tcp://10.201.2.159:43279 got new rank 2
[17:54:35] task [xgboost.dask-tcp://10.201.2.159:43545]:tcp://10.201.2.159:43545 got new rank 3
[17:54:35] task [xgboost.dask-tcp://10.201.2.163:41235]:tcp://10.201.2.163:41235 got new rank 4
[17:54:35] task [xgboost.dask-tcp://10.201.2.163:44435]:tcp://10.201.2.163:44435 got new rank 5
[17:54:35] task [xgboost.dask-tcp://10.201.2.163:44615]:tcp://10.201.2.163:44615 got new rank 6
[17:54:35] task [xgboost.dask-tcp://10.201.2.163:44671]:tcp://10.201.2.163:44671 got new rank 7
2024-04-19 17:57:07,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:57:07,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:57:45] task [xgboost.dask-tcp://10.201.2.159:39377]:tcp://10.201.2.159:39377 got new rank 0
[17:57:45] task [xgboost.dask-tcp://10.201.2.159:42465]:tcp://10.201.2.159:42465 got new rank 1
[17:57:45] task [xgboost.dask-tcp://10.201.2.159:43279]:tcp://10.201.2.159:43279 got new rank 2
[17:57:45] task [xgboost.dask-tcp://10.201.2.159:43545]:tcp://10.201.2.159:43545 got new rank 3
[17:57:45] task [xgboost.dask-tcp://10.201.2.163:41235]:tcp://10.201.2.163:41235 got new rank 4
[17:57:45] task [xgboost.dask-tcp://10.201.2.163:44435]:tcp://10.201.2.163:44435 got new rank 5
[17:57:45] task [xgboost.dask-tcp://10.201.2.163:44615]:tcp://10.201.2.163:44615 got new rank 6
[17:57:45] task [xgboost.dask-tcp://10.201.2.163:44671]:tcp://10.201.2.163:44671 got new rank 7
2024-04-19 17:59:57,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:59:57,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:59:58,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:59:58,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[18:00:37] task [xgboost.dask-tcp://10.201.2.159:39377]:tcp://10.201.2.159:39377 got new rank 0
[18:00:37] task [xgboost.dask-tcp://10.201.2.159:42465]:tcp://10.201.2.159:42465 got new rank 1
[18:00:37] task [xgboost.dask-tcp://10.201.2.159:43279]:tcp://10.201.2.159:43279 got new rank 2
[18:00:37] task [xgboost.dask-tcp://10.201.2.159:43545]:tcp://10.201.2.159:43545 got new rank 3
[18:00:37] task [xgboost.dask-tcp://10.201.2.163:41235]:tcp://10.201.2.163:41235 got new rank 4
[18:00:37] task [xgboost.dask-tcp://10.201.2.163:44435]:tcp://10.201.2.163:44435 got new rank 5
[18:00:37] task [xgboost.dask-tcp://10.201.2.163:44615]:tcp://10.201.2.163:44615 got new rank 6
[18:00:37] task [xgboost.dask-tcp://10.201.2.163:44671]:tcp://10.201.2.163:44671 got new rank 7
2024-04-19 18:02:52,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:53,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:53,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:53,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:54,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:06,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:07,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:08,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:08,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:08,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:08,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:09,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:10,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[18:03:53] task [xgboost.dask-tcp://10.201.2.159:39377]:tcp://10.201.2.159:39377 got new rank 0
[18:03:53] task [xgboost.dask-tcp://10.201.2.159:42465]:tcp://10.201.2.159:42465 got new rank 1
[18:03:53] task [xgboost.dask-tcp://10.201.2.159:43279]:tcp://10.201.2.159:43279 got new rank 2
[18:03:53] task [xgboost.dask-tcp://10.201.2.159:43545]:tcp://10.201.2.159:43545 got new rank 3
[18:03:53] task [xgboost.dask-tcp://10.201.2.163:41235]:tcp://10.201.2.163:41235 got new rank 4
[18:03:53] task [xgboost.dask-tcp://10.201.2.163:44435]:tcp://10.201.2.163:44435 got new rank 5
[18:03:53] task [xgboost.dask-tcp://10.201.2.163:44615]:tcp://10.201.2.163:44615 got new rank 6
[18:03:53] task [xgboost.dask-tcp://10.201.2.163:44671]:tcp://10.201.2.163:44671 got new rank 7
2024-04-19 18:06:00,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:06:35,336 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:39377. Reason: scheduler-close
2024-04-19 18:06:35,336 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:44435. Reason: scheduler-close
2024-04-19 18:06:35,336 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:43279. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:42465. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:44671. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.159:43545. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:41235. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.163:44615. Reason: scheduler-close
2024-04-19 18:06:35,338 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:42655'. Reason: scheduler-close
2024-04-19 18:06:35,338 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:41161'. Reason: scheduler-close
2024-04-19 18:06:35,338 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.163:40048 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.163:40048 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,339 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:39609'. Reason: scheduler-close
2024-04-19 18:06:35,337 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.163:40032 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.163:40032 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.163:42091'. Reason: scheduler-close
2024-04-19 18:06:35,338 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54772 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54772 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,338 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54792 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54792 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,338 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54788 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54788 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,338 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54778 remote=tcp://10.201.2.154:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.159:54778 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:35,347 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:36349'. Reason: scheduler-close
2024-04-19 18:06:35,347 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:42225'. Reason: scheduler-close
2024-04-19 18:06:35,347 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:44761'. Reason: scheduler-close
2024-04-19 18:06:35,347 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.159:38745'. Reason: scheduler-close
2024-04-19 18:06:36,144 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:46244 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:36,145 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:46230 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:36,145 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:06:36,146 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.163:46260 remote=tcp://10.201.2.154:8786>: Stream is closed
/10.201.2.163:46274 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:36,149 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,150 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,153 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,153 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,153 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:06:36,152 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:06:36,153 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:06:36,153 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.159:54800 remote=tcp://10.201.2.154:8786>: Stream is closed
/10.201.2.159:34416 remote=tcp://10.201.2.154:8786>: Stream is closed
/10.201.2.159:48148 remote=tcp://10.201.2.154:8786>: Stream is closed
/10.201.2.159:34432 remote=tcp://10.201.2.154:8786>: Stream is closed
2024-04-19 18:06:36,206 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,206 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,213 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,213 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,248 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,248 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,248 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,248 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,290 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,291 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:36,306 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.154:8786; closing.
2024-04-19 18:06:36,307 - distributed.nanny - INFO - Worker closed
2024-04-19 18:06:38,234 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:06:38,248 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:06:38,251 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:06:38,292 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:06:38,308 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:06:39,023 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:39609'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,023 - distributed.dask_worker - INFO - End worker
2024-04-19 18:06:39,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:42655'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,064 - distributed.dask_worker - INFO - End worker
2024-04-19 18:06:39,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:41161'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,136 - distributed.dask_worker - INFO - End worker
2024-04-19 18:06:39,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.163:42091'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,468 - distributed.dask_worker - INFO - End worker
2024-04-19 18:06:39,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:38745'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,521 - distributed.dask_worker - INFO - End worker
2024-04-19 18:06:39,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.159:42225'. Reason: nanny-close-gracefully
2024-04-19 18:06:39,529 - distributed.dask_worker - INFO - End worker
