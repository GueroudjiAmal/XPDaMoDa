2024-04-19 17:26:02,158 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,158 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,158 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,158 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,367 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,367 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,367 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,367 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.165:44827'
2024-04-19 17:26:02,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.165:40119'
2024-04-19 17:26:02,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.165:44199'
2024-04-19 17:26:02,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.165:37089'
2024-04-19 17:26:02,639 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,640 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,640 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,640 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:02,886 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,886 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,886 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,886 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:02,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.162:33117'
2024-04-19 17:26:02,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.162:37917'
2024-04-19 17:26:02,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.162:45785'
2024-04-19 17:26:02,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.162:45245'
2024-04-19 17:26:03,092 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,093 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,095 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,095 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,095 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,096 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,096 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,097 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,115 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,118 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,118 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,118 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,895 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,895 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,895 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,895 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:26:03,896 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,896 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,896 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,896 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:26:03,934 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:03,934 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:03,934 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:03,935 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:03,941 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,942 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,942 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:03,942 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:26:04,915 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:04,915 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:04,915 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:04,915 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:26:05,006 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4e8cd91e-be96-4f9a-ae27-b31205452550
2024-04-19 17:26:05,006 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a516762b-3066-4bf9-8625-ff307157b01f
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.165:35127
2024-04-19 17:26:05,006 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.165:35127
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.165:40809
2024-04-19 17:26:05,006 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.165:40809
2024-04-19 17:26:05,006 - distributed.worker - INFO -          dashboard at:         10.201.1.165:36209
2024-04-19 17:26:05,006 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,006 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j3jhn4nd
2024-04-19 17:26:05,006 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f367b9ae-e741-4dd9-a999-3c89e5ad9af2
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.165:35403
2024-04-19 17:26:05,006 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.165:35403
2024-04-19 17:26:05,006 - distributed.worker - INFO -          dashboard at:         10.201.1.165:43133
2024-04-19 17:26:05,006 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,006 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4lpf_mni
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b75da0df-52eb-4955-a1e6-2c148bc6204e
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.165:38513
2024-04-19 17:26:05,006 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.165:38513
2024-04-19 17:26:05,006 - distributed.worker - INFO -          dashboard at:         10.201.1.165:45593
2024-04-19 17:26:05,006 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,006 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kc9t31nb
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO -          dashboard at:         10.201.1.165:34455
2024-04-19 17:26:05,006 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,006 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bt6p6b5u
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,980 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3d0aed7f-7384-4dda-9a5b-9cbfacaa814a
2024-04-19 17:26:05,980 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-28146c4b-e162-49eb-81fb-479987874012
2024-04-19 17:26:05,980 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.162:38369
2024-04-19 17:26:05,980 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-eb513c48-5624-4ff2-bde7-c044d6c6e6c1
2024-04-19 17:26:05,980 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.162:38643
2024-04-19 17:26:05,980 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.162:38643
2024-04-19 17:26:05,980 - distributed.worker - INFO -          dashboard at:         10.201.1.162:33807
2024-04-19 17:26:05,981 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,980 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2cb8c92c-5ff4-46fa-b3bd-1479076cd66f
2024-04-19 17:26:05,980 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.162:42897
2024-04-19 17:26:05,981 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.162:42897
2024-04-19 17:26:05,981 - distributed.worker - INFO -          dashboard at:         10.201.1.162:37591
2024-04-19 17:26:05,981 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,980 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.162:37101
2024-04-19 17:26:05,980 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.162:37101
2024-04-19 17:26:05,980 - distributed.worker - INFO -          dashboard at:         10.201.1.162:43799
2024-04-19 17:26:05,981 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ex6bju4s
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,980 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.162:38369
2024-04-19 17:26:05,981 - distributed.worker - INFO -          dashboard at:         10.201.1.162:46537
2024-04-19 17:26:05,981 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.203:8786
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shych3u9
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_1f3kz3k
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:05,981 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:26:05,981 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:26:05,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k1jjda31
2024-04-19 17:26:05,981 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,501 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,501 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,501 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,503 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,503 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,504 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,506 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,507 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,507 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,508 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,508 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,508 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,509 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,509 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,510 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,510 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,510 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,511 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,511 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,512 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,512 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:08,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:26:08,513 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.203:8786
2024-04-19 17:26:08,513 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:26:08,513 - distributed.core - INFO - Starting established connection to tcp://10.201.3.203:8786
2024-04-19 17:26:19,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:19,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:25,877 - distributed.utils_perf - INFO - full garbage collection released 76.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:26,289 - distributed.utils_perf - INFO - full garbage collection released 93.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:26,384 - distributed.utils_perf - INFO - full garbage collection released 613.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:27,795 - distributed.utils_perf - INFO - full garbage collection released 101.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:28,025 - distributed.utils_perf - INFO - full garbage collection released 2.21 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:28,426 - distributed.utils_perf - INFO - full garbage collection released 443.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:30,282 - distributed.utils_perf - INFO - full garbage collection released 37.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:31,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:31,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:31,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:31,951 - distributed.utils_perf - INFO - full garbage collection released 2.77 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:32,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:32,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:33,462 - distributed.utils_perf - INFO - full garbage collection released 1.26 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:33,857 - distributed.utils_perf - INFO - full garbage collection released 1.86 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:34,807 - distributed.utils_perf - INFO - full garbage collection released 6.11 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:35,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:36,526 - distributed.utils_perf - INFO - full garbage collection released 1.80 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:38,249 - distributed.utils_perf - INFO - full garbage collection released 3.37 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:38,538 - distributed.utils_perf - INFO - full garbage collection released 1.28 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:38,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:38,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:41,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:42,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:45,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:48,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:48,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:48,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:49,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:49,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:50,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:51,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:54,458 - distributed.utils_perf - INFO - full garbage collection released 186.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:55,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:56,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:56,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:57,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:58,216 - distributed.utils_perf - INFO - full garbage collection released 41.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:26:58,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:26:59,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:00,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:01,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:04,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:04,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:08,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:10,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:16,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:17,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:18,303 - distributed.utils_perf - INFO - full garbage collection released 1.35 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:27:18,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:21,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:23,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:24,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:25,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:25,485 - distributed.utils_perf - INFO - full garbage collection released 829.21 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:27:27,289 - distributed.utils_perf - INFO - full garbage collection released 175.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:27:27,539 - distributed.utils_perf - INFO - full garbage collection released 262.68 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:27:30,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:30,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:32,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:36,179 - distributed.utils_perf - INFO - full garbage collection released 22.33 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:27:36,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:40,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:42,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:42,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:48,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:50,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:51,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:52,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:54,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:54,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:27:56,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:00,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:00,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:02,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:05,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:07,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:09,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:11,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:11,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:12,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:15,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:16,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 51.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:19,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:19,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:19,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:21,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:25,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:26,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:27,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:31,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:32,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:34,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:36,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:37,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:37,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:38,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:41,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:43,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:46,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:47,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:49,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:51,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:52,011 - distributed.utils_perf - INFO - full garbage collection released 241.02 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:28:52,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:54,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:55,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:28:55,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:01,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:03,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:05,475 - distributed.utils_perf - INFO - full garbage collection released 288.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:29:06,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:07,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:07,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:10,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:12,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:15,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:17,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:20,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:22,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:22,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:24,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:25,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:26,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:28,690 - distributed.utils_perf - INFO - full garbage collection released 2.52 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:29:31,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:32,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:35,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:36,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:44,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:48,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:50,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:51,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:53,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:29:56,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:00,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:04,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:05,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:06,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:10,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:12,712 - distributed.utils_perf - INFO - full garbage collection released 291.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:30:13,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:14,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:15,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:15,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:17,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:19,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:22,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:24,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:26,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:33,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:36,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:37,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:37,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:41,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:44,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:44,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:46,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:47,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:52,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:52,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:53,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:55,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:57,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:58,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:02,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:03,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:05,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:08,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:10,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:16,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:18,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:20,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:21,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:22,132 - distributed.utils_perf - INFO - full garbage collection released 820.66 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:31:24,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:24,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:24,839 - distributed.utils_perf - INFO - full garbage collection released 14.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:31:32,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 55.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:33,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:38,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:38,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:40,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:43,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:43,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:44,234 - distributed.utils_perf - INFO - full garbage collection released 131.65 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:31:44,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:45,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:45,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:47,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:51,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:00,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:02,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:02,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:04,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:08,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:12,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:12,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:15,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:17,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:21,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:21,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:23,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:25,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:26,830 - distributed.utils_perf - INFO - full garbage collection released 0.93 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:32:27,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:27,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:30,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:33,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:34,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:36,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:40,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:42,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:42,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:43,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:45,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:48,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:48,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:48,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:52,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:54,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:56,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:59,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:03,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:04,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:08,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:08,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:08,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:09,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:12,731 - distributed.utils_perf - INFO - full garbage collection released 320.87 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:33:13,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:13,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:13,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:15,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:16,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:20,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:20,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:22,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:22,335 - distributed.utils_perf - INFO - full garbage collection released 43.60 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:33:23,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:24,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:27,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:28,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:28,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:32,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:33,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:38,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:38,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:39,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:40,912 - distributed.utils_perf - INFO - full garbage collection released 9.91 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:33:41,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:44,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:45,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:49,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:49,141 - distributed.utils_perf - INFO - full garbage collection released 29.34 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:33:50,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:51,603 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:33:53,062 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:33:53,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:54,876 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:33:55,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:56,646 - distributed.utils_perf - INFO - full garbage collection released 9.75 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:33:57,117 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:33:57,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:57,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:33:59,868 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:34:03,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:07,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:10,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:14,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:15,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:16,961 - distributed.utils_perf - INFO - full garbage collection released 278.84 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:34:20,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:21,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:21,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:23,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:23,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:26,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:26,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:28,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:29,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:30,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:34,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:35,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:36,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:39,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:39,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:44,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:47,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:48,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:49,051 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:34:49,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:50,213 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:50,680 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:51,227 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:51,708 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:34:51,897 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:52,721 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:53,726 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:54,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:54,996 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:56,601 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:57,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:58,595 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:34:58,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:00,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:01,020 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:35:01,837 - distributed.utils_perf - INFO - full garbage collection released 9.88 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:35:04,020 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:35:04,958 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:35:06,263 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:35:07,878 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:35:09,882 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 17:35:11,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:12,347 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 17:35:20,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:20,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:21,130 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:21,464 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:21,871 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:22,363 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:22,955 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:23,685 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:24,581 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:25,691 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:35:27,085 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:35:28,811 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:35:30,935 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:35:36,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:36:07] task [xgboost.dask-tcp://10.201.1.162:37101]:tcp://10.201.1.162:37101 got new rank 0
[17:36:07] task [xgboost.dask-tcp://10.201.1.162:38369]:tcp://10.201.1.162:38369 got new rank 1
[17:36:07] task [xgboost.dask-tcp://10.201.1.162:38643]:tcp://10.201.1.162:38643 got new rank 2
[17:36:07] task [xgboost.dask-tcp://10.201.1.162:42897]:tcp://10.201.1.162:42897 got new rank 3
[17:36:07] task [xgboost.dask-tcp://10.201.1.165:35127]:tcp://10.201.1.165:35127 got new rank 4
[17:36:07] task [xgboost.dask-tcp://10.201.1.165:35403]:tcp://10.201.1.165:35403 got new rank 5
[17:36:07] task [xgboost.dask-tcp://10.201.1.165:38513]:tcp://10.201.1.165:38513 got new rank 6
[17:36:07] task [xgboost.dask-tcp://10.201.1.165:40809]:tcp://10.201.1.165:40809 got new rank 7
2024-04-19 17:38:22,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:39:07] task [xgboost.dask-tcp://10.201.1.162:37101]:tcp://10.201.1.162:37101 got new rank 0
[17:39:07] task [xgboost.dask-tcp://10.201.1.162:38369]:tcp://10.201.1.162:38369 got new rank 1
[17:39:07] task [xgboost.dask-tcp://10.201.1.162:38643]:tcp://10.201.1.162:38643 got new rank 2
[17:39:07] task [xgboost.dask-tcp://10.201.1.162:42897]:tcp://10.201.1.162:42897 got new rank 3
[17:39:07] task [xgboost.dask-tcp://10.201.1.165:35127]:tcp://10.201.1.165:35127 got new rank 4
[17:39:07] task [xgboost.dask-tcp://10.201.1.165:35403]:tcp://10.201.1.165:35403 got new rank 5
[17:39:07] task [xgboost.dask-tcp://10.201.1.165:38513]:tcp://10.201.1.165:38513 got new rank 6
[17:39:07] task [xgboost.dask-tcp://10.201.1.165:40809]:tcp://10.201.1.165:40809 got new rank 7
2024-04-19 17:41:48,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:48,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:49,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:49,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:42:27] task [xgboost.dask-tcp://10.201.1.162:37101]:tcp://10.201.1.162:37101 got new rank 0
[17:42:27] task [xgboost.dask-tcp://10.201.1.162:38369]:tcp://10.201.1.162:38369 got new rank 1
[17:42:27] task [xgboost.dask-tcp://10.201.1.162:38643]:tcp://10.201.1.162:38643 got new rank 2
[17:42:27] task [xgboost.dask-tcp://10.201.1.162:42897]:tcp://10.201.1.162:42897 got new rank 3
[17:42:27] task [xgboost.dask-tcp://10.201.1.165:35127]:tcp://10.201.1.165:35127 got new rank 4
[17:42:27] task [xgboost.dask-tcp://10.201.1.165:35403]:tcp://10.201.1.165:35403 got new rank 5
[17:42:27] task [xgboost.dask-tcp://10.201.1.165:38513]:tcp://10.201.1.165:38513 got new rank 6
[17:42:27] task [xgboost.dask-tcp://10.201.1.165:40809]:tcp://10.201.1.165:40809 got new rank 7
2024-04-19 17:45:32,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:46:14] task [xgboost.dask-tcp://10.201.1.162:37101]:tcp://10.201.1.162:37101 got new rank 0
[17:46:14] task [xgboost.dask-tcp://10.201.1.162:38369]:tcp://10.201.1.162:38369 got new rank 1
[17:46:14] task [xgboost.dask-tcp://10.201.1.162:38643]:tcp://10.201.1.162:38643 got new rank 2
[17:46:14] task [xgboost.dask-tcp://10.201.1.162:42897]:tcp://10.201.1.162:42897 got new rank 3
[17:46:14] task [xgboost.dask-tcp://10.201.1.165:35127]:tcp://10.201.1.165:35127 got new rank 4
[17:46:14] task [xgboost.dask-tcp://10.201.1.165:35403]:tcp://10.201.1.165:35403 got new rank 5
[17:46:14] task [xgboost.dask-tcp://10.201.1.165:38513]:tcp://10.201.1.165:38513 got new rank 6
[17:46:14] task [xgboost.dask-tcp://10.201.1.165:40809]:tcp://10.201.1.165:40809 got new rank 7
2024-04-19 17:48:28,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:29,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:42,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:43,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:44,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:44,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:45,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:46,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:46,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:47,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:49:28] task [xgboost.dask-tcp://10.201.1.162:37101]:tcp://10.201.1.162:37101 got new rank 0
[17:49:28] task [xgboost.dask-tcp://10.201.1.162:38369]:tcp://10.201.1.162:38369 got new rank 1
[17:49:28] task [xgboost.dask-tcp://10.201.1.162:38643]:tcp://10.201.1.162:38643 got new rank 2
[17:49:28] task [xgboost.dask-tcp://10.201.1.162:42897]:tcp://10.201.1.162:42897 got new rank 3
[17:49:28] task [xgboost.dask-tcp://10.201.1.165:35127]:tcp://10.201.1.165:35127 got new rank 4
[17:49:28] task [xgboost.dask-tcp://10.201.1.165:35403]:tcp://10.201.1.165:35403 got new rank 5
[17:49:28] task [xgboost.dask-tcp://10.201.1.165:38513]:tcp://10.201.1.165:38513 got new rank 6
[17:49:28] task [xgboost.dask-tcp://10.201.1.165:40809]:tcp://10.201.1.165:40809 got new rank 7
2024-04-19 17:51:33,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:51:33,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:52:05,948 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.165:38513. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.162:37101. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.165:35127. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.162:38369. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.165:40809. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.165:35403. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.162:38643. Reason: scheduler-close
2024-04-19 17:52:05,949 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.162:42897. Reason: scheduler-close
2024-04-19 17:52:05,951 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.165:37089'. Reason: scheduler-close
2024-04-19 17:52:05,951 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.165:44199'. Reason: scheduler-close
2024-04-19 17:52:05,951 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.165:44827'. Reason: scheduler-close
2024-04-19 17:52:05,951 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.165:40119'. Reason: scheduler-close
2024-04-19 17:52:05,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.162:37917'. Reason: scheduler-close
2024-04-19 17:52:05,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.162:33117'. Reason: scheduler-close
2024-04-19 17:52:05,951 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.162:45646 remote=tcp://10.201.3.203:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.162:45646 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:05,951 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.162:45610 remote=tcp://10.201.3.203:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.162:45610 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:05,960 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.162:45785'. Reason: scheduler-close
2024-04-19 17:52:05,960 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.162:45245'. Reason: scheduler-close
2024-04-19 17:52:06,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:52:06,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.162:45676 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:06,744 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:52:06,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.162:45656 remote=tcp://10.201.3.203:8786>: Stream is closed
/10.201.1.162:36826 remote=tcp://10.201.3.203:8786>: Stream is closed
/10.201.1.162:36822 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:06,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:52:06,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.165:46556 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:06,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.165:46562 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:06,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.165:46542 remote=tcp://10.201.3.203:8786>: Stream is closed
/10.201.1.165:46528 remote=tcp://10.201.3.203:8786>: Stream is closed
2024-04-19 17:52:06,794 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,794 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,841 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,841 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,856 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,856 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,857 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,857 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,857 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,858 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,857 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,858 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,875 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,875 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:06,927 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.203:8786; closing.
2024-04-19 17:52:06,927 - distributed.nanny - INFO - Worker closed
2024-04-19 17:52:08,802 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:08,859 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:08,859 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:08,859 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:08,859 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:08,928 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:52:10,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.165:44827'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,029 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.165:44199'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,040 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,046 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.165:40119'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,046 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.162:45245'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,150 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.162:33117'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,156 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.162:37917'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,263 - distributed.dask_worker - INFO - End worker
2024-04-19 17:52:10,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.165:37089'. Reason: nanny-close-gracefully
2024-04-19 17:52:10,529 - distributed.dask_worker - INFO - End worker
