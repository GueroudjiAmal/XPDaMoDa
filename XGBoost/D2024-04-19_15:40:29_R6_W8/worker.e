2024-04-19 15:41:04,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,720 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:04,720 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:04,720 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:04,720 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:04,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.24:46669'
2024-04-19 15:41:04,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.24:34059'
2024-04-19 15:41:04,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.24:41851'
2024-04-19 15:41:04,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.24:34467'
2024-04-19 15:41:04,915 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,916 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,916 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:04,916 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,173 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,173 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,173 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,174 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.26:40549'
2024-04-19 15:41:05,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.26:45475'
2024-04-19 15:41:05,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.26:45831'
2024-04-19 15:41:05,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.4.26:36387'
2024-04-19 15:41:05,460 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:05,460 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:05,460 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:05,460 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:05,461 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,461 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,461 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,461 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,483 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,483 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,483 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,484 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,172 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,172 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,172 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,172 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,215 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,215 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,216 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,216 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,441 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:06,441 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:06,441 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:06,441 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,284 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,284 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,284 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,284 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,539 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0376ed46-ff1d-4650-9076-29e83334f9fd
2024-04-19 15:41:07,539 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-209046f2-2fe5-4032-a5d4-9a37af9c243f
2024-04-19 15:41:07,539 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a6643b52-ebbb-4d41-b0ef-a6088b346efb
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.24:40679
2024-04-19 15:41:07,539 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.24:40679
2024-04-19 15:41:07,539 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a6124bc2-7aa3-4a26-bddf-f6614a4d4fef
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.24:41207
2024-04-19 15:41:07,539 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.24:41207
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.24:35077
2024-04-19 15:41:07,539 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.24:35077
2024-04-19 15:41:07,539 - distributed.worker - INFO -          dashboard at:          10.201.4.24:35295
2024-04-19 15:41:07,539 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:07,539 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.24:45391
2024-04-19 15:41:07,539 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.24:45391
2024-04-19 15:41:07,539 - distributed.worker - INFO -          dashboard at:          10.201.4.24:39021
2024-04-19 15:41:07,539 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:07,539 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9r_qzv44
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -          dashboard at:          10.201.4.24:38143
2024-04-19 15:41:07,539 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:07,539 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o38b3xgb
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -          dashboard at:          10.201.4.24:35439
2024-04-19 15:41:07,539 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:07,539 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jgfxiet8
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:07,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b74_2n_s
2024-04-19 15:41:07,539 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5e7399c8-e71c-45c6-8a5f-a7ec78275c8f
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.26:46013
2024-04-19 15:41:08,383 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.26:46013
2024-04-19 15:41:08,383 - distributed.worker - INFO -          dashboard at:          10.201.4.26:35217
2024-04-19 15:41:08,383 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9fcf31ea-2e74-448d-bda1-f1ba72732c8b
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.26:42861
2024-04-19 15:41:08,383 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.26:42861
2024-04-19 15:41:08,383 - distributed.worker - INFO -          dashboard at:          10.201.4.26:36289
2024-04-19 15:41:08,383 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-05e3b68f-a6da-4bab-a4a3-255ededa34f3
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.26:38879
2024-04-19 15:41:08,383 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.26:38879
2024-04-19 15:41:08,383 - distributed.worker - INFO -          dashboard at:          10.201.4.26:36407
2024-04-19 15:41:08,383 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:08,383 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,383 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ra2flqhg
2024-04-19 15:41:08,384 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-65186203-d101-4fc7-bdb4-7169cb56a9fc
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Start worker at:    tcp://10.201.4.26:37969
2024-04-19 15:41:08,383 - distributed.worker - INFO -          Listening to:    tcp://10.201.4.26:37969
2024-04-19 15:41:08,383 - distributed.worker - INFO -          dashboard at:          10.201.4.26:42041
2024-04-19 15:41:08,383 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:08,383 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,384 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k4w1_3fq
2024-04-19 15:41:08,383 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:08,383 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,383 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ysj4t49w
2024-04-19 15:41:08,384 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.216:8786
2024-04-19 15:41:08,383 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,383 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,384 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z59rwbwh
2024-04-19 15:41:08,384 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,384 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,176 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,176 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,176 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,178 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,178 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,179 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,181 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,182 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,182 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,182 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,183 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,183 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,183 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,184 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,184 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,185 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,185 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,186 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,186 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,186 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:11,186 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:11,187 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.216:8786
2024-04-19 15:41:11,187 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:11,187 - distributed.core - INFO - Starting established connection to tcp://10.201.1.216:8786
2024-04-19 15:41:20,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:20,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:20,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:20,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:24,673 - distributed.utils_perf - INFO - full garbage collection released 315.71 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:27,896 - distributed.utils_perf - INFO - full garbage collection released 146.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,075 - distributed.utils_perf - INFO - full garbage collection released 836.33 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,103 - distributed.utils_perf - INFO - full garbage collection released 358.48 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,206 - distributed.utils_perf - INFO - full garbage collection released 783.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,118 - distributed.utils_perf - INFO - full garbage collection released 165.99 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,582 - distributed.utils_perf - INFO - full garbage collection released 1.42 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,800 - distributed.utils_perf - INFO - full garbage collection released 3.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:32,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:32,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,374 - distributed.utils_perf - INFO - full garbage collection released 309.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:34,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,490 - distributed.utils_perf - INFO - full garbage collection released 590.14 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:36,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:37,736 - distributed.utils_perf - INFO - full garbage collection released 705.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:40,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:42,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:50,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:52,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:57,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:58,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:59,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:59,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:00,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:10,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:12,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:16,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:18,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:19,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:19,343 - distributed.utils_perf - INFO - full garbage collection released 16.01 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:20,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:21,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:22,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:25,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:29,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:31,689 - distributed.utils_perf - INFO - full garbage collection released 115.37 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:32,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,844 - distributed.utils_perf - INFO - full garbage collection released 235.27 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:36,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:38,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:41,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:42,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:42,725 - distributed.utils_perf - INFO - full garbage collection released 759.87 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:42,846 - distributed.utils_perf - INFO - full garbage collection released 484.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:45,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:45,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:52,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:54,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:55,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:57,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:01,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:02,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:02,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:02,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:05,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:05,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:09,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:12,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:13,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:13,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:14,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:25,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:30,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:31,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:31,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:32,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:35,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:37,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:38,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:47,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:47,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:53,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:53,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:55,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:00,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:02,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:09,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:13,436 - distributed.utils_perf - INFO - full garbage collection released 1.36 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:15,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:16,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:19,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:19,895 - distributed.utils_perf - INFO - full garbage collection released 2.60 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:20,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:25,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:27,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:28,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:33,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:33,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:34,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:39,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:44,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:44,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:53,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:54,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:56,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:02,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:02,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:03,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 74.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:07,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:11,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:12,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:13,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:17,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 65.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:27,206 - distributed.utils_perf - INFO - full garbage collection released 16.39 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:27,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:28,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,008 - distributed.utils_perf - INFO - full garbage collection released 1.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:32,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:34,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:44,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:49,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:49,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:53,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:56,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:58,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:58,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:05,372 - distributed.utils_perf - INFO - full garbage collection released 6.03 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:06,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:06,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:10,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:10,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:12,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:14,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:16,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:17,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:17,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:19,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:21,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:24,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:30,409 - distributed.utils_perf - INFO - full garbage collection released 113.75 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:32,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:33,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:38,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:40,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:41,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:43,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:43,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:49,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:51,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:56,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:01,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:02,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:05,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:08,355 - distributed.utils_perf - INFO - full garbage collection released 736.36 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:47:09,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:10,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:13,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:14,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:25,577 - distributed.utils_perf - INFO - full garbage collection released 1.22 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:47:26,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:28,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:30,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:33,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:34,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:34,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:39,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:43,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:44,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:46,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:49,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:54,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:54,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:55,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:56,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:59,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:01,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:10,129 - distributed.utils_perf - INFO - full garbage collection released 1.55 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:11,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:11,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:11,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:14,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:14,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:15,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:21,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:22,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:25,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:27,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:30,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:30,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:31,916 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:48:32,346 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:32,868 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:33,508 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:33,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,291 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:35,258 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:35,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:36,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:36,449 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:37,472 - distributed.utils_perf - INFO - full garbage collection released 18.81 MiB from 152 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:37,964 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:38,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,830 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:40,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:41,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:42,105 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:42,483 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:42,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:44,143 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:45,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:46,183 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:46,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:49,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,050 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:50,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,458 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:50,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,945 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:51,539 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:51,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,264 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:48:53,165 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:48:54,273 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 15:48:54,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:55,659 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 15:48:56,034 - distributed.utils_perf - INFO - full garbage collection released 1.49 GiB from 93 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:56,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:56,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:57,385 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 15:48:59,507 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 15:49:03,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:05,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:06,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:10,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:12,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:14,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:14,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:25,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:31,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:35,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:36,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:37,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:43,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:48,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:51,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:53,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:56,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:59,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:59,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:01,700 - distributed.utils_perf - INFO - full garbage collection released 1.01 GiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:02,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:05,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:10,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:15,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:35,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:58,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:51:02,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:51:12,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:51:17,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:51:26,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:51:58] task [xgboost.dask-tcp://10.201.4.24:35077]:tcp://10.201.4.24:35077 got new rank 0
[15:51:58] task [xgboost.dask-tcp://10.201.4.24:40679]:tcp://10.201.4.24:40679 got new rank 1
[15:51:58] task [xgboost.dask-tcp://10.201.4.24:41207]:tcp://10.201.4.24:41207 got new rank 2
[15:51:58] task [xgboost.dask-tcp://10.201.4.24:45391]:tcp://10.201.4.24:45391 got new rank 3
[15:51:58] task [xgboost.dask-tcp://10.201.4.26:37969]:tcp://10.201.4.26:37969 got new rank 4
[15:51:58] task [xgboost.dask-tcp://10.201.4.26:38879]:tcp://10.201.4.26:38879 got new rank 5
[15:51:58] task [xgboost.dask-tcp://10.201.4.26:42861]:tcp://10.201.4.26:42861 got new rank 6
[15:51:58] task [xgboost.dask-tcp://10.201.4.26:46013]:tcp://10.201.4.26:46013 got new rank 7
2024-04-19 15:54:14,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:14,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:14,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:14,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:14,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:54:53] task [xgboost.dask-tcp://10.201.4.24:35077]:tcp://10.201.4.24:35077 got new rank 0
[15:54:53] task [xgboost.dask-tcp://10.201.4.24:40679]:tcp://10.201.4.24:40679 got new rank 1
[15:54:53] task [xgboost.dask-tcp://10.201.4.24:41207]:tcp://10.201.4.24:41207 got new rank 2
[15:54:53] task [xgboost.dask-tcp://10.201.4.24:45391]:tcp://10.201.4.24:45391 got new rank 3
[15:54:53] task [xgboost.dask-tcp://10.201.4.26:37969]:tcp://10.201.4.26:37969 got new rank 4
[15:54:53] task [xgboost.dask-tcp://10.201.4.26:38879]:tcp://10.201.4.26:38879 got new rank 5
[15:54:53] task [xgboost.dask-tcp://10.201.4.26:42861]:tcp://10.201.4.26:42861 got new rank 6
[15:54:53] task [xgboost.dask-tcp://10.201.4.26:46013]:tcp://10.201.4.26:46013 got new rank 7
2024-04-19 15:57:13,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:14,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:14,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:14,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:57:53] task [xgboost.dask-tcp://10.201.4.24:35077]:tcp://10.201.4.24:35077 got new rank 0
[15:57:53] task [xgboost.dask-tcp://10.201.4.24:40679]:tcp://10.201.4.24:40679 got new rank 1
[15:57:53] task [xgboost.dask-tcp://10.201.4.24:41207]:tcp://10.201.4.24:41207 got new rank 2
[15:57:53] task [xgboost.dask-tcp://10.201.4.24:45391]:tcp://10.201.4.24:45391 got new rank 3
[15:57:53] task [xgboost.dask-tcp://10.201.4.26:37969]:tcp://10.201.4.26:37969 got new rank 4
[15:57:53] task [xgboost.dask-tcp://10.201.4.26:38879]:tcp://10.201.4.26:38879 got new rank 5
[15:57:53] task [xgboost.dask-tcp://10.201.4.26:42861]:tcp://10.201.4.26:42861 got new rank 6
[15:57:53] task [xgboost.dask-tcp://10.201.4.26:46013]:tcp://10.201.4.26:46013 got new rank 7
2024-04-19 16:00:08,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:10,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:25,262 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
[16:00:53] task [xgboost.dask-tcp://10.201.4.24:35077]:tcp://10.201.4.24:35077 got new rank 0
[16:00:53] task [xgboost.dask-tcp://10.201.4.24:40679]:tcp://10.201.4.24:40679 got new rank 1
[16:00:53] task [xgboost.dask-tcp://10.201.4.24:41207]:tcp://10.201.4.24:41207 got new rank 2
[16:00:53] task [xgboost.dask-tcp://10.201.4.24:45391]:tcp://10.201.4.24:45391 got new rank 3
[16:00:53] task [xgboost.dask-tcp://10.201.4.26:37969]:tcp://10.201.4.26:37969 got new rank 4
[16:00:53] task [xgboost.dask-tcp://10.201.4.26:38879]:tcp://10.201.4.26:38879 got new rank 5
[16:00:53] task [xgboost.dask-tcp://10.201.4.26:42861]:tcp://10.201.4.26:42861 got new rank 6
[16:00:53] task [xgboost.dask-tcp://10.201.4.26:46013]:tcp://10.201.4.26:46013 got new rank 7
2024-04-19 16:03:10,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:10,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:10,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:11,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:11,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:24,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:25,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:26,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:26,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:26,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:26,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:26,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:27,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:56,468 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)
[16:04:10] task [xgboost.dask-tcp://10.201.4.24:35077]:tcp://10.201.4.24:35077 got new rank 0
[16:04:10] task [xgboost.dask-tcp://10.201.4.24:40679]:tcp://10.201.4.24:40679 got new rank 1
[16:04:10] task [xgboost.dask-tcp://10.201.4.24:41207]:tcp://10.201.4.24:41207 got new rank 2
[16:04:10] task [xgboost.dask-tcp://10.201.4.24:45391]:tcp://10.201.4.24:45391 got new rank 3
[16:04:10] task [xgboost.dask-tcp://10.201.4.26:37969]:tcp://10.201.4.26:37969 got new rank 4
[16:04:10] task [xgboost.dask-tcp://10.201.4.26:38879]:tcp://10.201.4.26:38879 got new rank 5
[16:04:10] task [xgboost.dask-tcp://10.201.4.26:42861]:tcp://10.201.4.26:42861 got new rank 6
[16:04:10] task [xgboost.dask-tcp://10.201.4.26:46013]:tcp://10.201.4.26:46013 got new rank 7
2024-04-19 16:06:21,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:06:21,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.24:45391. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.24:35077. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.24:40679. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.26:38879. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.24:41207. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.26:46013. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.26:42861. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.worker - INFO - Stopping worker at tcp://10.201.4.26:37969. Reason: scheduler-close
2024-04-19 16:06:50,821 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.24:34059'. Reason: scheduler-close
2024-04-19 16:06:50,821 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.24:41851'. Reason: scheduler-close
2024-04-19 16:06:50,821 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.24:46669'. Reason: scheduler-close
2024-04-19 16:06:50,820 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.4.24:53176 remote=tcp://10.201.1.216:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.4.24:53176 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:50,821 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44750 remote=tcp://10.201.1.216:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44750 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:50,827 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.24:34467'. Reason: scheduler-close
2024-04-19 16:06:50,821 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44780 remote=tcp://10.201.1.216:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44780 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:50,821 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44756 remote=tcp://10.201.1.216:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44756 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:50,821 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44770 remote=tcp://10.201.1.216:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.4.26:44770 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:50,828 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.26:40549'. Reason: scheduler-close
2024-04-19 16:06:50,828 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.26:36387'. Reason: scheduler-close
2024-04-19 16:06:50,828 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.26:45475'. Reason: scheduler-close
2024-04-19 16:06:50,828 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.4.26:45831'. Reason: scheduler-close
2024-04-19 16:06:51,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.24:54752 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:51,610 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.24:54738 remote=tcp://10.201.1.216:8786>: Stream is closed
/10.201.4.26:32856 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:51,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.24:60414 remote=tcp://10.201.1.216:8786>: Stream is closed
/10.201.4.26:44804 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.26:32872 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,612 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.24:60432 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.4.26:44796 remote=tcp://10.201.1.216:8786>: Stream is closed
2024-04-19 16:06:51,614 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,615 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,619 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,619 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,629 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,630 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,651 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,652 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,662 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,662 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,663 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,663 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,673 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,673 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:51,704 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.216:8786; closing.
2024-04-19 16:06:51,704 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:53,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:53,666 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:53,678 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:53,684 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:53,702 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:53,736 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:54,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.24:46669'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,459 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:54,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.24:34059'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,475 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:54,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.24:41851'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,483 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:54,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.26:45831'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,562 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:54,607 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.26:45475'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,608 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:54,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.26:36387'. Reason: nanny-close-gracefully
2024-04-19 16:06:54,614 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:55,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.26:40549'. Reason: nanny-close-gracefully
2024-04-19 16:06:55,027 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:55,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.4.24:34467'. Reason: nanny-close-gracefully
2024-04-19 16:06:55,028 - distributed.dask_worker - INFO - End worker
