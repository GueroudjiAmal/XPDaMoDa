2024-04-19 17:15:22,468 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,468 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,468 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,468 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,640 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:22,640 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:22,640 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:22,640 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:22,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.166:46153'
2024-04-19 17:15:22,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.166:38743'
2024-04-19 17:15:22,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.166:46867'
2024-04-19 17:15:22,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.166:37907'
2024-04-19 17:15:22,914 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,914 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,915 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:22,915 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:23,130 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,130 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,130 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,131 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.168:37881'
2024-04-19 17:15:23,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.168:33035'
2024-04-19 17:15:23,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.168:39017'
2024-04-19 17:15:23,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.168:41237'
2024-04-19 17:15:23,385 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:23,386 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:23,386 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:23,386 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:23,386 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:23,386 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:23,387 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:23,387 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:23,409 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,409 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,410 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:23,410 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:24,154 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:24,154 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:24,154 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:24,154 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:24,155 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:24,155 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:24,155 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:24,155 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:24,199 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:24,200 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:24,200 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:24,200 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:24,288 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:24,288 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:24,288 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:24,288 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:25,162 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:25,162 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:25,162 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:25,162 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
2024-04-19 17:15:25,165 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-tyt6liq0/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-tyt6liq0/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:15:25,164 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-mdexa6le/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-mdexa6le/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.166:46169
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.166:43065
2024-04-19 17:15:25,169 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.166:43065
2024-04-19 17:15:25,169 - distributed.worker - INFO -          dashboard at:         10.201.3.166:37585
2024-04-19 17:15:25,169 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:25,169 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,165 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-jo9x44mg/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-jo9x44mg/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.166:42185
2024-04-19 17:15:25,169 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.166:42185
2024-04-19 17:15:25,169 - distributed.worker - INFO -          dashboard at:         10.201.3.166:44371
2024-04-19 17:15:25,169 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:25,169 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,169 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:25,163 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-t5engr1f/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-t5engr1f/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:15:25,169 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.166:46169
2024-04-19 17:15:25,169 - distributed.worker - INFO -          dashboard at:         10.201.3.166:40225
2024-04-19 17:15:25,169 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:25,169 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,169 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:25,169 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mdexa6le
2024-04-19 17:15:25,170 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,169 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:25,169 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tyt6liq0
2024-04-19 17:15:25,170 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,169 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:25,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jo9x44mg
2024-04-19 17:15:25,170 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,170 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.166:45059
2024-04-19 17:15:25,170 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.166:45059
2024-04-19 17:15:25,170 - distributed.worker - INFO -          dashboard at:         10.201.3.166:35499
2024-04-19 17:15:25,170 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:25,170 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:25,170 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:25,170 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:25,170 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t5engr1f
2024-04-19 17:15:25,170 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,222 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-09863ee3-e654-4001-bb16-bf20bf259be2
2024-04-19 17:15:26,222 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b834f972-6b85-4e5d-910e-78dae85742dd
2024-04-19 17:15:26,222 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.168:35559
2024-04-19 17:15:26,222 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4fac3ade-10dc-4057-91d1-d7d254868e09
2024-04-19 17:15:26,222 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.168:33089
2024-04-19 17:15:26,222 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.168:33089
2024-04-19 17:15:26,222 - distributed.worker - INFO -          dashboard at:         10.201.3.168:35457
2024-04-19 17:15:26,222 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.168:35909
2024-04-19 17:15:26,222 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.168:35909
2024-04-19 17:15:26,222 - distributed.worker - INFO -          dashboard at:         10.201.3.168:46773
2024-04-19 17:15:26,222 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:26,222 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,222 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:26,222 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:26,222 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d682094f-bc65-47ee-a66c-a093a1e8f8ba
2024-04-19 17:15:26,222 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.168:46363
2024-04-19 17:15:26,222 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.168:46363
2024-04-19 17:15:26,222 - distributed.worker - INFO -          dashboard at:         10.201.3.168:42047
2024-04-19 17:15:26,222 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:26,222 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:26,222 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39v27cf6
2024-04-19 17:15:26,222 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.168:35559
2024-04-19 17:15:26,222 - distributed.worker - INFO -          dashboard at:         10.201.3.168:42391
2024-04-19 17:15:26,222 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.229:8786
2024-04-19 17:15:26,222 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,222 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:26,223 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:26,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yvkp6dah
2024-04-19 17:15:26,222 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,222 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:26,223 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:26,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h8tqabyx
2024-04-19 17:15:26,223 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,222 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,223 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:26,223 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:26,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0gp85jfr
2024-04-19 17:15:26,223 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,223 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:26,223 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,679 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,680 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,680 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,681 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,681 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,681 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,682 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,685 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,685 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,686 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,686 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,686 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,686 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,688 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,688 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,688 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,689 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,689 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,689 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,689 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,690 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,690 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,690 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:28,690 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:28,691 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.229:8786
2024-04-19 17:15:28,691 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:28,691 - distributed.core - INFO - Starting established connection to tcp://10.201.1.229:8786
2024-04-19 17:15:39,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:39,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:41,052 - distributed.utils_perf - INFO - full garbage collection released 79.84 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:42,965 - distributed.utils_perf - INFO - full garbage collection released 362.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:43,407 - distributed.utils_perf - INFO - full garbage collection released 137.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:43,452 - distributed.utils_perf - INFO - full garbage collection released 256.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:46,514 - distributed.utils_perf - INFO - full garbage collection released 347.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:46,640 - distributed.utils_perf - INFO - full garbage collection released 306.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:46,770 - distributed.utils_perf - INFO - full garbage collection released 499.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:46,954 - distributed.utils_perf - INFO - full garbage collection released 520.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:47,050 - distributed.utils_perf - INFO - full garbage collection released 811.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:47,068 - distributed.utils_perf - INFO - full garbage collection released 205.29 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:47,906 - distributed.utils_perf - INFO - full garbage collection released 559.98 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:48,621 - distributed.utils_perf - INFO - full garbage collection released 1.45 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:49,638 - distributed.utils_perf - INFO - full garbage collection released 1.17 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:50,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:51,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:51,143 - distributed.utils_perf - INFO - full garbage collection released 3.31 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:51,609 - distributed.utils_perf - INFO - full garbage collection released 492.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:51,818 - distributed.utils_perf - INFO - full garbage collection released 297.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:53,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:53,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:54,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:54,226 - distributed.utils_perf - INFO - full garbage collection released 633.08 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:55,461 - distributed.utils_perf - INFO - full garbage collection released 4.08 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:56,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:58,346 - distributed.utils_perf - INFO - full garbage collection released 1.98 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:00,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:01,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:02,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:02,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:04,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:05,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:06,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:07,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:08,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:08,541 - distributed.utils_perf - INFO - full garbage collection released 140.37 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:09,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:09,639 - distributed.utils_perf - INFO - full garbage collection released 84.21 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:10,584 - distributed.utils_perf - INFO - full garbage collection released 325.28 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:10,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:12,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:13,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:17,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:22,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:24,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:25,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,895 - distributed.utils_perf - INFO - full garbage collection released 90.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:29,871 - distributed.utils_perf - INFO - full garbage collection released 184.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:35,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:36,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,507 - distributed.utils_perf - INFO - full garbage collection released 46.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,947 - distributed.utils_perf - INFO - full garbage collection released 412.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:49,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,013 - distributed.utils_perf - INFO - full garbage collection released 151.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,275 - distributed.utils_perf - INFO - full garbage collection released 96.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:06,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:12,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 50.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:17,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,764 - distributed.utils_perf - INFO - full garbage collection released 13.09 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:19,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:26,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:47,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:57,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:35,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,548 - distributed.utils_perf - INFO - full garbage collection released 2.42 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:38,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,674 - distributed.utils_perf - INFO - full garbage collection released 2.03 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:47,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:52,835 - distributed.utils_perf - INFO - full garbage collection released 182.51 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:53,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 69.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:56,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,605 - distributed.utils_perf - INFO - full garbage collection released 77.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:06,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:20,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:23,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:31,736 - distributed.utils_perf - INFO - full garbage collection released 1.60 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:32,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:37,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:42,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:43,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:58,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 49.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:35,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,617 - distributed.utils_perf - INFO - full garbage collection released 1.50 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:42,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:03,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:21,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:36,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,155 - distributed.utils_perf - INFO - full garbage collection released 259.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:40,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,402 - distributed.utils_perf - INFO - full garbage collection released 1.15 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:48,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:49,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,467 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:22:04,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:06,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:09,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,588 - distributed.utils_perf - INFO - full garbage collection released 12.75 MiB from 208 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:29,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:37,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,127 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:22:43,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:51,296 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:51,666 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:52,119 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:22:52,675 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:22:53,347 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:54,164 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:55,177 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:55,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,994 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:22:56,455 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:22:56,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,032 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:23:02,934 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:23:02,934 - distributed.utils_perf - INFO - full garbage collection released 90.78 MiB from 93 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:03,065 - distributed.utils_perf - INFO - full garbage collection released 1.54 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:03,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,141 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:23:07,852 - distributed.utils_perf - WARNING - full garbage collections took 59% CPU time recently (threshold: 10%)
2024-04-19 17:23:08,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:11,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,152 - distributed.utils_perf - INFO - full garbage collection released 485.24 MiB from 170 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:16,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:22,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,058 - distributed.utils_perf - INFO - full garbage collection released 1.64 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:23,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,764 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:25,496 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:26,394 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:27,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:27,499 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:28,905 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:30,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,602 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:30,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,722 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:35,316 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:35,920 - distributed.utils_perf - INFO - full garbage collection released 143.57 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:37,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:44,707 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,988 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:45,334 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:45,758 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,268 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,893 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:47,651 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:48,595 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:48,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,718 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:23:49,757 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:49,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,239 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:51,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,054 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:53,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,173 - distributed.utils_perf - INFO - full garbage collection released 14.70 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:55,302 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:57,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,752 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 17:24:01,752 - distributed.utils_perf - INFO - full garbage collection released 660.30 MiB from 151 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:02,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:03,903 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 17:24:04,582 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 17:24:04,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,420 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,059 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,223 - distributed.utils_perf - INFO - full garbage collection released 17.80 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:06,452 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 17:24:07,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,745 - distributed.utils_perf - WARNING - full garbage collections took 32% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,019 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,235 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,333 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,884 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:10,677 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:11,328 - distributed.utils_perf - WARNING - full garbage collections took 26% CPU time recently (threshold: 10%)
2024-04-19 17:24:11,668 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:12,707 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:12,891 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:13,783 - distributed.utils_perf - WARNING - full garbage collections took 21% CPU time recently (threshold: 10%)
2024-04-19 17:24:14,442 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:24:15,045 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,349 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,831 - distributed.utils_perf - WARNING - full garbage collections took 22% CPU time recently (threshold: 10%)
2024-04-19 17:24:18,687 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:24:21,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:33,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:49,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:25:18] task [xgboost.dask-tcp://10.201.3.166:42185]:tcp://10.201.3.166:42185 got new rank 0
[17:25:18] task [xgboost.dask-tcp://10.201.3.166:43065]:tcp://10.201.3.166:43065 got new rank 1
[17:25:18] task [xgboost.dask-tcp://10.201.3.166:45059]:tcp://10.201.3.166:45059 got new rank 2
[17:25:18] task [xgboost.dask-tcp://10.201.3.166:46169]:tcp://10.201.3.166:46169 got new rank 3
[17:25:18] task [xgboost.dask-tcp://10.201.3.168:33089]:tcp://10.201.3.168:33089 got new rank 4
[17:25:18] task [xgboost.dask-tcp://10.201.3.168:35559]:tcp://10.201.3.168:35559 got new rank 5
[17:25:18] task [xgboost.dask-tcp://10.201.3.168:35909]:tcp://10.201.3.168:35909 got new rank 6
[17:25:18] task [xgboost.dask-tcp://10.201.3.168:46363]:tcp://10.201.3.168:46363 got new rank 7
[17:28:27] task [xgboost.dask-tcp://10.201.3.166:42185]:tcp://10.201.3.166:42185 got new rank 0
[17:28:27] task [xgboost.dask-tcp://10.201.3.166:43065]:tcp://10.201.3.166:43065 got new rank 1
[17:28:27] task [xgboost.dask-tcp://10.201.3.166:45059]:tcp://10.201.3.166:45059 got new rank 2
[17:28:27] task [xgboost.dask-tcp://10.201.3.166:46169]:tcp://10.201.3.166:46169 got new rank 3
[17:28:27] task [xgboost.dask-tcp://10.201.3.168:33089]:tcp://10.201.3.168:33089 got new rank 4
[17:28:27] task [xgboost.dask-tcp://10.201.3.168:35559]:tcp://10.201.3.168:35559 got new rank 5
[17:28:27] task [xgboost.dask-tcp://10.201.3.168:35909]:tcp://10.201.3.168:35909 got new rank 6
[17:28:27] task [xgboost.dask-tcp://10.201.3.168:46363]:tcp://10.201.3.168:46363 got new rank 7
2024-04-19 17:30:49,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:49,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:30:49,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:31:26] task [xgboost.dask-tcp://10.201.3.166:42185]:tcp://10.201.3.166:42185 got new rank 0
[17:31:26] task [xgboost.dask-tcp://10.201.3.166:43065]:tcp://10.201.3.166:43065 got new rank 1
[17:31:26] task [xgboost.dask-tcp://10.201.3.166:45059]:tcp://10.201.3.166:45059 got new rank 2
[17:31:26] task [xgboost.dask-tcp://10.201.3.166:46169]:tcp://10.201.3.166:46169 got new rank 3
[17:31:26] task [xgboost.dask-tcp://10.201.3.168:33089]:tcp://10.201.3.168:33089 got new rank 4
[17:31:26] task [xgboost.dask-tcp://10.201.3.168:35559]:tcp://10.201.3.168:35559 got new rank 5
[17:31:26] task [xgboost.dask-tcp://10.201.3.168:35909]:tcp://10.201.3.168:35909 got new rank 6
[17:31:26] task [xgboost.dask-tcp://10.201.3.168:46363]:tcp://10.201.3.168:46363 got new rank 7
2024-04-19 17:34:36,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:16] task [xgboost.dask-tcp://10.201.3.166:42185]:tcp://10.201.3.166:42185 got new rank 0
[17:35:16] task [xgboost.dask-tcp://10.201.3.166:43065]:tcp://10.201.3.166:43065 got new rank 1
[17:35:16] task [xgboost.dask-tcp://10.201.3.166:45059]:tcp://10.201.3.166:45059 got new rank 2
[17:35:16] task [xgboost.dask-tcp://10.201.3.166:46169]:tcp://10.201.3.166:46169 got new rank 3
[17:35:16] task [xgboost.dask-tcp://10.201.3.168:33089]:tcp://10.201.3.168:33089 got new rank 4
[17:35:16] task [xgboost.dask-tcp://10.201.3.168:35559]:tcp://10.201.3.168:35559 got new rank 5
[17:35:16] task [xgboost.dask-tcp://10.201.3.168:35909]:tcp://10.201.3.168:35909 got new rank 6
[17:35:16] task [xgboost.dask-tcp://10.201.3.168:46363]:tcp://10.201.3.168:46363 got new rank 7
[17:38:15] task [xgboost.dask-tcp://10.201.3.166:42185]:tcp://10.201.3.166:42185 got new rank 0
[17:38:15] task [xgboost.dask-tcp://10.201.3.166:43065]:tcp://10.201.3.166:43065 got new rank 1
[17:38:15] task [xgboost.dask-tcp://10.201.3.166:45059]:tcp://10.201.3.166:45059 got new rank 2
[17:38:15] task [xgboost.dask-tcp://10.201.3.166:46169]:tcp://10.201.3.166:46169 got new rank 3
[17:38:15] task [xgboost.dask-tcp://10.201.3.168:33089]:tcp://10.201.3.168:33089 got new rank 4
[17:38:15] task [xgboost.dask-tcp://10.201.3.168:35559]:tcp://10.201.3.168:35559 got new rank 5
[17:38:15] task [xgboost.dask-tcp://10.201.3.168:35909]:tcp://10.201.3.168:35909 got new rank 6
[17:38:15] task [xgboost.dask-tcp://10.201.3.168:46363]:tcp://10.201.3.168:46363 got new rank 7
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.166:42185. Reason: scheduler-close
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.168:33089. Reason: scheduler-close
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.166:45059. Reason: scheduler-close
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.168:35559. Reason: scheduler-close
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.166:46169. Reason: scheduler-close
2024-04-19 17:40:50,273 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.166:43065. Reason: scheduler-close
2024-04-19 17:40:50,274 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.168:35909. Reason: scheduler-close
2024-04-19 17:40:50,274 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.168:46363. Reason: scheduler-close
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52560 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52560 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52568 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52568 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52574 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52574 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51952 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51952 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52584 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.168:52584 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.168:41237'. Reason: scheduler-close
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51970 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51970 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.168:33035'. Reason: scheduler-close
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51986 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51986 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.168:37881'. Reason: scheduler-close
2024-04-19 17:40:50,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.168:39017'. Reason: scheduler-close
2024-04-19 17:40:50,275 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51954 remote=tcp://10.201.1.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.166:51954 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,282 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.166:46153'. Reason: scheduler-close
2024-04-19 17:40:50,282 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.166:46867'. Reason: scheduler-close
2024-04-19 17:40:50,282 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.166:38743'. Reason: scheduler-close
2024-04-19 17:40:50,282 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.166:37907'. Reason: scheduler-close
2024-04-19 17:40:50,451 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:50,451 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:50,454 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:50,455 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:50,478 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:50,478 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:50,503 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:50,503 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:50,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:50,950 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:50,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.168:52626 remote=tcp://10.201.1.229:8786>: Stream is closed
/10.201.3.168:52592 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:50,950 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.168:36562 remote=tcp://10.201.1.229:8786>: Stream is closed
/10.201.3.168:52620 remote=tcp://10.201.1.229:8786>: Stream is closed
2024-04-19 17:40:51,050 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:51,050 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:51,050 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:51,050 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:51,052 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:51,052 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:51,089 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.229:8786; closing.
2024-04-19 17:40:51,089 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:52,456 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:52,479 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:52,504 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:53,053 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:53,053 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:53,053 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:53,091 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:53,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.166:37907'. Reason: nanny-close-gracefully
2024-04-19 17:40:53,514 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:53,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.166:46867'. Reason: nanny-close-gracefully
2024-04-19 17:40:53,522 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:53,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.166:38743'. Reason: nanny-close-gracefully
2024-04-19 17:40:53,528 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:53,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.166:46153'. Reason: nanny-close-gracefully
2024-04-19 17:40:53,973 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:54,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.168:37881'. Reason: nanny-close-gracefully
2024-04-19 17:40:54,294 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:54,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.168:33035'. Reason: nanny-close-gracefully
2024-04-19 17:40:54,302 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:54,307 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.168:41237'. Reason: nanny-close-gracefully
2024-04-19 17:40:54,307 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:54,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.168:39017'. Reason: nanny-close-gracefully
2024-04-19 17:40:54,780 - distributed.dask_worker - INFO - End worker
