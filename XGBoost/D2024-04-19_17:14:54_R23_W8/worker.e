2024-04-19 17:15:59,639 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,639 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,640 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,640 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,815 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,815 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,815 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,815 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,906 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,906 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,906 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,906 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.2:35363'
2024-04-19 17:15:59,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.2:35721'
2024-04-19 17:15:59,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.2:35365'
2024-04-19 17:15:59,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.2:37753'
2024-04-19 17:16:00,082 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,082 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,082 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,082 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.1:34983'
2024-04-19 17:16:00,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.1:39751'
2024-04-19 17:16:00,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.1:44657'
2024-04-19 17:16:00,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.1:38115'
2024-04-19 17:16:01,003 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,004 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,004 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,004 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,004 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,004 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,005 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,005 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,048 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,050 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,050 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,051 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,149 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,150 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,150 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,150 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:01,150 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,150 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,150 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,151 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,196 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,196 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,196 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,197 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:02,046 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,046 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,046 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,046 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,200 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,200 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,200 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,200 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:03,101 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-4ad6c500-9405-411d-b213-697bd3be6505
2024-04-19 17:16:03,101 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-730930ac-83ae-4bf7-b11f-d29dd2c12a7d
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.2:36711
2024-04-19 17:16:03,102 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.2:36711
2024-04-19 17:16:03,102 - distributed.worker - INFO -          dashboard at:           10.201.2.2:42035
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.2:38537
2024-04-19 17:16:03,102 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.2:38537
2024-04-19 17:16:03,102 - distributed.worker - INFO -          dashboard at:           10.201.2.2:41197
2024-04-19 17:16:03,102 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,102 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-aa8d469e-68d4-4600-8f19-5e02c2add033
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.2:44173
2024-04-19 17:16:03,102 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.2:44173
2024-04-19 17:16:03,102 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e1b3856a-3f7c-4323-8509-5827d6b8b28f
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.2:33643
2024-04-19 17:16:03,102 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.2:33643
2024-04-19 17:16:03,102 - distributed.worker - INFO -          dashboard at:           10.201.2.2:36623
2024-04-19 17:16:03,102 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,102 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,102 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_z7iolp
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,102 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3hl6lhn5
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO -          dashboard at:           10.201.2.2:42021
2024-04-19 17:16:03,102 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,102 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1zksm99m
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,102 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dmfdz2_i
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,102 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,278 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-83155368-40f5-4a14-a2d1-af6a1281069b
2024-04-19 17:16:03,278 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c1a5981e-a47e-4d15-8d75-af847a15f46d
2024-04-19 17:16:03,278 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b4a8b9c4-9cde-4fa4-b27f-9e8286c93000
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.1:33969
2024-04-19 17:16:03,279 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.1:33969
2024-04-19 17:16:03,278 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.1:37937
2024-04-19 17:16:03,278 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.1:37937
2024-04-19 17:16:03,279 - distributed.worker - INFO -          dashboard at:           10.201.2.1:43993
2024-04-19 17:16:03,279 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,279 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a66015ef-8306-448a-a9cb-1b4a017f7cf9
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.1:34575
2024-04-19 17:16:03,279 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.1:34575
2024-04-19 17:16:03,279 - distributed.worker - INFO -          dashboard at:           10.201.2.1:45165
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Start worker at:     tcp://10.201.2.1:37497
2024-04-19 17:16:03,279 - distributed.worker - INFO -          Listening to:     tcp://10.201.2.1:37497
2024-04-19 17:16:03,279 - distributed.worker - INFO -          dashboard at:           10.201.2.1:37793
2024-04-19 17:16:03,279 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,279 - distributed.worker - INFO -          dashboard at:           10.201.2.1:39797
2024-04-19 17:16:03,279 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,279 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-creleg0q
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gyzoy04a
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.34:8786
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,279 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sy_11f28
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,279 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9zls1j82
2024-04-19 17:16:03,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,183 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,183 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,183 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,185 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,186 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,186 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,189 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,189 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,189 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,190 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,190 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,191 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,191 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,191 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,192 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,193 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,193 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,193 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,194 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,194 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:07,194 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:07,194 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.34:8786
2024-04-19 17:16:07,195 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:07,195 - distributed.core - INFO - Starting established connection to tcp://10.201.2.34:8786
2024-04-19 17:16:16,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:20,238 - distributed.utils_perf - INFO - full garbage collection released 1.18 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:20,830 - distributed.utils_perf - INFO - full garbage collection released 408.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,017 - distributed.utils_perf - INFO - full garbage collection released 0.98 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,742 - distributed.utils_perf - INFO - full garbage collection released 333.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,425 - distributed.utils_perf - INFO - full garbage collection released 90.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:28,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,186 - distributed.utils_perf - INFO - full garbage collection released 201.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:30,342 - distributed.utils_perf - INFO - full garbage collection released 1.14 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:33,256 - distributed.utils_perf - INFO - full garbage collection released 3.68 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:33,670 - distributed.utils_perf - INFO - full garbage collection released 5.31 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:33,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,106 - distributed.utils_perf - INFO - full garbage collection released 4.39 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:34,859 - distributed.utils_perf - INFO - full garbage collection released 530.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:37,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:42,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,361 - distributed.utils_perf - INFO - full garbage collection released 656.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,855 - distributed.utils_perf - INFO - full garbage collection released 295.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:51,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:51,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,046 - distributed.utils_perf - INFO - full garbage collection released 1.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:57,577 - distributed.utils_perf - INFO - full garbage collection released 582.31 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:59,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,573 - distributed.utils_perf - INFO - full garbage collection released 224.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:03,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,468 - distributed.utils_perf - INFO - full garbage collection released 461.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:09,701 - distributed.utils_perf - INFO - full garbage collection released 121.92 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:10,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,337 - distributed.utils_perf - INFO - full garbage collection released 198.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:15,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,816 - distributed.utils_perf - INFO - full garbage collection released 277.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:17,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:35,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:38,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,928 - distributed.utils_perf - INFO - full garbage collection released 217.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:44,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,163 - distributed.utils_perf - INFO - full garbage collection released 303.04 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:52,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,979 - distributed.utils_perf - INFO - full garbage collection released 155.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:56,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:59,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:09,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:19,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:20,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,546 - distributed.utils_perf - INFO - full garbage collection released 294.37 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:26,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:27,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,979 - distributed.utils_perf - INFO - full garbage collection released 1.06 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:42,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:47,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:47,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:49,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:01,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:04,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:25,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:27,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,289 - distributed.utils_perf - INFO - full garbage collection released 1.35 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:36,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:38,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:42,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,622 - distributed.utils_perf - INFO - full garbage collection released 677.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:47,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:52,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:00,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:47,411 - distributed.utils_perf - INFO - full garbage collection released 110.07 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:47,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:48,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:06,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:21,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,319 - distributed.utils_perf - INFO - full garbage collection released 2.12 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:24,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:31,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:47,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:06,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:07,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:30,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,542 - distributed.utils_perf - INFO - full garbage collection released 232.15 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:32,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:34,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:43,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:48,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:50,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:04,043 - distributed.utils_perf - INFO - full garbage collection released 363.21 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:04,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,920 - distributed.utils_perf - INFO - full garbage collection released 15.60 MiB from 285 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:22,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,362 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:27,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:27,617 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:28,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:28,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,212 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:23:30,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,163 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:31,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,562 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:23:36,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:42,872 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:43,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,604 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,503 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,608 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:45,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,997 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:48,730 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:49,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,866 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:51,381 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 17:23:51,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,518 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:23:53,703 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 17:23:55,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,580 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:56,792 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:57,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:06,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:13,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:17,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:17,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:19,479 - distributed.utils_perf - INFO - full garbage collection released 1.55 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:21,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:31,321 - distributed.utils_perf - INFO - full garbage collection released 10.90 MiB from 133 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:33,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:43,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:43,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:47,765 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:48,463 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:49,316 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:50,366 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:51,669 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:52,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:52,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,321 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:54,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,363 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:56,407 - distributed.utils_perf - INFO - full garbage collection released 359.30 MiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:56,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:57,845 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:58,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,923 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:01,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:07,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:09,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:12,867 - distributed.utils_perf - INFO - full garbage collection released 25.43 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:22,083 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:25:23,329 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:25:24,898 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
[17:25:59] task [xgboost.dask-tcp://10.201.2.1:33969]:tcp://10.201.2.1:33969 got new rank 0
[17:25:59] task [xgboost.dask-tcp://10.201.2.1:34575]:tcp://10.201.2.1:34575 got new rank 1
[17:25:59] task [xgboost.dask-tcp://10.201.2.1:37497]:tcp://10.201.2.1:37497 got new rank 2
[17:25:59] task [xgboost.dask-tcp://10.201.2.1:37937]:tcp://10.201.2.1:37937 got new rank 3
[17:25:59] task [xgboost.dask-tcp://10.201.2.2:33643]:tcp://10.201.2.2:33643 got new rank 4
[17:25:59] task [xgboost.dask-tcp://10.201.2.2:36711]:tcp://10.201.2.2:36711 got new rank 5
[17:25:59] task [xgboost.dask-tcp://10.201.2.2:38537]:tcp://10.201.2.2:38537 got new rank 6
[17:25:59] task [xgboost.dask-tcp://10.201.2.2:44173]:tcp://10.201.2.2:44173 got new rank 7
[17:28:58] task [xgboost.dask-tcp://10.201.2.1:33969]:tcp://10.201.2.1:33969 got new rank 0
[17:28:58] task [xgboost.dask-tcp://10.201.2.1:34575]:tcp://10.201.2.1:34575 got new rank 1
[17:28:58] task [xgboost.dask-tcp://10.201.2.1:37497]:tcp://10.201.2.1:37497 got new rank 2
[17:28:58] task [xgboost.dask-tcp://10.201.2.1:37937]:tcp://10.201.2.1:37937 got new rank 3
[17:28:58] task [xgboost.dask-tcp://10.201.2.2:33643]:tcp://10.201.2.2:33643 got new rank 4
[17:28:58] task [xgboost.dask-tcp://10.201.2.2:36711]:tcp://10.201.2.2:36711 got new rank 5
[17:28:58] task [xgboost.dask-tcp://10.201.2.2:38537]:tcp://10.201.2.2:38537 got new rank 6
[17:28:58] task [xgboost.dask-tcp://10.201.2.2:44173]:tcp://10.201.2.2:44173 got new rank 7
2024-04-19 17:31:47,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:48,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:48,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:48,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:32:30] task [xgboost.dask-tcp://10.201.2.1:33969]:tcp://10.201.2.1:33969 got new rank 0
[17:32:30] task [xgboost.dask-tcp://10.201.2.1:34575]:tcp://10.201.2.1:34575 got new rank 1
[17:32:30] task [xgboost.dask-tcp://10.201.2.1:37497]:tcp://10.201.2.1:37497 got new rank 2
[17:32:30] task [xgboost.dask-tcp://10.201.2.1:37937]:tcp://10.201.2.1:37937 got new rank 3
[17:32:30] task [xgboost.dask-tcp://10.201.2.2:33643]:tcp://10.201.2.2:33643 got new rank 4
[17:32:30] task [xgboost.dask-tcp://10.201.2.2:36711]:tcp://10.201.2.2:36711 got new rank 5
[17:32:30] task [xgboost.dask-tcp://10.201.2.2:38537]:tcp://10.201.2.2:38537 got new rank 6
[17:32:30] task [xgboost.dask-tcp://10.201.2.2:44173]:tcp://10.201.2.2:44173 got new rank 7
2024-04-19 17:34:53,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:54,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:54,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:32] task [xgboost.dask-tcp://10.201.2.1:33969]:tcp://10.201.2.1:33969 got new rank 0
[17:35:32] task [xgboost.dask-tcp://10.201.2.1:34575]:tcp://10.201.2.1:34575 got new rank 1
[17:35:32] task [xgboost.dask-tcp://10.201.2.1:37497]:tcp://10.201.2.1:37497 got new rank 2
[17:35:32] task [xgboost.dask-tcp://10.201.2.1:37937]:tcp://10.201.2.1:37937 got new rank 3
[17:35:32] task [xgboost.dask-tcp://10.201.2.2:33643]:tcp://10.201.2.2:33643 got new rank 4
[17:35:32] task [xgboost.dask-tcp://10.201.2.2:36711]:tcp://10.201.2.2:36711 got new rank 5
[17:35:32] task [xgboost.dask-tcp://10.201.2.2:38537]:tcp://10.201.2.2:38537 got new rank 6
[17:35:32] task [xgboost.dask-tcp://10.201.2.2:44173]:tcp://10.201.2.2:44173 got new rank 7
2024-04-19 17:37:39,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:39,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:40,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:40,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:40,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:54,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:55,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:56,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:57,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:57,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:07,142 - distributed.utils_perf - INFO - full garbage collection released 97.89 MiB from 230 reference cycles (threshold: 9.54 MiB)
[17:38:39] task [xgboost.dask-tcp://10.201.2.1:33969]:tcp://10.201.2.1:33969 got new rank 0
[17:38:39] task [xgboost.dask-tcp://10.201.2.1:34575]:tcp://10.201.2.1:34575 got new rank 1
[17:38:39] task [xgboost.dask-tcp://10.201.2.1:37497]:tcp://10.201.2.1:37497 got new rank 2
[17:38:39] task [xgboost.dask-tcp://10.201.2.1:37937]:tcp://10.201.2.1:37937 got new rank 3
[17:38:39] task [xgboost.dask-tcp://10.201.2.2:33643]:tcp://10.201.2.2:33643 got new rank 4
[17:38:39] task [xgboost.dask-tcp://10.201.2.2:36711]:tcp://10.201.2.2:36711 got new rank 5
[17:38:39] task [xgboost.dask-tcp://10.201.2.2:38537]:tcp://10.201.2.2:38537 got new rank 6
[17:38:39] task [xgboost.dask-tcp://10.201.2.2:44173]:tcp://10.201.2.2:44173 got new rank 7
2024-04-19 17:40:38,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:40,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.2:38537. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.1:37497. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.2:44173. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.2:33643. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.1:33969. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.1:34575. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.2:36711. Reason: scheduler-close
2024-04-19 17:41:11,750 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.1:37937. Reason: scheduler-close
2024-04-19 17:41:11,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.1:44657'. Reason: scheduler-close
2024-04-19 17:41:11,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.1:34983'. Reason: scheduler-close
2024-04-19 17:41:11,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.1:38115'. Reason: scheduler-close
2024-04-19 17:41:11,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.1:39751'. Reason: scheduler-close
2024-04-19 17:41:11,751 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41536 remote=tcp://10.201.2.34:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41536 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:11,757 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.2:35363'. Reason: scheduler-close
2024-04-19 17:41:11,751 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41552 remote=tcp://10.201.2.34:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41552 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:11,751 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41550 remote=tcp://10.201.2.34:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41550 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:11,751 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41538 remote=tcp://10.201.2.34:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.2:41538 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:11,759 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.2:35365'. Reason: scheduler-close
2024-04-19 17:41:11,759 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.2:37753'. Reason: scheduler-close
2024-04-19 17:41:11,760 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.2:35721'. Reason: scheduler-close
2024-04-19 17:41:12,533 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.1:53318 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,533 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.1:53346 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,533 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.1:51530 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,534 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.1:51526 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.2:46564 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:12,537 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.2:46550 remote=tcp://10.201.2.34:8786>: Stream is closed
/10.201.2.2:59664 remote=tcp://10.201.2.34:8786>: Stream is closed
/10.201.2.2:46578 remote=tcp://10.201.2.34:8786>: Stream is closed
2024-04-19 17:41:12,594 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,594 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,601 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,602 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,613 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,614 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,623 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,623 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,626 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,626 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,633 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,634 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,634 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,634 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:12,654 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.34:8786; closing.
2024-04-19 17:41:12,654 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:14,597 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,603 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,616 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,624 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,628 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,635 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,635 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:14,655 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:15,630 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.1:39751'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,631 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.1:34983'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,672 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.1:38115'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,678 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.2:35365'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,890 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,897 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.2:37753'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,898 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:15,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.2:35363'. Reason: nanny-close-gracefully
2024-04-19 17:41:15,937 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:16,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.1:44657'. Reason: nanny-close-gracefully
2024-04-19 17:41:16,103 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:16,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.2:35721'. Reason: nanny-close-gracefully
2024-04-19 17:41:16,351 - distributed.dask_worker - INFO - End worker
