2024-04-19 20:53:24,218 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,219 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,219 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,219 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,288 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,288 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,288 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,289 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:24,544 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,544 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,544 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,545 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,620 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,620 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,620 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,620 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:24,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:43101'
2024-04-19 20:53:24,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:33033'
2024-04-19 20:53:24,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:39303'
2024-04-19 20:53:24,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:45839'
2024-04-19 20:53:24,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:39757'
2024-04-19 20:53:24,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:42711'
2024-04-19 20:53:24,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:35401'
2024-04-19 20:53:24,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:42029'
2024-04-19 20:53:25,668 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,668 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,668 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,669 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,675 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,675 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,675 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,675 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:25,676 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,676 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,676 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,676 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:25,713 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,713 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,713 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,713 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,721 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,721 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,721 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:25,721 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:26,678 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:27,888 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a97cc20b-da82-4f5a-a176-bb7ed32f5d48
2024-04-19 20:53:27,888 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:44071
2024-04-19 20:53:27,888 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:44071
2024-04-19 20:53:27,888 - distributed.worker - INFO -          dashboard at:          10.201.2.91:44459
2024-04-19 20:53:27,888 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,888 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,888 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,888 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,888 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vf8bvon6
2024-04-19 20:53:27,888 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,890 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-da70e393-c51e-4dc4-9ad0-f732b12ab176
2024-04-19 20:53:27,890 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:36189
2024-04-19 20:53:27,890 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:36189
2024-04-19 20:53:27,890 - distributed.worker - INFO -          dashboard at:         10.201.1.105:38121
2024-04-19 20:53:27,890 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,890 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,890 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,890 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,890 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p0xe_ihx
2024-04-19 20:53:27,891 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,891 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-98eac7b4-c2b0-467e-a9e2-06a41fb4c5bd
2024-04-19 20:53:27,892 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:40423
2024-04-19 20:53:27,892 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:40423
2024-04-19 20:53:27,892 - distributed.worker - INFO -          dashboard at:          10.201.2.91:44491
2024-04-19 20:53:27,892 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,892 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,892 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,892 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4i1uf8ez
2024-04-19 20:53:27,892 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,894 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-fb8c44c9-ebb6-4579-9f17-54ec2ceb6775
2024-04-19 20:53:27,894 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:44701
2024-04-19 20:53:27,894 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:44701
2024-04-19 20:53:27,894 - distributed.worker - INFO -          dashboard at:         10.201.1.105:34309
2024-04-19 20:53:27,894 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,894 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,894 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,894 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,894 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lm67smsc
2024-04-19 20:53:27,894 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,895 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2ba6b1c9-7d9a-4450-9235-8bbc04846190
2024-04-19 20:53:27,895 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:36323
2024-04-19 20:53:27,896 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:36323
2024-04-19 20:53:27,896 - distributed.worker - INFO -          dashboard at:          10.201.2.91:41531
2024-04-19 20:53:27,896 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,896 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,896 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,896 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-021kakie
2024-04-19 20:53:27,896 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,897 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3caf7eff-b299-4b5a-b848-2acc898f9c27
2024-04-19 20:53:27,897 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:40369
2024-04-19 20:53:27,897 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:40369
2024-04-19 20:53:27,897 - distributed.worker - INFO -          dashboard at:         10.201.1.105:34769
2024-04-19 20:53:27,898 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,898 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,898 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iysgnple
2024-04-19 20:53:27,898 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,899 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c5554394-a5ca-414b-b10f-bb9484cca981
2024-04-19 20:53:27,899 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:33859
2024-04-19 20:53:27,899 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:33859
2024-04-19 20:53:27,899 - distributed.worker - INFO -          dashboard at:          10.201.2.91:38501
2024-04-19 20:53:27,899 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,899 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,899 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,899 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,899 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eq8ekyna
2024-04-19 20:53:27,899 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,901 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-270eb14c-1894-4b0e-8569-bcd1741b5e8c
2024-04-19 20:53:27,901 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:43641
2024-04-19 20:53:27,901 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:43641
2024-04-19 20:53:27,901 - distributed.worker - INFO -          dashboard at:         10.201.1.105:46297
2024-04-19 20:53:27,901 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 20:53:27,901 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:27,901 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:27,901 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:27,901 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aialkbsj
2024-04-19 20:53:27,901 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,693 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,693 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,694 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,695 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,695 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,695 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,698 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,698 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,698 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,699 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,699 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,700 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,700 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,701 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,701 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,702 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,702 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,703 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,703 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,703 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:31,703 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:31,703 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 20:53:31,704 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,704 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 20:53:41,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:41,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:45,050 - distributed.utils_perf - INFO - full garbage collection released 158.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:45,156 - distributed.utils_perf - INFO - full garbage collection released 401.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:47,791 - distributed.utils_perf - INFO - full garbage collection released 502.18 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:47,814 - distributed.utils_perf - INFO - full garbage collection released 167.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:48,343 - distributed.utils_perf - INFO - full garbage collection released 347.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:50,798 - distributed.utils_perf - INFO - full garbage collection released 1.23 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:52,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:52,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:53,308 - distributed.utils_perf - INFO - full garbage collection released 695.56 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:53,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:54,517 - distributed.utils_perf - INFO - full garbage collection released 907.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:54,591 - distributed.utils_perf - INFO - full garbage collection released 1.08 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:58,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,348 - distributed.utils_perf - INFO - full garbage collection released 263.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:59,561 - distributed.utils_perf - INFO - full garbage collection released 2.15 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:59,645 - distributed.utils_perf - INFO - full garbage collection released 439.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:00,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:00,243 - distributed.utils_perf - INFO - full garbage collection released 143.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:01,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:03,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:04,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:04,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:04,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:05,648 - distributed.utils_perf - INFO - full garbage collection released 268.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:06,686 - distributed.utils_perf - INFO - full garbage collection released 2.10 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:07,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:07,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:08,927 - distributed.utils_perf - INFO - full garbage collection released 213.09 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:08,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:09,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:10,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:13,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:13,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:16,343 - distributed.utils_perf - INFO - full garbage collection released 0.99 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:18,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:19,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:19,096 - distributed.utils_perf - INFO - full garbage collection released 102.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:19,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:19,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:22,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:22,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:23,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:24,427 - distributed.utils_perf - INFO - full garbage collection released 486.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:25,967 - distributed.utils_perf - INFO - full garbage collection released 13.42 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:28,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:29,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:30,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:30,427 - distributed.utils_perf - INFO - full garbage collection released 754.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:30,832 - distributed.utils_perf - INFO - full garbage collection released 23.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:31,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:33,501 - distributed.utils_perf - INFO - full garbage collection released 5.47 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:33,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:34,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:35,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:39,050 - distributed.utils_perf - INFO - full garbage collection released 563.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:39,482 - distributed.utils_perf - INFO - full garbage collection released 120.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:39,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:40,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:42,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:44,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:46,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:46,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:48,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:48,916 - distributed.utils_perf - INFO - full garbage collection released 489.23 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:50,803 - distributed.utils_perf - INFO - full garbage collection released 24.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:51,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:51,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:54,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:00,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:01,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:05,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:08,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:10,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:11,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:12,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:14,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:15,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:16,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:17,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:19,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:20,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:21,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:23,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:27,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:27,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:30,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:31,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:31,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:31,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:35,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:36,058 - distributed.utils_perf - INFO - full garbage collection released 101.00 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:36,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:39,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:40,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:41,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:43,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:44,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:44,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:45,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:50,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:51,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:51,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:52,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:53,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:55,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:56,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:57,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:58,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:59,130 - distributed.utils_perf - INFO - full garbage collection released 250.40 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:01,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:09,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:09,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:12,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:13,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:16,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:18,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:18,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:24,485 - distributed.utils_perf - INFO - full garbage collection released 298.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:56:24,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:25,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:26,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:26,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:27,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:31,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:33,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:36,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:38,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:39,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:39,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:40,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:41,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:42,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:45,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:47,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:47,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:50,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:50,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:52,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:56,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:58,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:58,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:58,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:59,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:06,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:06,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:09,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:10,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:11,579 - distributed.utils_perf - INFO - full garbage collection released 1.39 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:11,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:13,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:14,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:14,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:16,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:16,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:20,210 - distributed.utils_perf - INFO - full garbage collection released 3.79 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:20,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:21,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:26,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:26,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:27,101 - distributed.utils_perf - INFO - full garbage collection released 2.09 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:28,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:29,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:29,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:33,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:36,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:36,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:37,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:38,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:39,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:40,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:43,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:44,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:45,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:45,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:49,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:53,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:53,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:53,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:54,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:00,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:03,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:05,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:06,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:09,851 - distributed.utils_perf - INFO - full garbage collection released 218.39 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:09,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:11,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:13,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:17,137 - distributed.utils_perf - INFO - full garbage collection released 5.18 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:18,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:19,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:20,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:21,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:23,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:24,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:25,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:27,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:29,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:30,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:31,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:40,976 - distributed.utils_perf - INFO - full garbage collection released 685.39 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:41,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:42,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:45,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:45,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:46,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:46,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:47,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:49,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:56,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:57,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:57,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:58,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:59,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:02,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:03,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:06,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:08,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:10,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:11,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:14,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:16,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:16,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:20,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:21,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:22,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:24,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:26,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:26,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:27,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:27,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:27,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:31,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:32,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:33,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:35,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:36,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:37,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:42,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:42,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:43,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:49,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:54,284 - distributed.utils_perf - INFO - full garbage collection released 61.28 MiB from 132 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:59:55,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:55,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:55,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:57,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:59,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:00,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:00,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:02,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:03,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:04,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:07,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:07,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:08,604 - distributed.utils_perf - INFO - full garbage collection released 243.93 MiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:08,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:09,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:13,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:16,856 - distributed.utils_perf - INFO - full garbage collection released 1.20 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:18,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:20,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:23,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:24,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:25,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:26,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:27,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:30,794 - distributed.utils_perf - INFO - full garbage collection released 4.91 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:00:31,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:31,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:31,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:32,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:35,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:35,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:38,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:39,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:40,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:42,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:44,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:44,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:44,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:45,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:45,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:47,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:47,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:48,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:53,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:55,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:55,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:58,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:58,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:59,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:01,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:03,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:03,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:08,665 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 21:01:08,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:08,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:11,024 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:01:12,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:13,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:13,931 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:01:14,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:15,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:18,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:21,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:21,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:22,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:26,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:27,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:32,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:33,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:35,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:37,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:37,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:41,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:43,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:44,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:44,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:48,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:52,721 - distributed.utils_perf - INFO - full garbage collection released 19.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:53,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:54,154 - distributed.utils_perf - INFO - full garbage collection released 1.46 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:54,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:56,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:57,599 - distributed.utils_perf - INFO - full garbage collection released 18.87 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:58,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:05,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:05,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:06,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:07,665 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 21:02:08,198 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:08,847 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:09,130 - distributed.utils_perf - INFO - full garbage collection released 1.60 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:02:09,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:09,652 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:10,639 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:11,845 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:13,369 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:15,246 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:15,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:17,555 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:18,605 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 21:02:20,400 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 21:02:21,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:21,736 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 21:02:23,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:25,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:26,025 - distributed.utils_perf - INFO - full garbage collection released 0.99 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:02:29,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:30,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:31,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:52,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:09,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:20,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:25,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:31,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:03:34,410 - distributed.utils_perf - INFO - full garbage collection released 704.82 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:03:50,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:04:24] task [xgboost.dask-tcp://10.201.1.105:36189]:tcp://10.201.1.105:36189 got new rank 0
[21:04:24] task [xgboost.dask-tcp://10.201.1.105:40369]:tcp://10.201.1.105:40369 got new rank 1
[21:04:24] task [xgboost.dask-tcp://10.201.1.105:43641]:tcp://10.201.1.105:43641 got new rank 2
[21:04:24] task [xgboost.dask-tcp://10.201.1.105:44701]:tcp://10.201.1.105:44701 got new rank 3
[21:04:24] task [xgboost.dask-tcp://10.201.2.91:33859]:tcp://10.201.2.91:33859 got new rank 4
[21:04:25] task [xgboost.dask-tcp://10.201.2.91:36323]:tcp://10.201.2.91:36323 got new rank 5
[21:04:25] task [xgboost.dask-tcp://10.201.2.91:40423]:tcp://10.201.2.91:40423 got new rank 6
[21:04:25] task [xgboost.dask-tcp://10.201.2.91:44071]:tcp://10.201.2.91:44071 got new rank 7
2024-04-19 21:06:44,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:06:44,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:06:45,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:06:45,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:06:45,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:06:45,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:07:26] task [xgboost.dask-tcp://10.201.1.105:36189]:tcp://10.201.1.105:36189 got new rank 0
[21:07:26] task [xgboost.dask-tcp://10.201.1.105:40369]:tcp://10.201.1.105:40369 got new rank 1
[21:07:26] task [xgboost.dask-tcp://10.201.1.105:43641]:tcp://10.201.1.105:43641 got new rank 2
[21:07:26] task [xgboost.dask-tcp://10.201.1.105:44701]:tcp://10.201.1.105:44701 got new rank 3
[21:07:26] task [xgboost.dask-tcp://10.201.2.91:33859]:tcp://10.201.2.91:33859 got new rank 4
[21:07:26] task [xgboost.dask-tcp://10.201.2.91:36323]:tcp://10.201.2.91:36323 got new rank 5
[21:07:26] task [xgboost.dask-tcp://10.201.2.91:40423]:tcp://10.201.2.91:40423 got new rank 6
[21:07:26] task [xgboost.dask-tcp://10.201.2.91:44071]:tcp://10.201.2.91:44071 got new rank 7
2024-04-19 21:09:44,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:09:44,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:09:44,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:10:18] task [xgboost.dask-tcp://10.201.1.105:36189]:tcp://10.201.1.105:36189 got new rank 0
[21:10:18] task [xgboost.dask-tcp://10.201.1.105:40369]:tcp://10.201.1.105:40369 got new rank 1
[21:10:19] task [xgboost.dask-tcp://10.201.1.105:43641]:tcp://10.201.1.105:43641 got new rank 2
[21:10:19] task [xgboost.dask-tcp://10.201.1.105:44701]:tcp://10.201.1.105:44701 got new rank 3
[21:10:19] task [xgboost.dask-tcp://10.201.2.91:33859]:tcp://10.201.2.91:33859 got new rank 4
[21:10:19] task [xgboost.dask-tcp://10.201.2.91:36323]:tcp://10.201.2.91:36323 got new rank 5
[21:10:19] task [xgboost.dask-tcp://10.201.2.91:40423]:tcp://10.201.2.91:40423 got new rank 6
[21:10:19] task [xgboost.dask-tcp://10.201.2.91:44071]:tcp://10.201.2.91:44071 got new rank 7
2024-04-19 21:12:30,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:30,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:12:30,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:13:12] task [xgboost.dask-tcp://10.201.1.105:36189]:tcp://10.201.1.105:36189 got new rank 0
[21:13:12] task [xgboost.dask-tcp://10.201.1.105:40369]:tcp://10.201.1.105:40369 got new rank 1
[21:13:12] task [xgboost.dask-tcp://10.201.1.105:43641]:tcp://10.201.1.105:43641 got new rank 2
[21:13:12] task [xgboost.dask-tcp://10.201.1.105:44701]:tcp://10.201.1.105:44701 got new rank 3
[21:13:12] task [xgboost.dask-tcp://10.201.2.91:33859]:tcp://10.201.2.91:33859 got new rank 4
[21:13:12] task [xgboost.dask-tcp://10.201.2.91:36323]:tcp://10.201.2.91:36323 got new rank 5
[21:13:12] task [xgboost.dask-tcp://10.201.2.91:40423]:tcp://10.201.2.91:40423 got new rank 6
[21:13:12] task [xgboost.dask-tcp://10.201.2.91:44071]:tcp://10.201.2.91:44071 got new rank 7
2024-04-19 21:15:30,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:30,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:30,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:30,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:31,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:31,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:43,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:44,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:45,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:46,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:46,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:46,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:47,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:15:48,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:16:29] task [xgboost.dask-tcp://10.201.1.105:36189]:tcp://10.201.1.105:36189 got new rank 0
[21:16:29] task [xgboost.dask-tcp://10.201.1.105:40369]:tcp://10.201.1.105:40369 got new rank 1
[21:16:29] task [xgboost.dask-tcp://10.201.1.105:43641]:tcp://10.201.1.105:43641 got new rank 2
[21:16:29] task [xgboost.dask-tcp://10.201.1.105:44701]:tcp://10.201.1.105:44701 got new rank 3
[21:16:29] task [xgboost.dask-tcp://10.201.2.91:33859]:tcp://10.201.2.91:33859 got new rank 4
[21:16:29] task [xgboost.dask-tcp://10.201.2.91:36323]:tcp://10.201.2.91:36323 got new rank 5
[21:16:29] task [xgboost.dask-tcp://10.201.2.91:40423]:tcp://10.201.2.91:40423 got new rank 6
[21:16:29] task [xgboost.dask-tcp://10.201.2.91:44071]:tcp://10.201.2.91:44071 got new rank 7
2024-04-19 21:18:35,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:18:35,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:36189. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:40369. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:43641. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:44701. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:40423. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:33859. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:36323. Reason: scheduler-close
2024-04-19 21:19:04,535 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:44071. Reason: scheduler-close
2024-04-19 21:19:04,537 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:42711'. Reason: scheduler-close
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41568 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41568 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,535 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41552 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41552 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33160 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33160 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41578 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41578 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33150 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33150 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41562 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:41562 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,536 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33146 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:33146 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:33033'. Reason: scheduler-close
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:43101'. Reason: scheduler-close
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:39303'. Reason: scheduler-close
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:45839'. Reason: scheduler-close
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:39757'. Reason: scheduler-close
2024-04-19 21:19:04,544 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:42029'. Reason: scheduler-close
2024-04-19 21:19:04,545 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:35401'. Reason: scheduler-close
2024-04-19 21:19:05,389 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:59146 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:05,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:19:05,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:41590 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.1.105:59126 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:05,391 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:59144 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:05,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.91:33184 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:05,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:19:05,390 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:19:05,391 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.91:55674 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.2.91:55660 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.2.91:33194 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 21:19:05,441 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,441 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,474 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,474 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,491 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,491 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,495 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,495 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,500 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,500 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,502 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,503 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,510 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,511 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:05,527 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 21:19:05,528 - distributed.nanny - INFO - Worker closed
2024-04-19 21:19:07,444 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,477 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,496 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,502 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,504 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,512 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:07,529 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:19:08,627 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.91:39757'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,628 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:08,636 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.91:42711'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,637 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:08,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.91:42029'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,643 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:08,653 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:33033'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,654 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:08,761 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:39303'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,761 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:08,766 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:43101'. Reason: nanny-close-gracefully
2024-04-19 21:19:08,767 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:09,092 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.91:35401'. Reason: nanny-close-gracefully
2024-04-19 21:19:09,093 - distributed.dask_worker - INFO - End worker
2024-04-19 21:19:09,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:45839'. Reason: nanny-close-gracefully
2024-04-19 21:19:09,116 - distributed.dask_worker - INFO - End worker
