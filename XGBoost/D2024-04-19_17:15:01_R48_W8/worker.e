2024-04-19 17:40:25,196 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,196 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,196 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,196 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,317 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,317 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,317 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,318 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,392 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,392 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,392 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,392 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.175:43437'
2024-04-19 17:40:25,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.175:42227'
2024-04-19 17:40:25,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.175:42835'
2024-04-19 17:40:25,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.175:45513'
2024-04-19 17:40:25,503 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,503 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,503 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,503 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.112:38653'
2024-04-19 17:40:25,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.112:41295'
2024-04-19 17:40:25,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.112:45141'
2024-04-19 17:40:25,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.112:44601'
2024-04-19 17:40:26,447 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,448 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,448 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,448 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,449 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,449 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,449 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,449 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,481 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,481 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,481 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,481 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,482 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,493 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,494 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,494 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,494 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,528 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,528 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,528 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,528 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:27,455 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,455 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,455 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,456 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,471 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,471 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,471 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,471 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:29,480 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-62a0b5c4-bbb8-4e06-8c1e-288528d0f447
2024-04-19 17:40:29,481 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.175:35851
2024-04-19 17:40:29,481 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.175:35851
2024-04-19 17:40:29,481 - distributed.worker - INFO -          dashboard at:         10.201.1.175:34761
2024-04-19 17:40:29,481 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,481 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,481 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,481 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cbzmmy3q
2024-04-19 17:40:29,481 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,483 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8f9c2ac2-62c1-4924-9b4c-88ecb2817036
2024-04-19 17:40:29,483 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.112:42593
2024-04-19 17:40:29,483 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.112:42593
2024-04-19 17:40:29,483 - distributed.worker - INFO -          dashboard at:         10.201.1.112:36381
2024-04-19 17:40:29,483 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,483 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,483 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,483 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,483 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1tigyfoe
2024-04-19 17:40:29,483 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,485 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-403391e1-d224-488f-a08a-6562b0e75325
2024-04-19 17:40:29,485 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.175:43971
2024-04-19 17:40:29,485 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.175:43971
2024-04-19 17:40:29,486 - distributed.worker - INFO -          dashboard at:         10.201.1.175:46813
2024-04-19 17:40:29,486 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,486 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,486 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,486 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2uir_1wn
2024-04-19 17:40:29,486 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,487 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-27cc5e80-c581-4e8d-9b17-9fd6b7b151b6
2024-04-19 17:40:29,487 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.112:40795
2024-04-19 17:40:29,487 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.112:40795
2024-04-19 17:40:29,487 - distributed.worker - INFO -          dashboard at:         10.201.1.112:39279
2024-04-19 17:40:29,487 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,487 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,487 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,487 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mg6rsln1
2024-04-19 17:40:29,487 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,490 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7a0c6d11-62bc-490d-b321-41a60d391316
2024-04-19 17:40:29,490 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.175:33535
2024-04-19 17:40:29,490 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.175:33535
2024-04-19 17:40:29,490 - distributed.worker - INFO -          dashboard at:         10.201.1.175:42339
2024-04-19 17:40:29,490 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,490 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,490 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,490 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,490 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqadjlnv
2024-04-19 17:40:29,490 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,492 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b325b61a-8ccc-48c8-849a-ea2fd62dd069
2024-04-19 17:40:29,492 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.112:37409
2024-04-19 17:40:29,492 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.112:37409
2024-04-19 17:40:29,492 - distributed.worker - INFO -          dashboard at:         10.201.1.112:44483
2024-04-19 17:40:29,492 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,492 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,492 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,492 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-48c5w3u0
2024-04-19 17:40:29,492 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,494 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9686367c-9e2c-443d-8948-789210ee6681
2024-04-19 17:40:29,494 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.175:32939
2024-04-19 17:40:29,494 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.175:32939
2024-04-19 17:40:29,494 - distributed.worker - INFO -          dashboard at:         10.201.1.175:38035
2024-04-19 17:40:29,494 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,494 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,494 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,494 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tbs0tp6p
2024-04-19 17:40:29,494 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,495 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-bdeebbf7-5a33-4fea-a0bf-f73c5445ea46
2024-04-19 17:40:29,495 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.112:32865
2024-04-19 17:40:29,495 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.112:32865
2024-04-19 17:40:29,496 - distributed.worker - INFO -          dashboard at:         10.201.1.112:39189
2024-04-19 17:40:29,496 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.1.129:8786
2024-04-19 17:40:29,496 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:29,496 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:29,496 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:29,496 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a05jhdc6
2024-04-19 17:40:29,496 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,227 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,227 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,228 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,228 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,228 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,229 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,229 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,229 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,230 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,230 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,231 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,231 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,290 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,290 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,291 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,291 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,291 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,292 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,292 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,292 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,293 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:31,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:31,293 - distributed.worker - INFO -         Registered to:    tcp://10.201.1.129:8786
2024-04-19 17:40:31,293 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:31,294 - distributed.core - INFO - Starting established connection to tcp://10.201.1.129:8786
2024-04-19 17:40:34,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:34,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:34,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:34,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:35,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:35,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:35,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:35,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:42,539 - distributed.utils_perf - INFO - full garbage collection released 161.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:44,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:44,209 - distributed.utils_perf - INFO - full garbage collection released 1.21 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:44,997 - distributed.utils_perf - INFO - full garbage collection released 42.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:48,217 - distributed.utils_perf - INFO - full garbage collection released 1.48 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:48,428 - distributed.utils_perf - INFO - full garbage collection released 5.57 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:48,468 - distributed.utils_perf - INFO - full garbage collection released 497.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:50,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:50,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:51,463 - distributed.utils_perf - INFO - full garbage collection released 23.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:52,663 - distributed.utils_perf - INFO - full garbage collection released 6.63 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:53,211 - distributed.utils_perf - INFO - full garbage collection released 101.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:53,845 - distributed.utils_perf - INFO - full garbage collection released 149.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:54,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:54,203 - distributed.utils_perf - INFO - full garbage collection released 141.85 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:54,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:54,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:55,595 - distributed.utils_perf - INFO - full garbage collection released 1.57 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:57,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:58,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:58,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:59,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:00,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:02,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:05,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:06,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:06,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:08,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:10,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:11,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:14,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:15,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:15,650 - distributed.utils_perf - INFO - full garbage collection released 590.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:15,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:17,817 - distributed.utils_perf - INFO - full garbage collection released 150.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:18,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:18,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:20,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:22,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:24,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:24,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:26,811 - distributed.utils_perf - INFO - full garbage collection released 446.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:27,325 - distributed.utils_perf - INFO - full garbage collection released 479.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:29,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:29,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:29,659 - distributed.utils_perf - INFO - full garbage collection released 185.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:33,784 - distributed.utils_perf - INFO - full garbage collection released 29.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:34,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:34,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:36,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:37,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:41,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:42,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:42,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:45,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:49,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:52,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:54,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:59,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:01,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:04,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:08,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:08,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:10,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:11,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:12,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:16,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:18,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:19,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:21,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:24,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:26,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:28,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:29,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:31,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:34,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:37,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:38,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:38,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:41,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:42,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:43,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:47,977 - distributed.utils_perf - INFO - full garbage collection released 584.28 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:48,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:54,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:57,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:59,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:01,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:03,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:04,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:07,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:11,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:12,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:13,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:13,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:17,434 - distributed.utils_perf - INFO - full garbage collection released 207.26 MiB from 74 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:17,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:19,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:20,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:21,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:22,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:24,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:25,484 - distributed.utils_perf - INFO - full garbage collection released 34.62 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:28,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:29,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:31,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:32,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:32,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:33,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:38,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:41,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:43,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:46,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:51,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:54,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:58,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:00,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 47.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:01,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:01,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:03,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:06,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:07,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:08,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:09,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:12,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:16,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:17,026 - distributed.utils_perf - INFO - full garbage collection released 2.44 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:17,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:21,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:21,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:23,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:24,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:26,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:28,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:30,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:31,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:31,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:33,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:37,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:38,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:38,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:39,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:41,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:41,292 - distributed.utils_perf - INFO - full garbage collection released 14.85 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:42,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:42,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:43,818 - distributed.utils_perf - INFO - full garbage collection released 2.66 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:48,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:51,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:51,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:55,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:58,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:02,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:03,600 - distributed.utils_perf - INFO - full garbage collection released 637.92 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:45:03,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:04,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:08,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:11,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:12,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:13,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:15,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:22,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:22,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:24,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:25,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:26,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:27,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:28,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:30,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:30,987 - distributed.utils_perf - INFO - full garbage collection released 163.06 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:45:34,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:35,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:37,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:40,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:41,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:43,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:47,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:48,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:52,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,958 - distributed.utils_perf - INFO - full garbage collection released 12.94 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:45:55,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:57,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:58,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:59,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:00,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:00,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:01,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:05,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:07,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:08,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:08,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:18,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:21,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:21,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:22,513 - distributed.utils_perf - INFO - full garbage collection released 2.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:46:23,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:24,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:24,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:27,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:27,417 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:46:28,510 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:46:29,867 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:46:30,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:31,593 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:46:35,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:37,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:37,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:38,042 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:46:38,610 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:46:39,305 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:46:40,179 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:46:41,257 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:46:41,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:42,576 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:46:43,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:44,262 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 17:46:45,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:46,329 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 17:46:47,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:51,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:51,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:55,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:58,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:00,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:01,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:08,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:11,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:12,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:19,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:23,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:24,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:25,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:27,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:28,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:31,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:32,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:33,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:36,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:37,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:39,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:41,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:43,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:52,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:52,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:53,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:59,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:00,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:01,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:02,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:03,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:05,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:05,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:06,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:10,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:12,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:12,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:12,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:13,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:14,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:18,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:21,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:24,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:28,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:35,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:36,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:38,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:39,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:40,236 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:48:40,657 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:48:41,071 - distributed.utils_perf - INFO - full garbage collection released 22.64 MiB from 132 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:48:41,172 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:41,729 - distributed.utils_perf - INFO - full garbage collection released 1.30 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:48:41,808 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:42,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:42,590 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:42,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:43,548 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:44,728 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:45,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:46,240 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:48,080 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:48:50,346 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:48:51,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:56,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:00,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:01,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:04,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:05,115 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:05,965 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:07,021 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:49:08,313 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:49:09,891 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:49:09,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:11,854 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:49:14,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:16,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:24,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:27,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:31,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:34,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:34,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:36,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:42,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:43,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:45,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:49,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:53,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:57,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:58,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:03,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:04,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:05,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:08,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:11,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:18,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:18,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:37,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:38,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:47,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:47,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:51,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:51:26] task [xgboost.dask-tcp://10.201.1.112:32865]:tcp://10.201.1.112:32865 got new rank 0
[17:51:26] task [xgboost.dask-tcp://10.201.1.112:37409]:tcp://10.201.1.112:37409 got new rank 1
[17:51:26] task [xgboost.dask-tcp://10.201.1.112:40795]:tcp://10.201.1.112:40795 got new rank 2
[17:51:26] task [xgboost.dask-tcp://10.201.1.112:42593]:tcp://10.201.1.112:42593 got new rank 3
[17:51:26] task [xgboost.dask-tcp://10.201.1.175:32939]:tcp://10.201.1.175:32939 got new rank 4
[17:51:26] task [xgboost.dask-tcp://10.201.1.175:33535]:tcp://10.201.1.175:33535 got new rank 5
[17:51:26] task [xgboost.dask-tcp://10.201.1.175:35851]:tcp://10.201.1.175:35851 got new rank 6
[17:51:26] task [xgboost.dask-tcp://10.201.1.175:43971]:tcp://10.201.1.175:43971 got new rank 7
2024-04-19 17:53:51,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:53:51,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:53:51,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:54:32] task [xgboost.dask-tcp://10.201.1.112:32865]:tcp://10.201.1.112:32865 got new rank 0
[17:54:32] task [xgboost.dask-tcp://10.201.1.112:37409]:tcp://10.201.1.112:37409 got new rank 1
[17:54:32] task [xgboost.dask-tcp://10.201.1.112:40795]:tcp://10.201.1.112:40795 got new rank 2
[17:54:32] task [xgboost.dask-tcp://10.201.1.112:42593]:tcp://10.201.1.112:42593 got new rank 3
[17:54:32] task [xgboost.dask-tcp://10.201.1.175:32939]:tcp://10.201.1.175:32939 got new rank 4
[17:54:32] task [xgboost.dask-tcp://10.201.1.175:33535]:tcp://10.201.1.175:33535 got new rank 5
[17:54:32] task [xgboost.dask-tcp://10.201.1.175:35851]:tcp://10.201.1.175:35851 got new rank 6
[17:54:32] task [xgboost.dask-tcp://10.201.1.175:43971]:tcp://10.201.1.175:43971 got new rank 7
2024-04-19 17:57:19,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:57:24,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:58:00] task [xgboost.dask-tcp://10.201.1.112:32865]:tcp://10.201.1.112:32865 got new rank 0
[17:58:00] task [xgboost.dask-tcp://10.201.1.112:37409]:tcp://10.201.1.112:37409 got new rank 1
[17:58:00] task [xgboost.dask-tcp://10.201.1.112:40795]:tcp://10.201.1.112:40795 got new rank 2
[17:58:00] task [xgboost.dask-tcp://10.201.1.112:42593]:tcp://10.201.1.112:42593 got new rank 3
[17:58:00] task [xgboost.dask-tcp://10.201.1.175:32939]:tcp://10.201.1.175:32939 got new rank 4
[17:58:00] task [xgboost.dask-tcp://10.201.1.175:33535]:tcp://10.201.1.175:33535 got new rank 5
[17:58:00] task [xgboost.dask-tcp://10.201.1.175:35851]:tcp://10.201.1.175:35851 got new rank 6
[17:58:00] task [xgboost.dask-tcp://10.201.1.175:43971]:tcp://10.201.1.175:43971 got new rank 7
2024-04-19 18:00:17,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:00:17,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:00:17,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:00:18,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[18:01:03] task [xgboost.dask-tcp://10.201.1.112:32865]:tcp://10.201.1.112:32865 got new rank 0
[18:01:03] task [xgboost.dask-tcp://10.201.1.112:37409]:tcp://10.201.1.112:37409 got new rank 1
[18:01:03] task [xgboost.dask-tcp://10.201.1.112:40795]:tcp://10.201.1.112:40795 got new rank 2
[18:01:03] task [xgboost.dask-tcp://10.201.1.112:42593]:tcp://10.201.1.112:42593 got new rank 3
[18:01:03] task [xgboost.dask-tcp://10.201.1.175:32939]:tcp://10.201.1.175:32939 got new rank 4
[18:01:03] task [xgboost.dask-tcp://10.201.1.175:33535]:tcp://10.201.1.175:33535 got new rank 5
[18:01:03] task [xgboost.dask-tcp://10.201.1.175:35851]:tcp://10.201.1.175:35851 got new rank 6
[18:01:03] task [xgboost.dask-tcp://10.201.1.175:43971]:tcp://10.201.1.175:43971 got new rank 7
2024-04-19 18:03:38,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:40,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:40,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:53,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:55,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:55,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:55,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:56,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:56,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:56,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:03:58,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:04:15,862 - distributed.utils_perf - INFO - full garbage collection released 1.77 GiB from 171 reference cycles (threshold: 9.54 MiB)
[18:04:39] task [xgboost.dask-tcp://10.201.1.112:32865]:tcp://10.201.1.112:32865 got new rank 0
[18:04:39] task [xgboost.dask-tcp://10.201.1.112:37409]:tcp://10.201.1.112:37409 got new rank 1
[18:04:39] task [xgboost.dask-tcp://10.201.1.112:40795]:tcp://10.201.1.112:40795 got new rank 2
[18:04:39] task [xgboost.dask-tcp://10.201.1.112:42593]:tcp://10.201.1.112:42593 got new rank 3
[18:04:39] task [xgboost.dask-tcp://10.201.1.175:32939]:tcp://10.201.1.175:32939 got new rank 4
[18:04:39] task [xgboost.dask-tcp://10.201.1.175:33535]:tcp://10.201.1.175:33535 got new rank 5
[18:04:39] task [xgboost.dask-tcp://10.201.1.175:35851]:tcp://10.201.1.175:35851 got new rank 6
[18:04:39] task [xgboost.dask-tcp://10.201.1.175:43971]:tcp://10.201.1.175:43971 got new rank 7
2024-04-19 18:06:54,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:06:54,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.175:35851. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.112:37409. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.175:43971. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.112:42593. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.175:32939. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.175:33535. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.112:40795. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.112:32865. Reason: scheduler-close
2024-04-19 18:07:28,497 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.112:44601'. Reason: scheduler-close
2024-04-19 18:07:28,498 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.112:41295'. Reason: scheduler-close
2024-04-19 18:07:28,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.112:60416 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.112:60416 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,500 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.112:45141'. Reason: scheduler-close
2024-04-19 18:07:28,495 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40804 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40804 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40836 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40836 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,495 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40816 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40816 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40824 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.175:40824 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.175:43437'. Reason: scheduler-close
2024-04-19 18:07:28,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.175:42227'. Reason: scheduler-close
2024-04-19 18:07:28,504 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.175:45513'. Reason: scheduler-close
2024-04-19 18:07:28,504 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.175:42835'. Reason: scheduler-close
2024-04-19 18:07:28,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.112:60398 remote=tcp://10.201.1.129:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.112:60398 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:28,505 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.112:38653'. Reason: scheduler-close
2024-04-19 18:07:29,289 - distributed.comm.tcp - INFO - Connection from tcp://10.201.1.175:41716 closed before handshake completed
2024-04-19 18:07:29,288 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:07:29,289 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.175:40852 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:29,288 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:07:29,289 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:07:29,287 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.175:40872 remote=tcp://10.201.1.129:8786>: Stream is closed
/10.201.1.175:40870 remote=tcp://10.201.1.129:8786>: Stream is closed
/10.201.1.112:60454 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:29,288 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.112:56666 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:29,288 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.112:56698 remote=tcp://10.201.1.129:8786>: Stream is closed
/10.201.1.112:56696 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:29,290 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.175:34392 remote=tcp://10.201.1.129:8786>: Stream is closed
2024-04-19 18:07:29,349 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,349 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,357 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,357 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,376 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,376 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,392 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,392 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,418 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,418 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,418 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,419 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,419 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,419 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:29,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.129:8786; closing.
2024-04-19 18:07:29,432 - distributed.nanny - INFO - Worker closed
2024-04-19 18:07:31,353 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,358 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,378 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,395 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,419 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,421 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,428 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:31,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:07:32,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.175:42835'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,646 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:32,654 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.175:43437'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,654 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:32,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.175:45513'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,663 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:32,746 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.112:45141'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,747 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:32,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.112:38653'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,754 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:32,761 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.112:44601'. Reason: nanny-close-gracefully
2024-04-19 18:07:32,762 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:33,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.175:42227'. Reason: nanny-close-gracefully
2024-04-19 18:07:33,085 - distributed.dask_worker - INFO - End worker
2024-04-19 18:07:33,205 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.112:41295'. Reason: nanny-close-gracefully
2024-04-19 18:07:33,206 - distributed.dask_worker - INFO - End worker
