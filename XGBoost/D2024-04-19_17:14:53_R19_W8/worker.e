2024-04-19 17:15:30,104 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,104 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,104 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,104 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,165 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,165 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,166 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,166 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.218:46477'
2024-04-19 17:15:30,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.218:39545'
2024-04-19 17:15:30,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.218:34953'
2024-04-19 17:15:30,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.218:38953'
2024-04-19 17:15:31,151 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,151 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,152 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,152 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,152 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,153 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,153 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,153 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,197 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,197 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,197 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,197 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,320 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,320 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,321 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,321 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,464 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,464 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,464 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,464 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.222:44381'
2024-04-19 17:15:31,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.222:42611'
2024-04-19 17:15:31,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.222:36629'
2024-04-19 17:15:31,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.222:35003'
2024-04-19 17:15:31,823 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:31,823 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:31,824 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:31,824 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,453 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,453 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,454 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,454 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:32,454 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,454 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,454 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,455 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:32,499 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,499 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,500 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,500 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,821 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-35337879-ba3e-45c0-aa89-3473d1781334
2024-04-19 17:15:32,821 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b5c8a897-78ac-4670-8d31-d3d3d02620a6
2024-04-19 17:15:32,821 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.218:40537
2024-04-19 17:15:32,821 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.218:46219
2024-04-19 17:15:32,821 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.218:46219
2024-04-19 17:15:32,821 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.218:40537
2024-04-19 17:15:32,822 - distributed.worker - INFO -          dashboard at:         10.201.3.218:44357
2024-04-19 17:15:32,822 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:32,822 - distributed.worker - INFO -          dashboard at:         10.201.3.218:34175
2024-04-19 17:15:32,822 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:32,822 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,822 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:32,822 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:32,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w8w8n5tu
2024-04-19 17:15:32,822 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,822 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:32,822 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:32,822 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b74m_3vf
2024-04-19 17:15:32,822 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,823 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-55b55cec-24d7-4548-8cd1-c0723163bc56
2024-04-19 17:15:32,823 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.218:39535
2024-04-19 17:15:32,823 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.218:39535
2024-04-19 17:15:32,823 - distributed.worker - INFO -          dashboard at:         10.201.3.218:43419
2024-04-19 17:15:32,823 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:32,823 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,824 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:32,824 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:32,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0_4ptcck
2024-04-19 17:15:32,824 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,824 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a1bf009e-6732-45c7-91da-f153e64e361d
2024-04-19 17:15:32,825 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.218:46101
2024-04-19 17:15:32,825 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.218:46101
2024-04-19 17:15:32,825 - distributed.worker - INFO -          dashboard at:         10.201.3.218:37711
2024-04-19 17:15:32,825 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:32,825 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:32,825 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:32,825 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:32,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-brjc7mnr
2024-04-19 17:15:32,825 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,456 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,456 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,456 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,456 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:34,453 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-97638594-22d2-4b02-b06a-80ef690f9d67
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.222:36951
2024-04-19 17:15:34,453 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.222:36951
2024-04-19 17:15:34,453 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2622a1a0-fc4d-4cb3-9832-2df4383f7e1a
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.222:39519
2024-04-19 17:15:34,453 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.222:39519
2024-04-19 17:15:34,453 - distributed.worker - INFO -          dashboard at:         10.201.3.222:43891
2024-04-19 17:15:34,453 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,453 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3a29feb2-1fff-4473-8d7f-baf092cdbc82
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.222:44879
2024-04-19 17:15:34,453 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.222:44879
2024-04-19 17:15:34,453 - distributed.worker - INFO -          dashboard at:         10.201.3.222:33909
2024-04-19 17:15:34,453 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,453 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ts2hc5p3
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b567d894-b20a-4769-8e20-d9ae0ccca2ec
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.222:33911
2024-04-19 17:15:34,453 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.222:33911
2024-04-19 17:15:34,453 - distributed.worker - INFO -          dashboard at:         10.201.3.222:36841
2024-04-19 17:15:34,453 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,453 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cs0r5zts
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -          dashboard at:         10.201.3.222:41219
2024-04-19 17:15:34,453 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.15:8786
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:34,453 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f8i52nb5
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:34,453 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:34,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8rf7_yn2
2024-04-19 17:15:34,453 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:35,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:35,385 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:35,385 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:35,386 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:35,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:35,388 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:35,388 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:35,388 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:35,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:35,391 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:35,391 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:35,391 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:35,391 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:35,392 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:35,392 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:35,393 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:37,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,370 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:37,370 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,370 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:37,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,371 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:37,371 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,372 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:37,372 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:37,372 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,373 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:37,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,373 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.15:8786
2024-04-19 17:15:37,373 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,374 - distributed.core - INFO - Starting established connection to tcp://10.201.3.15:8786
2024-04-19 17:15:47,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:47,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:48,367 - distributed.utils_perf - INFO - full garbage collection released 88.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:48,558 - distributed.utils_perf - INFO - full garbage collection released 847.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:48,933 - distributed.utils_perf - INFO - full garbage collection released 580.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:53,807 - distributed.utils_perf - INFO - full garbage collection released 447.61 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,305 - distributed.utils_perf - INFO - full garbage collection released 131.87 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,736 - distributed.utils_perf - INFO - full garbage collection released 3.06 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,857 - distributed.utils_perf - INFO - full garbage collection released 260.78 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:55,183 - distributed.utils_perf - INFO - full garbage collection released 555.08 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:56,490 - distributed.utils_perf - INFO - full garbage collection released 1.71 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:56,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:57,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:58,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:59,288 - distributed.utils_perf - INFO - full garbage collection released 41.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:00,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:00,821 - distributed.utils_perf - INFO - full garbage collection released 200.21 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:01,521 - distributed.utils_perf - INFO - full garbage collection released 2.14 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:01,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:02,110 - distributed.utils_perf - INFO - full garbage collection released 511.74 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:02,733 - distributed.utils_perf - INFO - full garbage collection released 594.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:02,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:03,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:05,333 - distributed.utils_perf - INFO - full garbage collection released 50.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:05,504 - distributed.utils_perf - INFO - full garbage collection released 1.41 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:06,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:09,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:10,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:12,013 - distributed.utils_perf - INFO - full garbage collection released 1.22 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:12,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:13,215 - distributed.utils_perf - INFO - full garbage collection released 133.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:13,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:14,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:15,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:18,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:19,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:21,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:22,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:26,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,032 - distributed.utils_perf - INFO - full garbage collection released 267.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:29,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:30,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,443 - distributed.utils_perf - INFO - full garbage collection released 75.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:31,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:37,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,502 - distributed.utils_perf - INFO - full garbage collection released 158.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:39,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,735 - distributed.utils_perf - INFO - full garbage collection released 2.82 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:44,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:49,853 - distributed.utils_perf - INFO - full garbage collection released 463.77 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,028 - distributed.utils_perf - INFO - full garbage collection released 328.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:04,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:06,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:16,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:16,266 - distributed.utils_perf - INFO - full garbage collection released 541.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:18,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,666 - distributed.utils_perf - INFO - full garbage collection released 49.93 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:20,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:21,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,776 - distributed.utils_perf - INFO - full garbage collection released 231.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:29,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:39,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:40,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:45,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:50,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:51,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:04,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,735 - distributed.utils_perf - INFO - full garbage collection released 281.38 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:20,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:21,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,704 - distributed.utils_perf - INFO - full garbage collection released 583.03 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:25,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:42,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:47,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:49,844 - distributed.utils_perf - INFO - full garbage collection released 2.68 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:55,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:03,078 - distributed.utils_perf - INFO - full garbage collection released 1.27 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:05,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:09,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:18,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:19,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:21,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:29,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:34,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,289 - distributed.utils_perf - INFO - full garbage collection released 4.19 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:36,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:42,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:44,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,975 - distributed.utils_perf - INFO - full garbage collection released 179.99 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:47,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:12,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:17,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:20,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:21,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,750 - distributed.utils_perf - INFO - full garbage collection released 7.19 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:24,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:51,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:58,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:59,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:04,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:11,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:14,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:22,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:23,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:24,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:27,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:33,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:39,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:48,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,327 - distributed.utils_perf - INFO - full garbage collection released 1.16 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:56,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:05,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:06,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:20,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,630 - distributed.utils_perf - INFO - full garbage collection released 1.73 GiB from 189 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:41,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,072 - distributed.utils_perf - INFO - full garbage collection released 7.03 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:45,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,156 - distributed.utils_perf - INFO - full garbage collection released 674.15 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:22:59,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:04,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,619 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:09,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,313 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:11,163 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:12,206 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:13,522 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:15,174 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:15,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,185 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:19,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,691 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:22,776 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:23:24,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:25,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:36,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,845 - distributed.utils_perf - INFO - full garbage collection released 1.43 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:44,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:54,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:56,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,910 - distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:00,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:02,186 - distributed.utils_perf - INFO - full garbage collection released 776.04 MiB from 151 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:04,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,328 - distributed.utils_perf - INFO - full garbage collection released 25.54 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:06,851 - distributed.utils_perf - INFO - full garbage collection released 299.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:07,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,434 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:07,434 - distributed.utils_perf - INFO - full garbage collection released 191.96 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:08,155 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:08,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,035 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,120 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)
2024-04-19 17:24:10,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,460 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:13,102 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:15,208 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,659 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:17,787 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:18,889 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:21,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,090 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:26,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,867 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:27,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,813 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:28,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,991 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:24:29,922 - distributed.utils_perf - INFO - full garbage collection released 12.92 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:30,487 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:31,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,332 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:24:33,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,580 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:34,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,468 - distributed.utils_perf - INFO - full garbage collection released 9.85 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:42,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:49,496 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:52,140 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:25:00,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:25:30] task [xgboost.dask-tcp://10.201.3.218:39535]:tcp://10.201.3.218:39535 got new rank 0
[17:25:30] task [xgboost.dask-tcp://10.201.3.218:40537]:tcp://10.201.3.218:40537 got new rank 1
[17:25:30] task [xgboost.dask-tcp://10.201.3.218:46101]:tcp://10.201.3.218:46101 got new rank 2
[17:25:30] task [xgboost.dask-tcp://10.201.3.218:46219]:tcp://10.201.3.218:46219 got new rank 3
[17:25:30] task [xgboost.dask-tcp://10.201.3.222:33911]:tcp://10.201.3.222:33911 got new rank 4
[17:25:30] task [xgboost.dask-tcp://10.201.3.222:36951]:tcp://10.201.3.222:36951 got new rank 5
[17:25:30] task [xgboost.dask-tcp://10.201.3.222:39519]:tcp://10.201.3.222:39519 got new rank 6
[17:25:30] task [xgboost.dask-tcp://10.201.3.222:44879]:tcp://10.201.3.222:44879 got new rank 7
2024-04-19 17:27:41,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:25] task [xgboost.dask-tcp://10.201.3.218:39535]:tcp://10.201.3.218:39535 got new rank 0
[17:28:25] task [xgboost.dask-tcp://10.201.3.218:40537]:tcp://10.201.3.218:40537 got new rank 1
[17:28:25] task [xgboost.dask-tcp://10.201.3.218:46101]:tcp://10.201.3.218:46101 got new rank 2
[17:28:25] task [xgboost.dask-tcp://10.201.3.218:46219]:tcp://10.201.3.218:46219 got new rank 3
[17:28:25] task [xgboost.dask-tcp://10.201.3.222:33911]:tcp://10.201.3.222:33911 got new rank 4
[17:28:25] task [xgboost.dask-tcp://10.201.3.222:36951]:tcp://10.201.3.222:36951 got new rank 5
[17:28:25] task [xgboost.dask-tcp://10.201.3.222:39519]:tcp://10.201.3.222:39519 got new rank 6
[17:28:25] task [xgboost.dask-tcp://10.201.3.222:44879]:tcp://10.201.3.222:44879 got new rank 7
[17:32:01] task [xgboost.dask-tcp://10.201.3.218:39535]:tcp://10.201.3.218:39535 got new rank 0
[17:32:01] task [xgboost.dask-tcp://10.201.3.218:40537]:tcp://10.201.3.218:40537 got new rank 1
[17:32:01] task [xgboost.dask-tcp://10.201.3.218:46101]:tcp://10.201.3.218:46101 got new rank 2
[17:32:01] task [xgboost.dask-tcp://10.201.3.218:46219]:tcp://10.201.3.218:46219 got new rank 3
[17:32:01] task [xgboost.dask-tcp://10.201.3.222:33911]:tcp://10.201.3.222:33911 got new rank 4
[17:32:01] task [xgboost.dask-tcp://10.201.3.222:36951]:tcp://10.201.3.222:36951 got new rank 5
[17:32:01] task [xgboost.dask-tcp://10.201.3.222:39519]:tcp://10.201.3.222:39519 got new rank 6
[17:32:01] task [xgboost.dask-tcp://10.201.3.222:44879]:tcp://10.201.3.222:44879 got new rank 7
[17:34:53] task [xgboost.dask-tcp://10.201.3.218:39535]:tcp://10.201.3.218:39535 got new rank 0
[17:34:53] task [xgboost.dask-tcp://10.201.3.218:40537]:tcp://10.201.3.218:40537 got new rank 1
[17:34:53] task [xgboost.dask-tcp://10.201.3.218:46101]:tcp://10.201.3.218:46101 got new rank 2
[17:34:53] task [xgboost.dask-tcp://10.201.3.218:46219]:tcp://10.201.3.218:46219 got new rank 3
[17:34:53] task [xgboost.dask-tcp://10.201.3.222:33911]:tcp://10.201.3.222:33911 got new rank 4
[17:34:53] task [xgboost.dask-tcp://10.201.3.222:36951]:tcp://10.201.3.222:36951 got new rank 5
[17:34:53] task [xgboost.dask-tcp://10.201.3.222:39519]:tcp://10.201.3.222:39519 got new rank 6
[17:34:53] task [xgboost.dask-tcp://10.201.3.222:44879]:tcp://10.201.3.222:44879 got new rank 7
2024-04-19 17:36:59,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:13,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:14,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:14,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:15,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:15,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:16,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:17,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:20,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:38:00] task [xgboost.dask-tcp://10.201.3.218:39535]:tcp://10.201.3.218:39535 got new rank 0
[17:38:00] task [xgboost.dask-tcp://10.201.3.218:40537]:tcp://10.201.3.218:40537 got new rank 1
[17:38:00] task [xgboost.dask-tcp://10.201.3.218:46101]:tcp://10.201.3.218:46101 got new rank 2
[17:38:00] task [xgboost.dask-tcp://10.201.3.218:46219]:tcp://10.201.3.218:46219 got new rank 3
[17:38:00] task [xgboost.dask-tcp://10.201.3.222:33911]:tcp://10.201.3.222:33911 got new rank 4
[17:38:00] task [xgboost.dask-tcp://10.201.3.222:36951]:tcp://10.201.3.222:36951 got new rank 5
[17:38:00] task [xgboost.dask-tcp://10.201.3.222:39519]:tcp://10.201.3.222:39519 got new rank 6
[17:38:00] task [xgboost.dask-tcp://10.201.3.222:44879]:tcp://10.201.3.222:44879 got new rank 7
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.218:46219. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.218:40537. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.218:46101. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.222:33911. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.218:39535. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.222:36951. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.222:39519. Reason: scheduler-close
2024-04-19 17:40:40,993 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.222:44879. Reason: scheduler-close
2024-04-19 17:40:40,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.222:44381'. Reason: scheduler-close
2024-04-19 17:40:40,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.218:38953'. Reason: scheduler-close
2024-04-19 17:40:40,995 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.222:36629'. Reason: scheduler-close
2024-04-19 17:40:40,996 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.222:42611'. Reason: scheduler-close
2024-04-19 17:40:40,996 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.222:35003'. Reason: scheduler-close
2024-04-19 17:40:40,994 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46154 remote=tcp://10.201.3.15:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46154 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:40,995 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46176 remote=tcp://10.201.3.15:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46176 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:40,994 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46188 remote=tcp://10.201.3.15:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.218:46188 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:41,002 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.218:39545'. Reason: scheduler-close
2024-04-19 17:40:41,002 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.218:46477'. Reason: scheduler-close
2024-04-19 17:40:41,002 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.218:34953'. Reason: scheduler-close
2024-04-19 17:40:41,784 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:41,784 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:41,784 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.218:40980 remote=tcp://10.201.3.15:8786>: Stream is closed
/10.201.3.218:46216 remote=tcp://10.201.3.15:8786>: Stream is closed
/10.201.3.218:46222 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:41,784 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.218:40992 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:41,783 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:41,782 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:41,783 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.222:51580 remote=tcp://10.201.3.15:8786>: Stream is closed
/10.201.3.222:59750 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:41,783 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.3.222:51596 remote=tcp://10.201.3.15:8786>: Stream is closed
/10.201.3.222:59730 remote=tcp://10.201.3.15:8786>: Stream is closed
2024-04-19 17:40:41,853 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,854 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,863 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,863 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,871 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,871 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,897 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,897 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,901 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,901 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,901 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,902 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,928 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,928 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:41,931 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.15:8786; closing.
2024-04-19 17:40:41,931 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:43,856 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:43,865 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:43,874 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:43,899 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:43,903 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:43,934 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:45,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.222:36629'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,143 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.218:38953'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,145 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.218:39545'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,153 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.218:46477'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,159 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,249 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.222:35003'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,250 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.222:44381'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,256 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,603 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.218:34953'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,604 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:45,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.222:42611'. Reason: nanny-close-gracefully
2024-04-19 17:40:45,620 - distributed.dask_worker - INFO - End worker
