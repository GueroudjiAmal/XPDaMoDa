2024-04-19 17:15:58,981 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:58,981 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:58,981 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:58,982 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,217 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,217 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,217 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,217 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.198:36363'
2024-04-19 17:15:59,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.198:46145'
2024-04-19 17:15:59,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.198:34227'
2024-04-19 17:15:59,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.198:39007'
2024-04-19 17:15:59,637 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,638 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,638 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,638 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:59,907 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,907 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,907 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,907 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:59,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.212:34851'
2024-04-19 17:15:59,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.212:34765'
2024-04-19 17:15:59,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.212:33585'
2024-04-19 17:15:59,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.212:34411'
2024-04-19 17:16:00,066 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,066 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,067 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,067 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,067 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,067 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,067 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,068 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,092 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,092 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,092 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,092 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:00,957 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,958 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,958 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,958 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:00,958 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,959 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,959 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:00,959 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:01,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,002 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,003 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,003 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:01,006 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,006 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,006 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:01,006 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,015 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,015 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,015 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,015 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:02,042 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-074fb713-83b1-4c85-b8c7-26b83bc19de7
2024-04-19 17:16:02,042 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-773989d6-3ba5-415a-b281-e07e468bfc58
2024-04-19 17:16:02,042 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3c7d55df-27d8-475c-b872-520ec89b3a70
2024-04-19 17:16:02,042 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.198:40235
2024-04-19 17:16:02,042 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.198:40235
2024-04-19 17:16:02,042 - distributed.worker - INFO -          dashboard at:         10.201.2.198:43949
2024-04-19 17:16:02,042 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-80a5712f-0654-48ba-986b-07605d320879
2024-04-19 17:16:02,042 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.198:39173
2024-04-19 17:16:02,042 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.198:39173
2024-04-19 17:16:02,042 - distributed.worker - INFO -          dashboard at:         10.201.2.198:33125
2024-04-19 17:16:02,042 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.198:45751
2024-04-19 17:16:02,042 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.198:45751
2024-04-19 17:16:02,042 - distributed.worker - INFO -          dashboard at:         10.201.2.198:35529
2024-04-19 17:16:02,042 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:02,042 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,042 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,042 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.198:40381
2024-04-19 17:16:02,042 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.198:40381
2024-04-19 17:16:02,042 - distributed.worker - INFO -          dashboard at:         10.201.2.198:42263
2024-04-19 17:16:02,042 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:02,042 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,042 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,043 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lk1r0nw6
2024-04-19 17:16:02,042 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:02,042 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,042 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,042 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b72wohne
2024-04-19 17:16:02,043 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,042 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:02,042 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,043 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:02,043 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ew8vf_70
2024-04-19 17:16:02,043 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,043 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:02,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0lod0jsz
2024-04-19 17:16:02,043 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:02,043 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5a10cd0a-d042-44c1-be47-211c2a8eb332
2024-04-19 17:16:03,084 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-680fb7e8-36a6-40c5-9e0e-48f2cf771f11
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.212:36039
2024-04-19 17:16:03,084 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.212:36039
2024-04-19 17:16:03,084 - distributed.worker - INFO -          dashboard at:         10.201.2.212:42175
2024-04-19 17:16:03,084 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c284ceb3-88d6-4e75-8b5d-1b81dff10deb
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.212:40013
2024-04-19 17:16:03,084 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.212:40013
2024-04-19 17:16:03,084 - distributed.worker - INFO -          dashboard at:         10.201.2.212:36633
2024-04-19 17:16:03,084 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,084 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-33xb1kr3
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.212:37355
2024-04-19 17:16:03,084 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.212:37355
2024-04-19 17:16:03,084 - distributed.worker - INFO -          dashboard at:         10.201.2.212:43095
2024-04-19 17:16:03,084 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,084 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nau6dxih
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-29eda46e-5870-48d3-81dc-48770b2a1b48
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.212:43001
2024-04-19 17:16:03,084 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.212:43001
2024-04-19 17:16:03,084 - distributed.worker - INFO -          dashboard at:         10.201.2.212:35957
2024-04-19 17:16:03,084 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.2.206:8786
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,084 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9v0lpup_
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:03,084 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:03,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2pngr6u4
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:03,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,686 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,686 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,687 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,688 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,688 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,688 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,689 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,691 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,691 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,691 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,692 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,693 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,693 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,694 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,694 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,695 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,695 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,696 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,696 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,697 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,697 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,697 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:05,697 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:05,698 - distributed.worker - INFO -         Registered to:    tcp://10.201.2.206:8786
2024-04-19 17:16:05,698 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:05,698 - distributed.core - INFO - Starting established connection to tcp://10.201.2.206:8786
2024-04-19 17:16:15,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:15,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:15,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:15,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:16,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:19,173 - distributed.utils_perf - INFO - full garbage collection released 30.04 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:22,647 - distributed.utils_perf - INFO - full garbage collection released 849.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:22,991 - distributed.utils_perf - INFO - full garbage collection released 606.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,844 - distributed.utils_perf - INFO - full garbage collection released 752.45 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:23,995 - distributed.utils_perf - INFO - full garbage collection released 41.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:24,138 - distributed.utils_perf - INFO - full garbage collection released 631.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:26,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:26,906 - distributed.utils_perf - INFO - full garbage collection released 590.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:27,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:27,806 - distributed.utils_perf - INFO - full garbage collection released 0.91 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:27,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:29,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,052 - distributed.utils_perf - INFO - full garbage collection released 1.32 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:32,640 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:32,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,612 - distributed.utils_perf - INFO - full garbage collection released 2.16 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:40,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:41,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,548 - distributed.utils_perf - INFO - full garbage collection released 74.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:47,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:50,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:52,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:59,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:03,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,159 - distributed.utils_perf - INFO - full garbage collection released 57.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:15,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:15,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:17,112 - distributed.utils_perf - INFO - full garbage collection released 130.69 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:17,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,083 - distributed.utils_perf - INFO - full garbage collection released 251.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:26,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:29,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,134 - distributed.utils_perf - INFO - full garbage collection released 38.27 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:31,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:32,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:33,337 - distributed.utils_perf - INFO - full garbage collection released 49.53 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:41,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:46,566 - distributed.utils_perf - INFO - full garbage collection released 140.51 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:49,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:54,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:58,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:01,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:05,141 - distributed.utils_perf - INFO - full garbage collection released 100.68 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:07,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:23,876 - distributed.utils_perf - INFO - full garbage collection released 1.21 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:27,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:29,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:31,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,641 - distributed.utils_perf - INFO - full garbage collection released 3.64 GiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:34,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:40,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:56,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,927 - distributed.utils_perf - INFO - full garbage collection released 1.36 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:02,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:09,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:16,358 - distributed.utils_perf - INFO - full garbage collection released 467.92 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:17,034 - distributed.utils_perf - INFO - full garbage collection released 246.99 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:18,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:23,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,933 - distributed.utils_perf - INFO - full garbage collection released 121.16 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:38,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:48,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:53,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,421 - distributed.utils_perf - INFO - full garbage collection released 37.54 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:55,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:08,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:13,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:19,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:27,990 - distributed.utils_perf - INFO - full garbage collection released 55.76 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:28,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,638 - distributed.utils_perf - INFO - full garbage collection released 239.71 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:34,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 41.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:41,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:43,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:49,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:50,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:52,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,251 - distributed.utils_perf - INFO - full garbage collection released 72.34 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:55,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:56,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:07,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:09,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:16,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:21,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:25,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:37,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:43,980 - distributed.utils_perf - INFO - full garbage collection released 1.42 GiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:45,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:45,757 - distributed.utils_perf - INFO - full garbage collection released 702.11 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:46,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:50,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,181 - distributed.utils_perf - INFO - full garbage collection released 0.95 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:58,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:15,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:24,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:25,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:26,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:41,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:46,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 67.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:58,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:00,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:01,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:15,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,969 - distributed.utils_perf - INFO - full garbage collection released 691.65 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:21,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:22,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,591 - distributed.utils_perf - INFO - full garbage collection released 5.41 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:23,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:24,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:30,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:36,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,086 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:23:37,285 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:37,525 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:37,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,815 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:38,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:38,157 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:38,575 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:39,090 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:39,718 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:40,491 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:41,440 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:42,618 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,103 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:23:45,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:45,926 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:46,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,455 - distributed.utils_perf - INFO - full garbage collection released 175.41 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:50,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,741 - distributed.utils_perf - INFO - full garbage collection released 17.74 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:52,832 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:23:53,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,455 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:23:54,227 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:54,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:55,168 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:23:56,340 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 17:23:57,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,834 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 17:23:59,671 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2024-04-19 17:24:01,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,915 - distributed.utils_perf - WARNING - full garbage collections took 32% CPU time recently (threshold: 10%)
2024-04-19 17:24:03,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:13,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,719 - distributed.utils_perf - INFO - full garbage collection released 1.17 GiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:20,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:22,247 - distributed.utils_perf - INFO - full garbage collection released 920.43 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:22,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:37,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:38,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:42,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:48,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:53,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:54,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:01,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:03,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:05,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:06,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:17,182 - distributed.utils_perf - INFO - full garbage collection released 1.82 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:17,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:17,927 - distributed.utils_perf - INFO - full garbage collection released 27.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:21,438 - distributed.utils_perf - INFO - full garbage collection released 16.58 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:38,162 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
[17:26:13] task [xgboost.dask-tcp://10.201.2.198:39173]:tcp://10.201.2.198:39173 got new rank 0
[17:26:13] task [xgboost.dask-tcp://10.201.2.198:40235]:tcp://10.201.2.198:40235 got new rank 1
[17:26:13] task [xgboost.dask-tcp://10.201.2.198:40381]:tcp://10.201.2.198:40381 got new rank 2
[17:26:13] task [xgboost.dask-tcp://10.201.2.198:45751]:tcp://10.201.2.198:45751 got new rank 3
[17:26:13] task [xgboost.dask-tcp://10.201.2.212:36039]:tcp://10.201.2.212:36039 got new rank 4
[17:26:13] task [xgboost.dask-tcp://10.201.2.212:37355]:tcp://10.201.2.212:37355 got new rank 5
[17:26:13] task [xgboost.dask-tcp://10.201.2.212:40013]:tcp://10.201.2.212:40013 got new rank 6
[17:26:13] task [xgboost.dask-tcp://10.201.2.212:43001]:tcp://10.201.2.212:43001 got new rank 7
2024-04-19 17:28:14,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:28:57] task [xgboost.dask-tcp://10.201.2.198:39173]:tcp://10.201.2.198:39173 got new rank 0
[17:28:57] task [xgboost.dask-tcp://10.201.2.198:40235]:tcp://10.201.2.198:40235 got new rank 1
[17:28:57] task [xgboost.dask-tcp://10.201.2.198:40381]:tcp://10.201.2.198:40381 got new rank 2
[17:28:57] task [xgboost.dask-tcp://10.201.2.198:45751]:tcp://10.201.2.198:45751 got new rank 3
[17:28:57] task [xgboost.dask-tcp://10.201.2.212:36039]:tcp://10.201.2.212:36039 got new rank 4
[17:28:57] task [xgboost.dask-tcp://10.201.2.212:37355]:tcp://10.201.2.212:37355 got new rank 5
[17:28:57] task [xgboost.dask-tcp://10.201.2.212:40013]:tcp://10.201.2.212:40013 got new rank 6
[17:28:57] task [xgboost.dask-tcp://10.201.2.212:43001]:tcp://10.201.2.212:43001 got new rank 7
2024-04-19 17:31:30,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:31,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:31,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:31,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:31,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:31:31,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:32:12] task [xgboost.dask-tcp://10.201.2.198:39173]:tcp://10.201.2.198:39173 got new rank 0
[17:32:12] task [xgboost.dask-tcp://10.201.2.198:40235]:tcp://10.201.2.198:40235 got new rank 1
[17:32:12] task [xgboost.dask-tcp://10.201.2.198:40381]:tcp://10.201.2.198:40381 got new rank 2
[17:32:12] task [xgboost.dask-tcp://10.201.2.198:45751]:tcp://10.201.2.198:45751 got new rank 3
[17:32:12] task [xgboost.dask-tcp://10.201.2.212:36039]:tcp://10.201.2.212:36039 got new rank 4
[17:32:12] task [xgboost.dask-tcp://10.201.2.212:37355]:tcp://10.201.2.212:37355 got new rank 5
[17:32:12] task [xgboost.dask-tcp://10.201.2.212:40013]:tcp://10.201.2.212:40013 got new rank 6
[17:32:12] task [xgboost.dask-tcp://10.201.2.212:43001]:tcp://10.201.2.212:43001 got new rank 7
2024-04-19 17:34:43,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:43,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:43,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:43,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:43,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:44,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:01,459 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 17:35:05,005 - distributed.utils_perf - INFO - full garbage collection released 2.77 GiB from 178 reference cycles (threshold: 9.54 MiB)
[17:35:20] task [xgboost.dask-tcp://10.201.2.198:39173]:tcp://10.201.2.198:39173 got new rank 0
[17:35:20] task [xgboost.dask-tcp://10.201.2.198:40235]:tcp://10.201.2.198:40235 got new rank 1
[17:35:20] task [xgboost.dask-tcp://10.201.2.198:40381]:tcp://10.201.2.198:40381 got new rank 2
[17:35:20] task [xgboost.dask-tcp://10.201.2.198:45751]:tcp://10.201.2.198:45751 got new rank 3
[17:35:20] task [xgboost.dask-tcp://10.201.2.212:36039]:tcp://10.201.2.212:36039 got new rank 4
[17:35:20] task [xgboost.dask-tcp://10.201.2.212:37355]:tcp://10.201.2.212:37355 got new rank 5
[17:35:20] task [xgboost.dask-tcp://10.201.2.212:40013]:tcp://10.201.2.212:40013 got new rank 6
[17:35:20] task [xgboost.dask-tcp://10.201.2.212:43001]:tcp://10.201.2.212:43001 got new rank 7
2024-04-19 17:37:37,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:51,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:51,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:52,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:52,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:53,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:55,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:56,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:27,074 - distributed.utils_perf - WARNING - full garbage collections took 23% CPU time recently (threshold: 10%)
[17:38:38] task [xgboost.dask-tcp://10.201.2.198:39173]:tcp://10.201.2.198:39173 got new rank 0
[17:38:38] task [xgboost.dask-tcp://10.201.2.198:40235]:tcp://10.201.2.198:40235 got new rank 1
[17:38:38] task [xgboost.dask-tcp://10.201.2.198:40381]:tcp://10.201.2.198:40381 got new rank 2
[17:38:38] task [xgboost.dask-tcp://10.201.2.198:45751]:tcp://10.201.2.198:45751 got new rank 3
[17:38:38] task [xgboost.dask-tcp://10.201.2.212:36039]:tcp://10.201.2.212:36039 got new rank 4
[17:38:38] task [xgboost.dask-tcp://10.201.2.212:37355]:tcp://10.201.2.212:37355 got new rank 5
[17:38:38] task [xgboost.dask-tcp://10.201.2.212:40013]:tcp://10.201.2.212:40013 got new rank 6
[17:38:38] task [xgboost.dask-tcp://10.201.2.212:43001]:tcp://10.201.2.212:43001 got new rank 7
2024-04-19 17:40:49,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.212:43001. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.198:40381. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.212:40013. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.198:40235. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.212:37355. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.198:39173. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.212:36039. Reason: scheduler-close
2024-04-19 17:41:21,358 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.198:45751. Reason: scheduler-close
2024-04-19 17:41:21,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.212:34765'. Reason: scheduler-close
2024-04-19 17:41:21,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.212:34851'. Reason: scheduler-close
2024-04-19 17:41:21,361 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.198:39007'. Reason: scheduler-close
2024-04-19 17:41:21,360 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44350 remote=tcp://10.201.2.206:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44350 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:21,360 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44340 remote=tcp://10.201.2.206:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44340 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:21,360 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44366 remote=tcp://10.201.2.206:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.198:44366 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:21,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.198:34227'. Reason: scheduler-close
2024-04-19 17:41:21,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.198:46145'. Reason: scheduler-close
2024-04-19 17:41:21,367 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.198:36363'. Reason: scheduler-close
2024-04-19 17:41:21,359 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.212:48168 remote=tcp://10.201.2.206:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.212:48168 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:21,359 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.212:48144 remote=tcp://10.201.2.206:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.212:48144 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:21,369 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.212:34411'. Reason: scheduler-close
2024-04-19 17:41:21,369 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.212:33585'. Reason: scheduler-close
2024-04-19 17:41:22,153 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:22,155 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.198:44382 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:22,154 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:22,155 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.198:59248 remote=tcp://10.201.2.206:8786>: Stream is closed
/10.201.2.198:44374 remote=tcp://10.201.2.206:8786>: Stream is closed
/10.201.2.198:59250 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:22,154 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.212:50428 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:22,154 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:22,155 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:41:22,155 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.212:36454 remote=tcp://10.201.2.206:8786>: Stream is closed
/10.201.2.212:50464 remote=tcp://10.201.2.206:8786>: Stream is closed
/10.201.2.212:36468 remote=tcp://10.201.2.206:8786>: Stream is closed
2024-04-19 17:41:22,196 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,196 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,253 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,253 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,253 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,253 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,265 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,265 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,271 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,271 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,273 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,273 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,273 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,274 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:22,294 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.206:8786; closing.
2024-04-19 17:41:22,294 - distributed.nanny - INFO - Worker closed
2024-04-19 17:41:24,199 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:24,255 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:24,267 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:24,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:24,275 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:24,295 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:41:25,443 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.198:36363'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,444 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,451 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.198:39007'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,451 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.198:46145'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,458 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.212:33585'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,507 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.212:34851'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,515 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.212:34411'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,522 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.198:34227'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,902 - distributed.dask_worker - INFO - End worker
2024-04-19 17:41:25,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.212:34765'. Reason: nanny-close-gracefully
2024-04-19 17:41:25,989 - distributed.dask_worker - INFO - End worker
