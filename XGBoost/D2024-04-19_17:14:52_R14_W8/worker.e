2024-04-19 17:15:30,162 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,162 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,163 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,163 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,215 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:30,436 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,436 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,436 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,437 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,465 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,465 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,465 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,465 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.15:36567'
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.15:33631'
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.15:33023'
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.15:39705'
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.13:44295'
2024-04-19 17:15:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.13:37557'
2024-04-19 17:15:30,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.13:44587'
2024-04-19 17:15:30,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.13:45957'
2024-04-19 17:15:31,484 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,484 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,484 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,484 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,485 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,485 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,485 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,486 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,494 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,494 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,494 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,494 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:15:31,495 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,495 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,495 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,495 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:15:31,529 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,530 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,530 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,531 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,539 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,540 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,540 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:31,540 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:15:32,486 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,486 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,486 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,486 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,493 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,493 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,493 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:32,493 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:15:33,583 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-08138256-2ac9-4a1b-b54d-60fb5c4a6ff2
2024-04-19 17:15:33,583 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.13:39463
2024-04-19 17:15:33,583 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.13:39463
2024-04-19 17:15:33,583 - distributed.worker - INFO -          dashboard at:          10.201.1.13:41337
2024-04-19 17:15:33,583 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,583 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,583 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,583 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tekahdv6
2024-04-19 17:15:33,583 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,585 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f9a1f241-fcb5-491c-805d-f185e39cc470
2024-04-19 17:15:33,585 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1f55bc80-0949-4f47-9901-311567cea346
2024-04-19 17:15:33,585 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.15:35085
2024-04-19 17:15:33,585 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.15:41735
2024-04-19 17:15:33,585 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.15:41735
2024-04-19 17:15:33,585 - distributed.worker - INFO -          dashboard at:          10.201.1.15:36521
2024-04-19 17:15:33,585 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,585 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.15:35085
2024-04-19 17:15:33,585 - distributed.worker - INFO -          dashboard at:          10.201.1.15:42561
2024-04-19 17:15:33,585 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,585 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,585 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,585 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,585 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-055vz24g
2024-04-19 17:15:33,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0zh261oy
2024-04-19 17:15:33,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-833c859d-bdf4-44fd-af09-4e72ef697303
2024-04-19 17:15:33,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-c4bcd1d2-721b-49eb-8553-9cbc285a5367
2024-04-19 17:15:33,586 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.13:42505
2024-04-19 17:15:33,586 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.13:42505
2024-04-19 17:15:33,587 - distributed.worker - INFO -          dashboard at:          10.201.1.13:39439
2024-04-19 17:15:33,587 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,587 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.13:33919
2024-04-19 17:15:33,587 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.13:33919
2024-04-19 17:15:33,587 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,587 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cntsz30c
2024-04-19 17:15:33,587 - distributed.worker - INFO -          dashboard at:          10.201.1.13:33519
2024-04-19 17:15:33,587 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,587 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,587 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_wzwrr1m
2024-04-19 17:15:33,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,590 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e0687294-4f45-49a2-a08c-303f8869250d
2024-04-19 17:15:33,591 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.15:42973
2024-04-19 17:15:33,591 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.15:42973
2024-04-19 17:15:33,591 - distributed.worker - INFO -          dashboard at:          10.201.1.15:46031
2024-04-19 17:15:33,591 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,591 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,591 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,591 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aoop2aof
2024-04-19 17:15:33,591 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,592 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0ea5fe62-8c53-4a60-b303-47bb97e0c8c7
2024-04-19 17:15:33,592 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.13:36123
2024-04-19 17:15:33,592 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.13:36123
2024-04-19 17:15:33,592 - distributed.worker - INFO -          dashboard at:          10.201.1.13:33187
2024-04-19 17:15:33,592 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,592 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,592 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,592 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vrhbh2ya
2024-04-19 17:15:33,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,594 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f0812d81-4b16-4e68-8dd8-dbac0ea8b805
2024-04-19 17:15:33,594 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.15:45227
2024-04-19 17:15:33,594 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.15:45227
2024-04-19 17:15:33,594 - distributed.worker - INFO -          dashboard at:          10.201.1.15:32863
2024-04-19 17:15:33,595 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.18:8786
2024-04-19 17:15:33,595 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:33,595 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:15:33,595 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:15:33,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g0bw5q6v
2024-04-19 17:15:33,595 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,389 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,389 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,390 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,391 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,391 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,392 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,394 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,394 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,395 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,396 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,396 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,397 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,397 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,397 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,397 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,398 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,398 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,399 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,399 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,399 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,400 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:15:37,400 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:37,400 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.18:8786
2024-04-19 17:15:37,401 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:15:37,401 - distributed.core - INFO - Starting established connection to tcp://10.201.1.18:8786
2024-04-19 17:15:46,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:46,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:49,709 - distributed.utils_perf - INFO - full garbage collection released 103.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:49,972 - distributed.utils_perf - INFO - full garbage collection released 426.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:50,091 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:50,146 - distributed.utils_perf - INFO - full garbage collection released 203.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:52,622 - distributed.utils_perf - INFO - full garbage collection released 854.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:53,288 - distributed.utils_perf - INFO - full garbage collection released 2.00 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:53,584 - distributed.utils_perf - INFO - full garbage collection released 876.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:54,363 - distributed.utils_perf - INFO - full garbage collection released 760.26 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:55,742 - distributed.utils_perf - INFO - full garbage collection released 1.63 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:15:57,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:57,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:15:58,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:00,407 - distributed.utils_perf - INFO - full garbage collection released 4.30 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:00,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:00,876 - distributed.utils_perf - INFO - full garbage collection released 2.96 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:03,289 - distributed.utils_perf - INFO - full garbage collection released 338.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:03,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:03,721 - distributed.utils_perf - INFO - full garbage collection released 1.77 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:06,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:07,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:07,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:09,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:11,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:11,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:12,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:13,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:14,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:14,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:20,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:21,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:22,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:24,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:25,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:25,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:28,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:32,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:34,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:37,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:38,190 - distributed.utils_perf - INFO - full garbage collection released 159.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:39,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,253 - distributed.utils_perf - INFO - full garbage collection released 297.67 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,499 - distributed.utils_perf - INFO - full garbage collection released 100.34 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:56,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,243 - distributed.utils_perf - INFO - full garbage collection released 128.97 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:59,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:01,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:09,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:10,146 - distributed.utils_perf - INFO - full garbage collection released 277.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:11,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:13,072 - distributed.utils_perf - INFO - full garbage collection released 71.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:14,219 - distributed.utils_perf - INFO - full garbage collection released 1.28 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:14,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:16,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:17,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:18,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:20,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:23,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:26,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:34,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:36,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:37,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:43,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:47,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:48,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:49,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:00,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,502 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:04,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:08,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:15,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:17,416 - distributed.utils_perf - INFO - full garbage collection released 2.21 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:18,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:19,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:26,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:32,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:33,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:34,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:37,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:38,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:41,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:50,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:53,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:55,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:56,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:58,454 - distributed.utils_perf - INFO - full garbage collection released 634.51 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:59,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:06,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:10,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:11,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:19,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:25,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:30,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:32,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:37,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:37,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:43,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:47,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:50,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:51,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 39.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:56,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:58,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:59,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:03,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:04,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:09,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:10,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:18,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:24,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:25,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:28,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:29,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:34,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:36,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:39,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,629 - distributed.utils_perf - INFO - full garbage collection released 2.39 GiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:58,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:59,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:02,746 - distributed.utils_perf - INFO - full garbage collection released 313.35 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:04,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:04,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:08,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 44.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:13,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:17,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:34,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:35,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:38,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,471 - distributed.utils_perf - INFO - full garbage collection released 0.97 GiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:42,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,419 - distributed.utils_perf - INFO - full garbage collection released 385.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:44,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:46,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:51,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:54,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:59,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:01,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:02,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:03,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:03,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:10,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:11,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:12,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:16,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:17,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:18,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:27,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:29,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:30,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:31,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:35,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:38,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:42,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:47,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:49,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:52,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:59,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:04,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:05,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:06,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:09,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:10,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:14,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,107 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:18,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:18,816 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:23:20,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,913 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:23:21,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:23,512 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:23:24,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,670 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:23:27,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:34,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:47,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:50,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:59,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:00,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:04,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:05,055 - distributed.utils_perf - INFO - full garbage collection released 35.43 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:06,050 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,369 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,757 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:07,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,229 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:07,801 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:08,502 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:09,367 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:10,431 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:11,771 - distributed.utils_perf - WARNING - full garbage collections took 55% CPU time recently (threshold: 10%)
2024-04-19 17:24:13,452 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:15,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,496 - distributed.utils_perf - WARNING - full garbage collections took 54% CPU time recently (threshold: 10%)
2024-04-19 17:24:16,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,039 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 17:24:23,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,304 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:27,043 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:27,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:27,937 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:24:28,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:29,052 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)
2024-04-19 17:24:29,543 - distributed.utils_perf - INFO - full garbage collection released 1.37 GiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:30,460 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:24:32,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:32,197 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:24:35,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:45,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:45,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:12,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:28,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:32,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:44,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:49,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:57,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:29] task [xgboost.dask-tcp://10.201.1.13:33919]:tcp://10.201.1.13:33919 got new rank 0
[17:26:29] task [xgboost.dask-tcp://10.201.1.13:36123]:tcp://10.201.1.13:36123 got new rank 1
[17:26:29] task [xgboost.dask-tcp://10.201.1.13:39463]:tcp://10.201.1.13:39463 got new rank 2
[17:26:30] task [xgboost.dask-tcp://10.201.1.13:42505]:tcp://10.201.1.13:42505 got new rank 3
[17:26:30] task [xgboost.dask-tcp://10.201.1.15:35085]:tcp://10.201.1.15:35085 got new rank 4
[17:26:30] task [xgboost.dask-tcp://10.201.1.15:41735]:tcp://10.201.1.15:41735 got new rank 5
[17:26:30] task [xgboost.dask-tcp://10.201.1.15:42973]:tcp://10.201.1.15:42973 got new rank 6
[17:26:30] task [xgboost.dask-tcp://10.201.1.15:45227]:tcp://10.201.1.15:45227 got new rank 7
2024-04-19 17:28:35,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:29:15] task [xgboost.dask-tcp://10.201.1.13:33919]:tcp://10.201.1.13:33919 got new rank 0
[17:29:15] task [xgboost.dask-tcp://10.201.1.13:36123]:tcp://10.201.1.13:36123 got new rank 1
[17:29:15] task [xgboost.dask-tcp://10.201.1.13:39463]:tcp://10.201.1.13:39463 got new rank 2
[17:29:15] task [xgboost.dask-tcp://10.201.1.13:42505]:tcp://10.201.1.13:42505 got new rank 3
[17:29:15] task [xgboost.dask-tcp://10.201.1.15:35085]:tcp://10.201.1.15:35085 got new rank 4
[17:29:15] task [xgboost.dask-tcp://10.201.1.15:41735]:tcp://10.201.1.15:41735 got new rank 5
[17:29:15] task [xgboost.dask-tcp://10.201.1.15:42973]:tcp://10.201.1.15:42973 got new rank 6
[17:29:15] task [xgboost.dask-tcp://10.201.1.15:45227]:tcp://10.201.1.15:45227 got new rank 7
2024-04-19 17:32:18,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:18,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:32:18,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:32:53] task [xgboost.dask-tcp://10.201.1.13:33919]:tcp://10.201.1.13:33919 got new rank 0
[17:32:53] task [xgboost.dask-tcp://10.201.1.13:36123]:tcp://10.201.1.13:36123 got new rank 1
[17:32:53] task [xgboost.dask-tcp://10.201.1.13:39463]:tcp://10.201.1.13:39463 got new rank 2
[17:32:53] task [xgboost.dask-tcp://10.201.1.13:42505]:tcp://10.201.1.13:42505 got new rank 3
[17:32:53] task [xgboost.dask-tcp://10.201.1.15:35085]:tcp://10.201.1.15:35085 got new rank 4
[17:32:53] task [xgboost.dask-tcp://10.201.1.15:41735]:tcp://10.201.1.15:41735 got new rank 5
[17:32:53] task [xgboost.dask-tcp://10.201.1.15:42973]:tcp://10.201.1.15:42973 got new rank 6
[17:32:53] task [xgboost.dask-tcp://10.201.1.15:45227]:tcp://10.201.1.15:45227 got new rank 7
2024-04-19 17:35:16,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:17,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:17,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:35:17,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:36:01] task [xgboost.dask-tcp://10.201.1.13:33919]:tcp://10.201.1.13:33919 got new rank 0
[17:36:01] task [xgboost.dask-tcp://10.201.1.13:36123]:tcp://10.201.1.13:36123 got new rank 1
[17:36:01] task [xgboost.dask-tcp://10.201.1.13:39463]:tcp://10.201.1.13:39463 got new rank 2
[17:36:01] task [xgboost.dask-tcp://10.201.1.13:42505]:tcp://10.201.1.13:42505 got new rank 3
[17:36:01] task [xgboost.dask-tcp://10.201.1.15:35085]:tcp://10.201.1.15:35085 got new rank 4
[17:36:01] task [xgboost.dask-tcp://10.201.1.15:41735]:tcp://10.201.1.15:41735 got new rank 5
[17:36:01] task [xgboost.dask-tcp://10.201.1.15:42973]:tcp://10.201.1.15:42973 got new rank 6
[17:36:01] task [xgboost.dask-tcp://10.201.1.15:45227]:tcp://10.201.1.15:45227 got new rank 7
2024-04-19 17:38:31,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:31,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:32,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:45,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:45,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:45,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:45,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:46,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:46,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:47,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:38:48,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:39:29] task [xgboost.dask-tcp://10.201.1.13:33919]:tcp://10.201.1.13:33919 got new rank 0
[17:39:29] task [xgboost.dask-tcp://10.201.1.13:36123]:tcp://10.201.1.13:36123 got new rank 1
[17:39:29] task [xgboost.dask-tcp://10.201.1.13:39463]:tcp://10.201.1.13:39463 got new rank 2
[17:39:29] task [xgboost.dask-tcp://10.201.1.13:42505]:tcp://10.201.1.13:42505 got new rank 3
[17:39:29] task [xgboost.dask-tcp://10.201.1.15:35085]:tcp://10.201.1.15:35085 got new rank 4
[17:39:29] task [xgboost.dask-tcp://10.201.1.15:41735]:tcp://10.201.1.15:41735 got new rank 5
[17:39:29] task [xgboost.dask-tcp://10.201.1.15:42973]:tcp://10.201.1.15:42973 got new rank 6
[17:39:29] task [xgboost.dask-tcp://10.201.1.15:45227]:tcp://10.201.1.15:45227 got new rank 7
2024-04-19 17:41:32,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:03,167 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.15:45227. Reason: scheduler-close
2024-04-19 17:42:03,167 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.15:35085. Reason: scheduler-close
2024-04-19 17:42:03,167 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.15:41735. Reason: scheduler-close
2024-04-19 17:42:03,167 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.15:42973. Reason: scheduler-close
2024-04-19 17:42:03,168 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.13:42505. Reason: scheduler-close
2024-04-19 17:42:03,168 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.13:39463. Reason: scheduler-close
2024-04-19 17:42:03,168 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.13:36123. Reason: scheduler-close
2024-04-19 17:42:03,168 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.13:33919. Reason: scheduler-close
2024-04-19 17:42:03,169 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.15:33023'. Reason: scheduler-close
2024-04-19 17:42:03,170 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.15:33631'. Reason: scheduler-close
2024-04-19 17:42:03,170 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.15:36567'. Reason: scheduler-close
2024-04-19 17:42:03,170 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48370 remote=tcp://10.201.1.18:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48370 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,170 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48364 remote=tcp://10.201.1.18:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48364 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,170 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48344 remote=tcp://10.201.1.18:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48344 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,170 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48352 remote=tcp://10.201.1.18:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.13:48352 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.13:45957'. Reason: scheduler-close
2024-04-19 17:42:03,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.13:44587'. Reason: scheduler-close
2024-04-19 17:42:03,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.13:44295'. Reason: scheduler-close
2024-04-19 17:42:03,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.13:37557'. Reason: scheduler-close
2024-04-19 17:42:03,168 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.15:44758 remote=tcp://10.201.1.18:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.15:44758 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,181 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.15:39705'. Reason: scheduler-close
2024-04-19 17:42:03,968 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:42:03,969 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.13:59820 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,969 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.13:59844 remote=tcp://10.201.1.18:8786>: Stream is closed
/10.201.1.13:59804 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,969 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.13:59834 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,970 - distributed.comm.tcp - INFO - Connection from tcp://10.201.1.15:51502 closed before handshake completed
2024-04-19 17:42:03,969 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.15:44786 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,970 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:42:03,970 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.15:44796 remote=tcp://10.201.1.18:8786>: Stream is closed
/10.201.1.15:44992 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:03,970 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.15:36352 remote=tcp://10.201.1.18:8786>: Stream is closed
2024-04-19 17:42:04,033 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,033 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,066 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,066 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,076 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,076 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,078 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,078 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,079 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,079 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,094 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,095 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,104 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,104 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:04,115 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.18:8786; closing.
2024-04-19 17:42:04,115 - distributed.nanny - INFO - Worker closed
2024-04-19 17:42:06,036 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,069 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,078 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,079 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,081 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,096 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,107 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:06,116 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:42:07,257 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.13:37557'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,257 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.13:44295'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,268 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,273 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.13:44587'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,274 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.15:36567'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,357 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.15:33023'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,366 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.15:33631'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,372 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.13:45957'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,721 - distributed.dask_worker - INFO - End worker
2024-04-19 17:42:07,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.15:39705'. Reason: nanny-close-gracefully
2024-04-19 17:42:07,832 - distributed.dask_worker - INFO - End worker
