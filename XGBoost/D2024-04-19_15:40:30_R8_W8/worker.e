2024-04-19 15:41:05,186 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,186 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,187 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,187 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,373 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,373 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,373 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,374 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:05,559 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,559 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,559 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,559 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,680 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,681 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:05,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:38185'
2024-04-19 15:41:05,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:34739'
2024-04-19 15:41:05,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:42607'
2024-04-19 15:41:05,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.91:37823'
2024-04-19 15:41:05,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:40035'
2024-04-19 15:41:05,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:43135'
2024-04-19 15:41:05,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:37967'
2024-04-19 15:41:05,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.105:45381'
2024-04-19 15:41:06,664 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,664 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,664 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,664 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,672 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,672 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,672 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,672 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:41:06,673 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,673 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,673 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,673 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:41:06,710 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,710 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,710 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,710 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,718 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,719 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,719 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:06,719 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:41:07,757 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,757 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,757 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,757 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,761 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,761 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,761 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:07,761 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:41:08,845 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a650b28d-1091-4c10-a547-2f3cc8a3710d
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:45797
2024-04-19 15:41:08,845 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-347c2a5d-bdc5-4e2a-b41c-b27225a7bc59
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:36061
2024-04-19 15:41:08,845 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-744ea450-b6eb-4e0b-b81e-3659067e7da5
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:44005
2024-04-19 15:41:08,845 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:44005
2024-04-19 15:41:08,845 - distributed.worker - INFO -          dashboard at:          10.201.2.91:39373
2024-04-19 15:41:08,845 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,845 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d4a3dced-5922-43ce-8173-e33620e12b1b
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.91:40929
2024-04-19 15:41:08,845 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:40929
2024-04-19 15:41:08,845 - distributed.worker - INFO -          dashboard at:          10.201.2.91:46409
2024-04-19 15:41:08,845 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:45797
2024-04-19 15:41:08,845 - distributed.worker - INFO -          dashboard at:          10.201.2.91:33651
2024-04-19 15:41:08,845 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,845 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o72dslkb
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.91:36061
2024-04-19 15:41:08,845 - distributed.worker - INFO -          dashboard at:          10.201.2.91:39519
2024-04-19 15:41:08,845 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,845 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aye1_z9f
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,845 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llupxxi7
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,845 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,845 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o9h__dn1
2024-04-19 15:41:08,845 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,925 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-5dd0d8ff-e192-4313-80a0-9ebedb66f8fa
2024-04-19 15:41:08,925 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a92f5eb5-20cf-4eca-a363-4840a676c312
2024-04-19 15:41:08,925 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:43533
2024-04-19 15:41:08,925 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e0c6abb6-a19a-4a1b-8389-ecc86dc46216
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:43711
2024-04-19 15:41:08,926 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:43711
2024-04-19 15:41:08,926 - distributed.worker - INFO -          dashboard at:         10.201.1.105:36853
2024-04-19 15:41:08,925 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:42317
2024-04-19 15:41:08,925 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:42317
2024-04-19 15:41:08,926 - distributed.worker - INFO -          dashboard at:         10.201.1.105:34099
2024-04-19 15:41:08,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,926 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,925 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d8855eba-c9db-42b5-88e3-253b1270652e
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.105:41921
2024-04-19 15:41:08,926 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:43533
2024-04-19 15:41:08,926 - distributed.worker - INFO -          dashboard at:         10.201.1.105:35637
2024-04-19 15:41:08,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,926 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_77ly1eb
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,926 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-397a5ooy
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dt9n2uv7
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.105:41921
2024-04-19 15:41:08,926 - distributed.worker - INFO -          dashboard at:         10.201.1.105:37583
2024-04-19 15:41:08,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.1.90:8786
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:08,926 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:41:08,926 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:41:08,926 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbhppjil
2024-04-19 15:41:08,926 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,702 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,702 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,702 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,705 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,706 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,706 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,708 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,708 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,709 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,709 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,709 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,710 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,710 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,711 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,711 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,712 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,713 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,713 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,713 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,713 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,713 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:41:12,714 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:12,714 - distributed.worker - INFO -         Registered to:     tcp://10.201.1.90:8786
2024-04-19 15:41:12,714 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:41:12,715 - distributed.core - INFO - Starting established connection to tcp://10.201.1.90:8786
2024-04-19 15:41:21,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:21,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,526 - distributed.utils_perf - INFO - full garbage collection released 404.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:26,006 - distributed.utils_perf - INFO - full garbage collection released 1.46 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,095 - distributed.utils_perf - INFO - full garbage collection released 69.09 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,208 - distributed.utils_perf - INFO - full garbage collection released 268.11 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:28,935 - distributed.utils_perf - INFO - full garbage collection released 792.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,061 - distributed.utils_perf - INFO - full garbage collection released 488.55 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:29,156 - distributed.utils_perf - INFO - full garbage collection released 1.07 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:33,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,232 - distributed.utils_perf - INFO - full garbage collection released 810.38 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:33,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:33,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:34,899 - distributed.utils_perf - INFO - full garbage collection released 2.89 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:35,443 - distributed.utils_perf - INFO - full garbage collection released 3.08 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:36,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,659 - distributed.utils_perf - INFO - full garbage collection released 3.92 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:38,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:38,690 - distributed.utils_perf - INFO - full garbage collection released 2.90 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:42,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:45,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:46,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:47,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:48,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:48,901 - distributed.utils_perf - INFO - full garbage collection released 1.81 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:49,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,946 - distributed.utils_perf - INFO - full garbage collection released 170.80 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:51,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:51,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:53,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:54,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,148 - distributed.utils_perf - INFO - full garbage collection released 144.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:55,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:58,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:03,469 - distributed.utils_perf - INFO - full garbage collection released 226.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:03,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:04,180 - distributed.utils_perf - INFO - full garbage collection released 252.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:05,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:05,750 - distributed.utils_perf - INFO - full garbage collection released 169.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:09,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:11,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:13,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:15,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:16,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:19,856 - distributed.utils_perf - INFO - full garbage collection released 43.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:25,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:25,790 - distributed.utils_perf - INFO - full garbage collection released 861.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:26,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:26,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:27,081 - distributed.utils_perf - INFO - full garbage collection released 18.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:27,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:32,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:35,391 - distributed.utils_perf - INFO - full garbage collection released 252.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:38,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:41,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:41,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:42,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:48,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:50,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:54,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:55,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:59,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:00,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:00,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:01,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:04,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:07,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:10,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:11,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:15,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:18,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:21,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:23,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:25,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:26,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:28,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:33,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:34,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:35,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:37,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:40,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:47,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:47,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:50,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:52,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:55,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:56,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:56,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:57,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:59,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:08,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:10,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:10,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:11,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:11,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:13,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:16,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:20,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:21,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:23,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:25,877 - distributed.utils_perf - INFO - full garbage collection released 900.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:26,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:27,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:28,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:32,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:36,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:36,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:38,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:39,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:42,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:43,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:48,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:50,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:50,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:52,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:55,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:59,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:03,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:04,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:11,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:14,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:15,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:18,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:19,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:21,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:23,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:24,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:26,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:30,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:31,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:31,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:32,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:36,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:37,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:39,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:39,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:42,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:42,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:45,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:46,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:47,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:47,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:48,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:53,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:57,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:58,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:02,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:05,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:06,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:07,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:10,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:14,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:14,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:15,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:19,437 - distributed.utils_perf - INFO - full garbage collection released 707.31 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:20,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:21,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:26,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:31,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:35,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:37,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:40,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:41,989 - distributed.utils_perf - INFO - full garbage collection released 750.11 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:42,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:45,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:45,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:46,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:51,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:52,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:52,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:53,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:54,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:58,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:06,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:07,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:09,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:10,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:14,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:17,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:19,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:20,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:22,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:25,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:26,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:27,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:28,728 - distributed.utils_perf - INFO - full garbage collection released 129.54 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:47:31,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:32,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:33,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:37,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,027 - distributed.utils_perf - INFO - full garbage collection released 801.21 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:47:38,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:40,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:41,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:41,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:50,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:53,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:53,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:54,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:55,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:58,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:00,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:00,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:02,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:03,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:06,423 - distributed.utils_perf - INFO - full garbage collection released 1.08 GiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:10,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:13,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:15,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:15,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:19,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:20,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:21,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:22,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:23,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,742 - distributed.utils_perf - INFO - full garbage collection released 31.48 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:28,020 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:28,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:28,367 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:28,790 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 15:48:29,303 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 15:48:29,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:29,933 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 15:48:30,703 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)
2024-04-19 15:48:31,645 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:31,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:32,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:32,804 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:34,289 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:34,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:35,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:36,118 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 15:48:38,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:38,385 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 15:48:39,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:39,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:40,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:41,603 - distributed.utils_perf - INFO - full garbage collection released 1.07 GiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:48:42,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:43,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:44,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:45,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:45,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:47,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:47,518 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 15:48:47,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:48,031 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 15:48:48,655 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:48:49,418 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:48:50,370 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:48:51,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:51,527 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)
2024-04-19 15:48:51,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:52,984 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 15:48:54,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:54,803 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:48:57,042 - distributed.utils_perf - WARNING - full garbage collections took 31% CPU time recently (threshold: 10%)
2024-04-19 15:48:57,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:58,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:59,797 - distributed.utils_perf - WARNING - full garbage collections took 28% CPU time recently (threshold: 10%)
2024-04-19 15:49:03,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:04,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:05,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:06,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:07,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:08,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:14,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:15,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:15,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:16,364 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:49:21,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:21,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:23,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:23,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:25,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:27,758 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:28,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:28,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:28,446 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:28,827 - distributed.utils_perf - INFO - full garbage collection released 19.64 MiB from 75 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:29,305 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 15:49:30,365 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:31,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:31,694 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:32,098 - distributed.utils_perf - INFO - full garbage collection released 779.24 MiB from 56 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:49:33,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:33,364 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:35,407 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:37,088 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 15:49:37,932 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:37,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,047 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:42,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:42,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:43,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:44,907 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 15:49:45,900 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:46,504 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:47,244 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:48,149 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:49,267 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:49,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:50,659 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:51,900 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:52,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:52,407 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:52,576 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:52,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:53,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:53,413 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:49:54,434 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:49:54,556 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:55,705 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:49:56,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:57,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:57,200 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 15:49:57,336 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:49:57,482 - distributed.utils_perf - WARNING - full garbage collections took 37% CPU time recently (threshold: 10%)
2024-04-19 15:49:58,810 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 15:49:59,325 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 15:50:00,486 - distributed.utils_perf - WARNING - full garbage collections took 35% CPU time recently (threshold: 10%)
2024-04-19 15:50:02,548 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)
2024-04-19 15:50:05,088 - distributed.utils_perf - WARNING - full garbage collections took 32% CPU time recently (threshold: 10%)
2024-04-19 15:50:05,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:06,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:07,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:08,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:10,327 - distributed.utils_perf - INFO - full garbage collection released 36.65 MiB from 114 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:50:14,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:50:59] task [xgboost.dask-tcp://10.201.1.105:41921]:tcp://10.201.1.105:41921 got new rank 0
[15:50:59] task [xgboost.dask-tcp://10.201.1.105:42317]:tcp://10.201.1.105:42317 got new rank 1
[15:50:59] task [xgboost.dask-tcp://10.201.1.105:43533]:tcp://10.201.1.105:43533 got new rank 2
[15:50:59] task [xgboost.dask-tcp://10.201.1.105:43711]:tcp://10.201.1.105:43711 got new rank 3
[15:50:59] task [xgboost.dask-tcp://10.201.2.91:36061]:tcp://10.201.2.91:36061 got new rank 4
[15:50:59] task [xgboost.dask-tcp://10.201.2.91:40929]:tcp://10.201.2.91:40929 got new rank 5
[15:50:59] task [xgboost.dask-tcp://10.201.2.91:44005]:tcp://10.201.2.91:44005 got new rank 6
[15:50:59] task [xgboost.dask-tcp://10.201.2.91:45797]:tcp://10.201.2.91:45797 got new rank 7
2024-04-19 15:53:05,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:06,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:53:06,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:53:46] task [xgboost.dask-tcp://10.201.1.105:41921]:tcp://10.201.1.105:41921 got new rank 0
[15:53:46] task [xgboost.dask-tcp://10.201.1.105:42317]:tcp://10.201.1.105:42317 got new rank 1
[15:53:46] task [xgboost.dask-tcp://10.201.1.105:43533]:tcp://10.201.1.105:43533 got new rank 2
[15:53:46] task [xgboost.dask-tcp://10.201.1.105:43711]:tcp://10.201.1.105:43711 got new rank 3
[15:53:46] task [xgboost.dask-tcp://10.201.2.91:36061]:tcp://10.201.2.91:36061 got new rank 4
[15:53:46] task [xgboost.dask-tcp://10.201.2.91:40929]:tcp://10.201.2.91:40929 got new rank 5
[15:53:46] task [xgboost.dask-tcp://10.201.2.91:44005]:tcp://10.201.2.91:44005 got new rank 6
[15:53:46] task [xgboost.dask-tcp://10.201.2.91:45797]:tcp://10.201.2.91:45797 got new rank 7
2024-04-19 15:56:08,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:56:11,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:56:44] task [xgboost.dask-tcp://10.201.1.105:41921]:tcp://10.201.1.105:41921 got new rank 0
[15:56:44] task [xgboost.dask-tcp://10.201.1.105:42317]:tcp://10.201.1.105:42317 got new rank 1
[15:56:44] task [xgboost.dask-tcp://10.201.1.105:43533]:tcp://10.201.1.105:43533 got new rank 2
[15:56:44] task [xgboost.dask-tcp://10.201.1.105:43711]:tcp://10.201.1.105:43711 got new rank 3
[15:56:44] task [xgboost.dask-tcp://10.201.2.91:36061]:tcp://10.201.2.91:36061 got new rank 4
[15:56:44] task [xgboost.dask-tcp://10.201.2.91:40929]:tcp://10.201.2.91:40929 got new rank 5
[15:56:44] task [xgboost.dask-tcp://10.201.2.91:44005]:tcp://10.201.2.91:44005 got new rank 6
[15:56:44] task [xgboost.dask-tcp://10.201.2.91:45797]:tcp://10.201.2.91:45797 got new rank 7
2024-04-19 15:59:04,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:59:04,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:59:47] task [xgboost.dask-tcp://10.201.1.105:41921]:tcp://10.201.1.105:41921 got new rank 0
[15:59:47] task [xgboost.dask-tcp://10.201.1.105:42317]:tcp://10.201.1.105:42317 got new rank 1
[15:59:47] task [xgboost.dask-tcp://10.201.1.105:43533]:tcp://10.201.1.105:43533 got new rank 2
[15:59:47] task [xgboost.dask-tcp://10.201.1.105:43711]:tcp://10.201.1.105:43711 got new rank 3
[15:59:47] task [xgboost.dask-tcp://10.201.2.91:36061]:tcp://10.201.2.91:36061 got new rank 4
[15:59:47] task [xgboost.dask-tcp://10.201.2.91:40929]:tcp://10.201.2.91:40929 got new rank 5
[15:59:47] task [xgboost.dask-tcp://10.201.2.91:44005]:tcp://10.201.2.91:44005 got new rank 6
[15:59:47] task [xgboost.dask-tcp://10.201.2.91:45797]:tcp://10.201.2.91:45797 got new rank 7
2024-04-19 16:02:37,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:38,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:38,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:50,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:52,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:52,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:52,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:53,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:53,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:53,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:02:53,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:03:35] task [xgboost.dask-tcp://10.201.1.105:41921]:tcp://10.201.1.105:41921 got new rank 0
[16:03:35] task [xgboost.dask-tcp://10.201.1.105:42317]:tcp://10.201.1.105:42317 got new rank 1
[16:03:35] task [xgboost.dask-tcp://10.201.1.105:43533]:tcp://10.201.1.105:43533 got new rank 2
[16:03:35] task [xgboost.dask-tcp://10.201.1.105:43711]:tcp://10.201.1.105:43711 got new rank 3
[16:03:35] task [xgboost.dask-tcp://10.201.2.91:36061]:tcp://10.201.2.91:36061 got new rank 4
[16:03:35] task [xgboost.dask-tcp://10.201.2.91:40929]:tcp://10.201.2.91:40929 got new rank 5
[16:03:35] task [xgboost.dask-tcp://10.201.2.91:44005]:tcp://10.201.2.91:44005 got new rank 6
[16:03:35] task [xgboost.dask-tcp://10.201.2.91:45797]:tcp://10.201.2.91:45797 got new rank 7
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:40929. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:45797. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:36061. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.91:44005. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:42317. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:41921. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:43533. Reason: scheduler-close
2024-04-19 16:06:04,090 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.105:43711. Reason: scheduler-close
2024-04-19 16:06:04,091 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:45381'. Reason: scheduler-close
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45006 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45006 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55176 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55176 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55150 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55150 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55166 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.105:55166 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,098 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:42607'. Reason: scheduler-close
2024-04-19 16:06:04,098 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:37967'. Reason: scheduler-close
2024-04-19 16:06:04,098 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:43135'. Reason: scheduler-close
2024-04-19 16:06:04,098 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.105:40035'. Reason: scheduler-close
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45032 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45032 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:44990 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:44990 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,091 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45020 remote=tcp://10.201.1.90:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.91:45020 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,100 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:38185'. Reason: scheduler-close
2024-04-19 16:06:04,100 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:37823'. Reason: scheduler-close
2024-04-19 16:06:04,100 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.91:34739'. Reason: scheduler-close
2024-04-19 16:06:04,877 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:04,877 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:55196 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.1.105:33130 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,878 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:55192 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,879 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.105:33116 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,884 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.91:36648 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,885 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:04,885 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:06:04,885 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.91:36662 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.2.91:36666 remote=tcp://10.201.1.90:8786>: Stream is closed
/10.201.2.91:36634 remote=tcp://10.201.1.90:8786>: Stream is closed
2024-04-19 16:06:04,899 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,899 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:04,903 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,904 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:04,904 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,904 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:04,919 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,920 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:04,964 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,965 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:04,989 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:04,989 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:05,009 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:05,010 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:05,020 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.1.90:8786; closing.
2024-04-19 16:06:05,020 - distributed.nanny - INFO - Worker closed
2024-04-19 16:06:06,968 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:06,968 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:06,991 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:07,011 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:07,023 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:06:07,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:40035'. Reason: nanny-close-gracefully
2024-04-19 16:06:07,748 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:07,855 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:37967'. Reason: nanny-close-gracefully
2024-04-19 16:06:07,856 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:07,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:45381'. Reason: nanny-close-gracefully
2024-04-19 16:06:07,862 - distributed.dask_worker - INFO - End worker
2024-04-19 16:06:08,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.105:43135'. Reason: nanny-close-gracefully
2024-04-19 16:06:08,225 - distributed.dask_worker - INFO - End worker
