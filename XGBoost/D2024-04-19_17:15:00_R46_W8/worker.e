2024-04-19 17:40:25,202 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,202 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,202 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,203 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,212 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,213 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,213 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,214 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:25,432 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,432 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,433 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,433 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,433 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,432 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,433 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,433 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:40153'
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:38019'
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:35847'
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:36331'
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:46605'
2024-04-19 17:40:25,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.16:34999'
2024-04-19 17:40:25,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:45495'
2024-04-19 17:40:25,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.17:35891'
2024-04-19 17:40:26,508 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,509 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,509 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,509 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,509 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,510 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,510 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,510 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,523 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,524 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,524 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,524 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:40:26,524 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,525 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,525 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,525 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:40:26,554 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,554 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,555 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,555 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,569 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,569 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,569 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:26,569 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:40:27,496 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,496 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,496 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,496 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,520 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,520 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,520 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:27,520 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:40:28,557 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-9137996b-931c-42e4-905b-6f8ae016af64
2024-04-19 17:40:28,557 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3d397975-0b58-47bb-821a-ce5b1211a15c
2024-04-19 17:40:28,557 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:40885
2024-04-19 17:40:28,557 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:40885
2024-04-19 17:40:28,557 - distributed.worker - INFO -          dashboard at:          10.201.2.17:45589
2024-04-19 17:40:28,557 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:34433
2024-04-19 17:40:28,557 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:34433
2024-04-19 17:40:28,557 - distributed.worker - INFO -          dashboard at:          10.201.2.17:45569
2024-04-19 17:40:28,557 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-691bafdc-356d-450b-bc72-135be6d8c81b
2024-04-19 17:40:28,557 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:35649
2024-04-19 17:40:28,557 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:35649
2024-04-19 17:40:28,558 - distributed.worker - INFO -          dashboard at:          10.201.2.17:32979
2024-04-19 17:40:28,558 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,557 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-08a8d23d-df9e-49fc-8a56-c13b0c1d02a0
2024-04-19 17:40:28,557 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.17:43517
2024-04-19 17:40:28,557 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.17:43517
2024-04-19 17:40:28,558 - distributed.worker - INFO -          dashboard at:          10.201.2.17:40949
2024-04-19 17:40:28,558 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,558 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4bsjm_cj
2024-04-19 17:40:28,557 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,557 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,558 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n0pzg6lm
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,558 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lqig7mt7
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,558 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-onsmg8j8
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,558 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a9261d31-4aa7-4fcb-a477-c786a96739b2
2024-04-19 17:40:28,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a552da14-d395-45ae-a0be-c0c4c9584cc7
2024-04-19 17:40:28,586 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:38381
2024-04-19 17:40:28,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a03af096-a817-4dea-9d5d-ad29fbfa94a1
2024-04-19 17:40:28,586 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:42743
2024-04-19 17:40:28,586 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:42743
2024-04-19 17:40:28,586 - distributed.worker - INFO -          dashboard at:          10.201.2.16:39437
2024-04-19 17:40:28,586 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,586 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,586 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,586 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:33657
2024-04-19 17:40:28,586 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:33657
2024-04-19 17:40:28,586 - distributed.worker - INFO -          dashboard at:          10.201.2.16:41361
2024-04-19 17:40:28,586 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,586 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-108f4293-1dcf-4b9d-8a2d-ee681e22d9c3
2024-04-19 17:40:28,586 - distributed.worker - INFO -       Start worker at:    tcp://10.201.2.16:39811
2024-04-19 17:40:28,586 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:39811
2024-04-19 17:40:28,586 - distributed.worker - INFO -          dashboard at:          10.201.2.16:34815
2024-04-19 17:40:28,586 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,586 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,587 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,587 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hoac8nas
2024-04-19 17:40:28,586 - distributed.worker - INFO -          Listening to:    tcp://10.201.2.16:38381
2024-04-19 17:40:28,586 - distributed.worker - INFO -          dashboard at:          10.201.2.16:41989
2024-04-19 17:40:28,586 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.2.39:8786
2024-04-19 17:40:28,586 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,587 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,587 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yochs_yf
2024-04-19 17:40:28,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,586 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ltrn0ctz
2024-04-19 17:40:28,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,587 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:40:28,587 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:40:28,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mhafxuwj
2024-04-19 17:40:28,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:28,587 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,283 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,283 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,284 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,285 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,285 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,285 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,286 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,288 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,288 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,289 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,290 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,290 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,290 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,292 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,292 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,292 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,293 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,293 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,293 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,293 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,294 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:40:32,294 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:32,295 - distributed.worker - INFO -         Registered to:     tcp://10.201.2.39:8786
2024-04-19 17:40:32,295 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:40:32,295 - distributed.core - INFO - Starting established connection to tcp://10.201.2.39:8786
2024-04-19 17:40:41,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:41,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:44,044 - distributed.utils_perf - INFO - full garbage collection released 70.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:45,516 - distributed.utils_perf - INFO - full garbage collection released 10.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:45,537 - distributed.utils_perf - INFO - full garbage collection released 742.57 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:47,902 - distributed.utils_perf - INFO - full garbage collection released 76.74 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:48,535 - distributed.utils_perf - INFO - full garbage collection released 1.03 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:48,717 - distributed.utils_perf - INFO - full garbage collection released 105.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:51,082 - distributed.utils_perf - INFO - full garbage collection released 3.17 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:52,509 - distributed.utils_perf - INFO - full garbage collection released 2.61 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:52,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:52,615 - distributed.utils_perf - INFO - full garbage collection released 3.01 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:53,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:54,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:55,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:55,622 - distributed.utils_perf - INFO - full garbage collection released 3.59 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:57,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:57,733 - distributed.utils_perf - INFO - full garbage collection released 7.05 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:40:58,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:40:58,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:03,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:04,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:05,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:06,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:09,617 - distributed.utils_perf - INFO - full garbage collection released 409.40 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:09,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:11,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:12,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:12,337 - distributed.utils_perf - INFO - full garbage collection released 64.17 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:13,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:14,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:15,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:16,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:19,064 - distributed.utils_perf - INFO - full garbage collection released 113.00 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:19,442 - distributed.utils_perf - INFO - full garbage collection released 37.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:20,648 - distributed.utils_perf - INFO - full garbage collection released 76.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:20,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:20,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:22,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:26,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:28,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:28,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:29,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:30,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:30,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:30,520 - distributed.utils_perf - INFO - full garbage collection released 3.87 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:35,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:39,293 - distributed.utils_perf - INFO - full garbage collection released 122.74 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:44,165 - distributed.utils_perf - INFO - full garbage collection released 101.24 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:44,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:44,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:45,804 - distributed.utils_perf - INFO - full garbage collection released 145.67 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:41:45,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:48,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:49,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:52,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:52,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:53,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:41:55,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:00,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:03,925 - distributed.utils_perf - INFO - full garbage collection released 120.87 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:05,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:07,080 - distributed.utils_perf - INFO - full garbage collection released 207.56 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:08,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:09,055 - distributed.utils_perf - INFO - full garbage collection released 106.37 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:42:09,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:11,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:12,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:12,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:18,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:19,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:19,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:21,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:22,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:23,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:25,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:28,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:29,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:29,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:31,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:32,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:33,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:33,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:34,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:35,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:36,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:37,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:38,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:41,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:44,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:45,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:46,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:47,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:49,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:51,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:54,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:55,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:57,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:42:59,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:02,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:02,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:07,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:08,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:09,337 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:10,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:10,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:12,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:13,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:18,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:19,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:22,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:23,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:24,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:25,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:27,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:28,078 - distributed.utils_perf - INFO - full garbage collection released 1.91 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:43:31,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:37,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:43,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:43,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:43,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:44,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:46,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:47,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:48,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:48,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:48,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:49,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:51,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:51,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:51,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:57,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:59,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:43:59,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:00,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:01,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:10,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:11,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:15,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:17,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:19,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:20,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:22,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:22,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:25,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:26,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:26,892 - distributed.utils_perf - INFO - full garbage collection released 283.08 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:28,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:28,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 36.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:29,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:34,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:35,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:36,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:36,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:37,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:37,976 - distributed.utils_perf - INFO - full garbage collection released 80.67 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:38,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:44,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:46,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:46,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:48,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:49,990 - distributed.utils_perf - INFO - full garbage collection released 492.11 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:44:50,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:51,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:51,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:44:53,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:00,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:00,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:01,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:03,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:03,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:07,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:10,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:15,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:15,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:17,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:17,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:19,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:26,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:29,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:29,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:30,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:32,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:33,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:36,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:38,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:39,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:40,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:44,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:45,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:45,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:48,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:49,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:49,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:50,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:51,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:53,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:45:57,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:00,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:06,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:06,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:08,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:14,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:15,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:21,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:22,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:23,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:23,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:29,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:30,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:32,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:33,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:34,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:34,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:36,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:38,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:40,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:41,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:44,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:45,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:47,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:47,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:50,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:52,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:52,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:52,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:55,654 - distributed.utils_perf - INFO - full garbage collection released 699.18 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:46:56,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:57,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:46:57,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:00,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:01,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:02,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:02,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:06,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:06,688 - distributed.utils_perf - INFO - full garbage collection released 160.95 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:07,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:09,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:12,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:13,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:14,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:15,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:16,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:18,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:18,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:19,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:19,948 - distributed.utils_perf - INFO - full garbage collection released 1.10 GiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:20,414 - distributed.utils_perf - INFO - full garbage collection released 0.93 GiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:20,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:22,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:23,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:24,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:24,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:26,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:27,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:31,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:32,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:34,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:36,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:36,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:37,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:38,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:39,276 - distributed.utils_perf - INFO - full garbage collection released 36.82 MiB from 38 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:40,022 - distributed.utils_perf - INFO - full garbage collection released 599.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:47:40,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:43,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:44,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:45,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:47,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:51,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:52,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:52,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:54,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:54,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:57,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:58,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:47:59,205 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:47:59,787 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:47:59,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:00,509 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:01,384 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:01,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:02,472 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:02,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:03,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:03,835 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:48:04,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:05,570 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:07,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:07,676 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:48:09,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:09,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:11,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:12,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:14,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:14,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:14,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:15,811 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:15,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:16,399 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:17,122 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:17,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:18,011 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)
2024-04-19 17:48:18,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:19,100 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:48:20,460 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 17:48:22,191 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 17:48:24,309 - distributed.utils_perf - WARNING - full garbage collections took 38% CPU time recently (threshold: 10%)
2024-04-19 17:48:24,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:25,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:26,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:26,947 - distributed.utils_perf - WARNING - full garbage collections took 36% CPU time recently (threshold: 10%)
2024-04-19 17:48:26,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:28,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:30,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:31,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:32,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:34,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:35,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:36,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:36,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:37,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:40,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:42,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:43,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:44,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:46,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:47,514 - distributed.utils_perf - INFO - full garbage collection released 9.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:48:49,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:50,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:51,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:54,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:54,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:48:57,576 - distributed.utils_perf - INFO - full garbage collection released 11.76 MiB from 152 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:48:57,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:01,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:03,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:06,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:08,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:09,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:10,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:11,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:12,392 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:49:12,633 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:49:12,924 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:13,277 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:13,700 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:14,004 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:14,217 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:14,836 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:14,853 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:15,639 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:15,867 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:49:16,594 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:17,131 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:49:17,769 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:17,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:18,743 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:49:19,248 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:19,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:20,715 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:21,101 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)
2024-04-19 17:49:21,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:22,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:23,135 - distributed.utils_perf - WARNING - full garbage collections took 46% CPU time recently (threshold: 10%)
2024-04-19 17:49:23,373 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:49:24,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:26,214 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:49:26,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:30,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:34,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:37,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:49:38,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:50:00,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:50:30] task [xgboost.dask-tcp://10.201.2.16:33657]:tcp://10.201.2.16:33657 got new rank 0
[17:50:30] task [xgboost.dask-tcp://10.201.2.16:38381]:tcp://10.201.2.16:38381 got new rank 1
[17:50:30] task [xgboost.dask-tcp://10.201.2.16:39811]:tcp://10.201.2.16:39811 got new rank 2
[17:50:30] task [xgboost.dask-tcp://10.201.2.16:42743]:tcp://10.201.2.16:42743 got new rank 3
[17:50:30] task [xgboost.dask-tcp://10.201.2.17:34433]:tcp://10.201.2.17:34433 got new rank 4
[17:50:30] task [xgboost.dask-tcp://10.201.2.17:35649]:tcp://10.201.2.17:35649 got new rank 5
[17:50:30] task [xgboost.dask-tcp://10.201.2.17:40885]:tcp://10.201.2.17:40885 got new rank 6
[17:50:30] task [xgboost.dask-tcp://10.201.2.17:43517]:tcp://10.201.2.17:43517 got new rank 7
2024-04-19 17:52:45,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:52:46,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:53:28] task [xgboost.dask-tcp://10.201.2.16:33657]:tcp://10.201.2.16:33657 got new rank 0
[17:53:28] task [xgboost.dask-tcp://10.201.2.16:38381]:tcp://10.201.2.16:38381 got new rank 1
[17:53:28] task [xgboost.dask-tcp://10.201.2.16:39811]:tcp://10.201.2.16:39811 got new rank 2
[17:53:28] task [xgboost.dask-tcp://10.201.2.16:42743]:tcp://10.201.2.16:42743 got new rank 3
[17:53:28] task [xgboost.dask-tcp://10.201.2.17:34433]:tcp://10.201.2.17:34433 got new rank 4
[17:53:28] task [xgboost.dask-tcp://10.201.2.17:35649]:tcp://10.201.2.17:35649 got new rank 5
[17:53:28] task [xgboost.dask-tcp://10.201.2.17:40885]:tcp://10.201.2.17:40885 got new rank 6
[17:53:28] task [xgboost.dask-tcp://10.201.2.17:43517]:tcp://10.201.2.17:43517 got new rank 7
2024-04-19 17:55:41,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:56:22] task [xgboost.dask-tcp://10.201.2.16:33657]:tcp://10.201.2.16:33657 got new rank 0
[17:56:22] task [xgboost.dask-tcp://10.201.2.16:38381]:tcp://10.201.2.16:38381 got new rank 1
[17:56:22] task [xgboost.dask-tcp://10.201.2.16:39811]:tcp://10.201.2.16:39811 got new rank 2
[17:56:22] task [xgboost.dask-tcp://10.201.2.16:42743]:tcp://10.201.2.16:42743 got new rank 3
[17:56:22] task [xgboost.dask-tcp://10.201.2.17:34433]:tcp://10.201.2.17:34433 got new rank 4
[17:56:22] task [xgboost.dask-tcp://10.201.2.17:35649]:tcp://10.201.2.17:35649 got new rank 5
[17:56:22] task [xgboost.dask-tcp://10.201.2.17:40885]:tcp://10.201.2.17:40885 got new rank 6
[17:56:22] task [xgboost.dask-tcp://10.201.2.17:43517]:tcp://10.201.2.17:43517 got new rank 7
2024-04-19 17:58:49,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:58:49,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:59:31] task [xgboost.dask-tcp://10.201.2.16:33657]:tcp://10.201.2.16:33657 got new rank 0
[17:59:31] task [xgboost.dask-tcp://10.201.2.16:38381]:tcp://10.201.2.16:38381 got new rank 1
[17:59:31] task [xgboost.dask-tcp://10.201.2.16:39811]:tcp://10.201.2.16:39811 got new rank 2
[17:59:31] task [xgboost.dask-tcp://10.201.2.16:42743]:tcp://10.201.2.16:42743 got new rank 3
[17:59:31] task [xgboost.dask-tcp://10.201.2.17:34433]:tcp://10.201.2.17:34433 got new rank 4
[17:59:31] task [xgboost.dask-tcp://10.201.2.17:35649]:tcp://10.201.2.17:35649 got new rank 5
[17:59:31] task [xgboost.dask-tcp://10.201.2.17:40885]:tcp://10.201.2.17:40885 got new rank 6
[17:59:31] task [xgboost.dask-tcp://10.201.2.17:43517]:tcp://10.201.2.17:43517 got new rank 7
2024-04-19 18:01:51,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:01:51,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:05,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:06,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:07,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:07,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:08,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:08,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:08,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:02:09,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[18:02:52] task [xgboost.dask-tcp://10.201.2.16:33657]:tcp://10.201.2.16:33657 got new rank 0
[18:02:52] task [xgboost.dask-tcp://10.201.2.16:38381]:tcp://10.201.2.16:38381 got new rank 1
[18:02:52] task [xgboost.dask-tcp://10.201.2.16:39811]:tcp://10.201.2.16:39811 got new rank 2
[18:02:52] task [xgboost.dask-tcp://10.201.2.16:42743]:tcp://10.201.2.16:42743 got new rank 3
[18:02:52] task [xgboost.dask-tcp://10.201.2.17:34433]:tcp://10.201.2.17:34433 got new rank 4
[18:02:52] task [xgboost.dask-tcp://10.201.2.17:35649]:tcp://10.201.2.17:35649 got new rank 5
[18:02:52] task [xgboost.dask-tcp://10.201.2.17:40885]:tcp://10.201.2.17:40885 got new rank 6
[18:02:52] task [xgboost.dask-tcp://10.201.2.17:43517]:tcp://10.201.2.17:43517 got new rank 7
2024-04-19 18:04:52,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:04:55,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:04:55,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:04:55,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:42743. Reason: scheduler-close
2024-04-19 18:05:25,140 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:38381. Reason: scheduler-close
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:33657. Reason: scheduler-close
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.16:39811. Reason: scheduler-close
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:40885. Reason: scheduler-close
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:34433. Reason: scheduler-close
2024-04-19 18:05:25,140 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:35649. Reason: scheduler-close
2024-04-19 18:05:25,139 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.17:43517. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:40153'. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:35847'. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:38019'. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.16:34999'. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:45495'. Reason: scheduler-close
2024-04-19 18:05:25,142 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:36331'. Reason: scheduler-close
2024-04-19 18:05:25,141 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:47852 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:47852 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,141 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:47862 remote=tcp://10.201.2.39:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.17:47862 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,149 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:46605'. Reason: scheduler-close
2024-04-19 18:05:25,150 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.17:35891'. Reason: scheduler-close
2024-04-19 18:05:25,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:05:25,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:05:25,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.17:52414 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.16:49784 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.17:47890 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 18:05:25,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.17:52426 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:49756 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,941 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:43392 remote=tcp://10.201.2.39:8786>: Stream is closed
/10.201.2.17:52404 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.16:43404 remote=tcp://10.201.2.39:8786>: Stream is closed
2024-04-19 18:05:25,996 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:25,997 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,029 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,029 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,039 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,039 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,040 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,040 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,042 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,042 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,050 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,050 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,055 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,055 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:26,101 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.2.39:8786; closing.
2024-04-19 18:05:26,102 - distributed.nanny - INFO - Worker closed
2024-04-19 18:05:27,998 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,032 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,041 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,042 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,044 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,052 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,057 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:28,103 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 18:05:29,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:35847'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,040 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:34999'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,146 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:40153'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,152 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:35891'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,198 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:45495'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,303 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,308 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.17:46605'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,309 - distributed.dask_worker - INFO - End worker
2024-04-19 18:05:29,501 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.16:38019'. Reason: nanny-close-gracefully
2024-04-19 18:05:29,502 - distributed.dask_worker - INFO - End worker
