2024-04-19 15:40:51,029 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,030 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,030 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,030 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,238 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,238 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,238 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,238 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:46735'
2024-04-19 15:40:51,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:39195'
2024-04-19 15:40:51,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:33963'
2024-04-19 15:40:51,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.237:43583'
2024-04-19 15:40:51,393 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,393 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,393 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,393 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:51,647 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,647 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,647 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,648 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:51,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:34563'
2024-04-19 15:40:51,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:35129'
2024-04-19 15:40:51,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:41619'
2024-04-19 15:40:51,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:40999'
2024-04-19 15:40:52,044 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,044 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,045 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,045 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,045 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,045 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,045 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,046 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,088 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,088 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,088 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,089 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,476 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,477 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,477 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,477 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 15:40:52,478 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,478 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,478 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,478 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 15:40:52,520 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,521 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,521 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,521 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 15:40:52,896 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:52,896 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:52,896 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:52,896 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,482 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,482 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,482 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,482 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 15:40:53,914 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f2c020e2-ffdd-4bc5-be7a-79c3ab7a317d
2024-04-19 15:40:53,914 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a8ea24df-8f5b-4f2b-b482-382fe378f41a
2024-04-19 15:40:53,914 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:33741
2024-04-19 15:40:53,914 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:38275
2024-04-19 15:40:53,914 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:38275
2024-04-19 15:40:53,914 - distributed.worker - INFO -          dashboard at:         10.201.2.237:35639
2024-04-19 15:40:53,914 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3e9243ac-0579-489c-913f-d4b66e0b2891
2024-04-19 15:40:53,914 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:38055
2024-04-19 15:40:53,915 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:38055
2024-04-19 15:40:53,915 - distributed.worker - INFO -          dashboard at:         10.201.2.237:35011
2024-04-19 15:40:53,914 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:33741
2024-04-19 15:40:53,915 - distributed.worker - INFO -          dashboard at:         10.201.2.237:44363
2024-04-19 15:40:53,915 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:53,914 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8a15c919-1ffd-4199-8c6c-ae3c1e35a251
2024-04-19 15:40:53,915 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.237:35177
2024-04-19 15:40:53,915 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.237:35177
2024-04-19 15:40:53,915 - distributed.worker - INFO -          dashboard at:         10.201.2.237:39041
2024-04-19 15:40:53,915 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:53,915 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:53,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jojv2gix
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:53,915 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:53,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_1dj5hw
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:53,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p21gii6o
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:53,915 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:53,915 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:53,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1otpfvpw
2024-04-19 15:40:53,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e6ee09a3-d0ee-4047-b42c-7b3530d3b30f
2024-04-19 15:40:54,593 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-10101a68-0955-4cbb-859c-89586fac47b4
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35169
2024-04-19 15:40:54,593 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35169
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:33065
2024-04-19 15:40:54,593 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:33065
2024-04-19 15:40:54,593 - distributed.worker - INFO -          dashboard at:         10.201.2.137:37025
2024-04-19 15:40:54,593 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,593 - distributed.worker - INFO -          dashboard at:         10.201.2.137:38071
2024-04-19 15:40:54,593 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,593 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,593 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-6a378cd2-d7d4-41cd-9b92-3c9ef98c3caf
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35227
2024-04-19 15:40:54,593 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35227
2024-04-19 15:40:54,593 - distributed.worker - INFO -          dashboard at:         10.201.2.137:44367
2024-04-19 15:40:54,593 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-01d46964-eb41-4788-b79f-7cce8df35cfe
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:33787
2024-04-19 15:40:54,593 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:33787
2024-04-19 15:40:54,593 - distributed.worker - INFO -          dashboard at:         10.201.2.137:38493
2024-04-19 15:40:54,593 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,593 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hx410c70
2024-04-19 15:40:54,594 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o7nax80a
2024-04-19 15:40:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jy4vq4xs
2024-04-19 15:40:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,593 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 15:40:54,594 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:54,594 - distributed.worker - INFO -               Threads:                          8
2024-04-19 15:40:54,594 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 15:40:54,594 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m8qba9id
2024-04-19 15:40:54,594 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:55,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:55,469 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:55,469 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:55,470 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:55,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:55,483 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:55,483 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:55,483 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:55,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:55,484 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:55,484 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:55,484 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:55,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:55,485 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:55,485 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:55,485 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:57,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,061 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:57,061 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,062 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:57,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,062 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:57,063 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,063 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,063 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:57,063 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:57,063 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,064 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:57,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 15:40:57,065 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 15:40:57,065 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 15:40:57,065 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 15:40:58,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:40:58,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:40:58,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:40:58,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:00,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:05,685 - distributed.utils_perf - INFO - full garbage collection released 146.02 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:06,610 - distributed.utils_perf - INFO - full garbage collection released 32.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:06,729 - distributed.utils_perf - INFO - full garbage collection released 1.03 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:09,852 - distributed.utils_perf - INFO - full garbage collection released 98.69 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:09,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:12,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:13,496 - distributed.utils_perf - INFO - full garbage collection released 3.12 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:13,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:14,387 - distributed.utils_perf - INFO - full garbage collection released 293.93 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:16,339 - distributed.utils_perf - INFO - full garbage collection released 147.10 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:16,974 - distributed.utils_perf - INFO - full garbage collection released 141.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:17,216 - distributed.utils_perf - INFO - full garbage collection released 24.40 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:17,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:17,715 - distributed.utils_perf - INFO - full garbage collection released 105.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:17,905 - distributed.utils_perf - INFO - full garbage collection released 76.42 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:17,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:18,591 - distributed.utils_perf - INFO - full garbage collection released 615.19 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:18,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:19,044 - distributed.utils_perf - INFO - full garbage collection released 17.01 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:20,131 - distributed.utils_perf - INFO - full garbage collection released 3.14 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:20,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:24,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:25,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:27,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:28,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:32,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:35,852 - distributed.utils_perf - INFO - full garbage collection released 1.09 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:36,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:36,837 - distributed.utils_perf - INFO - full garbage collection released 293.88 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:37,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:39,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:41,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:41,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:44,389 - distributed.utils_perf - INFO - full garbage collection released 618.36 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:45,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:48,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:49,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:50,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:55,305 - distributed.utils_perf - INFO - full garbage collection released 1.85 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:56,750 - distributed.utils_perf - INFO - full garbage collection released 184.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:41:57,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:57,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:41:59,217 - distributed.utils_perf - INFO - full garbage collection released 89.19 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:00,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:01,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:02,970 - distributed.utils_perf - INFO - full garbage collection released 21.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:04,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:10,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:11,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:16,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:18,643 - distributed.utils_perf - INFO - full garbage collection released 50.81 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:19,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 52.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:20,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:22,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:26,942 - distributed.utils_perf - INFO - full garbage collection released 257.31 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:27,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:30,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:33,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:36,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:37,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:37,492 - distributed.utils_perf - INFO - full garbage collection released 289.45 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:42:38,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:42,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:45,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:47,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:47,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:49,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 58.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:53,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:55,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:56,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:42:58,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:03,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:04,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:04,312 - distributed.utils_perf - INFO - full garbage collection released 1.38 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:05,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:06,938 - distributed.utils_perf - INFO - full garbage collection released 875.54 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:08,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:11,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:12,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:16,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:19,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:20,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:22,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:24,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:25,977 - distributed.utils_perf - INFO - full garbage collection released 4.40 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:29,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:29,818 - distributed.utils_perf - INFO - full garbage collection released 1.65 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:31,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:32,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:36,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:37,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:42,410 - distributed.utils_perf - INFO - full garbage collection released 565.53 MiB from 36 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:43:42,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:45,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:48,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:49,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 63.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:43:52,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:02,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:04,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:06,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:06,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:09,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:19,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 56.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:19,298 - distributed.utils_perf - INFO - full garbage collection released 787.13 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:44:27,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:27,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:29,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:31,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:33,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:34,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 42.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:36,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:37,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:46,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:48,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:49,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:51,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:52,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 48.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:54,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:54,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:44:58,333 - distributed.utils_perf - INFO - full garbage collection released 284.73 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:01,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:05,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:07,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:08,655 - distributed.utils_perf - INFO - full garbage collection released 786.16 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:45:09,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:10,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:11,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:16,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:19,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:22,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:23,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:27,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:30,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:31,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:32,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:34,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:35,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:36,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:38,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:45,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:46,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:49,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:55,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:57,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:45:58,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:04,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:09,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:10,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:11,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:16,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:18,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:19,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:21,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:21,652 - distributed.utils_perf - INFO - full garbage collection released 48.82 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:22,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:23,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:23,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:25,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 53.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:27,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:28,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:30,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:30,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:31,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:33,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:34,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:36,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:40,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:44,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:48,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:51,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:53,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:54,056 - distributed.utils_perf - INFO - full garbage collection released 699.82 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:46:54,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:46:56,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:00,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:01,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:03,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:04,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:08,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 38.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:08,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:12,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:15,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:16,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:16,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:18,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:21,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:23,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:24,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:38,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:42,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:42,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:45,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:47,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:48,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:49,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:51,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:47:57,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:03,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:04,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:04,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:05,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:07,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 37.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:08,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:11,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:12,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:17,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:18,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 32.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:22,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:25,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:26,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:30,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:34,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:36,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:40,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:41,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:44,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:45,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:45,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:50,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:55,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:48:58,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:00,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:04,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:04,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:07,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:09,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:11,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:16,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:17,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:18,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:22,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:26,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:31,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:31,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:35,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:35,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:37,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:39,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:41,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:48,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:50,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:55,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:49:56,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:02,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:06,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:08,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:10,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:12,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:13,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:14,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:18,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:19,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:20,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:25,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:33,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:44,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:55,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:50:58,467 - distributed.utils_perf - INFO - full garbage collection released 45.75 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 15:51:10,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:51:41] task [xgboost.dask-tcp://10.201.2.137:33065]:tcp://10.201.2.137:33065 got new rank 0
[15:51:41] task [xgboost.dask-tcp://10.201.2.137:33787]:tcp://10.201.2.137:33787 got new rank 1
[15:51:41] task [xgboost.dask-tcp://10.201.2.137:35169]:tcp://10.201.2.137:35169 got new rank 2
[15:51:41] task [xgboost.dask-tcp://10.201.2.137:35227]:tcp://10.201.2.137:35227 got new rank 3
[15:51:41] task [xgboost.dask-tcp://10.201.2.237:33741]:tcp://10.201.2.237:33741 got new rank 4
[15:51:41] task [xgboost.dask-tcp://10.201.2.237:35177]:tcp://10.201.2.237:35177 got new rank 5
[15:51:41] task [xgboost.dask-tcp://10.201.2.237:38055]:tcp://10.201.2.237:38055 got new rank 6
[15:51:41] task [xgboost.dask-tcp://10.201.2.237:38275]:tcp://10.201.2.237:38275 got new rank 7
2024-04-19 15:54:01,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:54:01,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:54:45] task [xgboost.dask-tcp://10.201.2.137:33065]:tcp://10.201.2.137:33065 got new rank 0
[15:54:45] task [xgboost.dask-tcp://10.201.2.137:33787]:tcp://10.201.2.137:33787 got new rank 1
[15:54:45] task [xgboost.dask-tcp://10.201.2.137:35169]:tcp://10.201.2.137:35169 got new rank 2
[15:54:45] task [xgboost.dask-tcp://10.201.2.137:35227]:tcp://10.201.2.137:35227 got new rank 3
[15:54:45] task [xgboost.dask-tcp://10.201.2.237:33741]:tcp://10.201.2.237:33741 got new rank 4
[15:54:45] task [xgboost.dask-tcp://10.201.2.237:35177]:tcp://10.201.2.237:35177 got new rank 5
[15:54:45] task [xgboost.dask-tcp://10.201.2.237:38055]:tcp://10.201.2.237:38055 got new rank 6
[15:54:45] task [xgboost.dask-tcp://10.201.2.237:38275]:tcp://10.201.2.237:38275 got new rank 7
2024-04-19 15:57:18,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:18,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:19,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 15:57:19,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[15:57:57] task [xgboost.dask-tcp://10.201.2.137:33065]:tcp://10.201.2.137:33065 got new rank 0
[15:57:57] task [xgboost.dask-tcp://10.201.2.137:33787]:tcp://10.201.2.137:33787 got new rank 1
[15:57:57] task [xgboost.dask-tcp://10.201.2.137:35169]:tcp://10.201.2.137:35169 got new rank 2
[15:57:57] task [xgboost.dask-tcp://10.201.2.137:35227]:tcp://10.201.2.137:35227 got new rank 3
[15:57:57] task [xgboost.dask-tcp://10.201.2.237:33741]:tcp://10.201.2.237:33741 got new rank 4
[15:57:57] task [xgboost.dask-tcp://10.201.2.237:35177]:tcp://10.201.2.237:35177 got new rank 5
[15:57:57] task [xgboost.dask-tcp://10.201.2.237:38055]:tcp://10.201.2.237:38055 got new rank 6
[15:57:57] task [xgboost.dask-tcp://10.201.2.237:38275]:tcp://10.201.2.237:38275 got new rank 7
2024-04-19 16:00:15,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:15,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:15,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:00:16,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:00:56] task [xgboost.dask-tcp://10.201.2.137:33065]:tcp://10.201.2.137:33065 got new rank 0
[16:00:56] task [xgboost.dask-tcp://10.201.2.137:33787]:tcp://10.201.2.137:33787 got new rank 1
[16:00:56] task [xgboost.dask-tcp://10.201.2.137:35169]:tcp://10.201.2.137:35169 got new rank 2
[16:00:56] task [xgboost.dask-tcp://10.201.2.137:35227]:tcp://10.201.2.137:35227 got new rank 3
[16:00:56] task [xgboost.dask-tcp://10.201.2.237:33741]:tcp://10.201.2.237:33741 got new rank 4
[16:00:56] task [xgboost.dask-tcp://10.201.2.237:35177]:tcp://10.201.2.237:35177 got new rank 5
[16:00:56] task [xgboost.dask-tcp://10.201.2.237:38055]:tcp://10.201.2.237:38055 got new rank 6
[16:00:56] task [xgboost.dask-tcp://10.201.2.237:38275]:tcp://10.201.2.237:38275 got new rank 7
2024-04-19 16:03:31,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:31,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:42,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:43,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:43,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:45,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:46,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:46,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:46,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 16:03:47,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[16:04:30] task [xgboost.dask-tcp://10.201.2.137:33065]:tcp://10.201.2.137:33065 got new rank 0
[16:04:30] task [xgboost.dask-tcp://10.201.2.137:33787]:tcp://10.201.2.137:33787 got new rank 1
[16:04:30] task [xgboost.dask-tcp://10.201.2.137:35169]:tcp://10.201.2.137:35169 got new rank 2
[16:04:30] task [xgboost.dask-tcp://10.201.2.137:35227]:tcp://10.201.2.137:35227 got new rank 3
[16:04:30] task [xgboost.dask-tcp://10.201.2.237:33741]:tcp://10.201.2.237:33741 got new rank 4
[16:04:30] task [xgboost.dask-tcp://10.201.2.237:35177]:tcp://10.201.2.237:35177 got new rank 5
[16:04:30] task [xgboost.dask-tcp://10.201.2.237:38055]:tcp://10.201.2.237:38055 got new rank 6
[16:04:30] task [xgboost.dask-tcp://10.201.2.237:38275]:tcp://10.201.2.237:38275 got new rank 7
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:33741. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:35177. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:38275. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.237:38055. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35169. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:33065. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35227. Reason: scheduler-close
2024-04-19 16:07:22,988 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:33787. Reason: scheduler-close
2024-04-19 16:07:22,990 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:33963'. Reason: scheduler-close
2024-04-19 16:07:22,990 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:39195'. Reason: scheduler-close
2024-04-19 16:07:22,990 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:46735'. Reason: scheduler-close
2024-04-19 16:07:22,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:42554 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.237:42554 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:22,994 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.237:43583'. Reason: scheduler-close
2024-04-19 16:07:22,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60226 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60226 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:22,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60192 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60192 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:22,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60208 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60208 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:22,996 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:41619'. Reason: scheduler-close
2024-04-19 16:07:22,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60220 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:60220 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:22,997 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:34563'. Reason: scheduler-close
2024-04-19 16:07:22,997 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:40999'. Reason: scheduler-close
2024-04-19 16:07:22,997 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:35129'. Reason: scheduler-close
2024-04-19 16:07:23,778 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.237:40066 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:23,778 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.237:42604 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:23,779 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:07:23,779 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.237:40058 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.237:43160 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:23,785 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:07:23,785 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:56532 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:23,786 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 16:07:23,786 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.2.137:35854 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.137:35870 remote=tcp://10.201.3.173:8786>: Stream is closed
/10.201.2.137:35850 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 16:07:23,863 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,864 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,869 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,869 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,877 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,877 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,877 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,877 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,895 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,896 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,906 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,906 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,935 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,935 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:23,951 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 16:07:23,952 - distributed.nanny - INFO - Worker closed
2024-04-19 16:07:25,865 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,873 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,880 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,898 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,909 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,936 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:25,953 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 16:07:26,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:46735'. Reason: nanny-close-gracefully
2024-04-19 16:07:26,806 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:26,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:43583'. Reason: nanny-close-gracefully
2024-04-19 16:07:26,814 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:26,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:33963'. Reason: nanny-close-gracefully
2024-04-19 16:07:26,820 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:27,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.237:39195'. Reason: nanny-close-gracefully
2024-04-19 16:07:27,269 - distributed.dask_worker - INFO - End worker
2024-04-19 16:07:27,284 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:41619'. Reason: nanny-close-gracefully
2024-04-19 16:07:27,285 - distributed.dask_worker - INFO - End worker
