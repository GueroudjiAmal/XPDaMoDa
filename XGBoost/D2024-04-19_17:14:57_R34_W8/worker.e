2024-04-19 17:16:14,054 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,054 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,054 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,054 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,411 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,411 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,411 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,411 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:14,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:36025'
2024-04-19 17:16:14,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:45469'
2024-04-19 17:16:14,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:41615'
2024-04-19 17:16:14,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:36737'
2024-04-19 17:16:14,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,665 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:14,666 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,011 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,011 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,011 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,011 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:38073'
2024-04-19 17:16:15,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:42199'
2024-04-19 17:16:15,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:42207'
2024-04-19 17:16:15,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:39839'
2024-04-19 17:16:15,610 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,610 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,610 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,610 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:15,616 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,617 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,617 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,617 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:15,662 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,664 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,664 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:15,664 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,073 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,073 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,073 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,073 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,073 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 17:16:16,074 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,074 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,075 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 17:16:16,120 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,120 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,120 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,120 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 17:16:16,668 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,668 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,668 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:16,668 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,123 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,123 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,123 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 17:16:17,124 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
2024-04-19 17:16:17,580 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-yp3_7ts1/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-yp3_7ts1/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:16:17,580 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-aku6e6zj/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-aku6e6zj/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:16:17,580 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-qlrg28ri/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-qlrg28ri/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:33141
2024-04-19 17:16:17,584 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:33141
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:41245
2024-04-19 17:16:17,584 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:41245
2024-04-19 17:16:17,584 - distributed.worker - INFO -          dashboard at:          10.201.1.75:39507
2024-04-19 17:16:17,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:17,584 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,584 - distributed.worker - INFO -          dashboard at:          10.201.1.75:37323
2024-04-19 17:16:17,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:17,584 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,584 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,584 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,580 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-zgnsn5ap/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-zgnsn5ap/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:34007
2024-04-19 17:16:17,584 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:34007
2024-04-19 17:16:17,584 - distributed.worker - INFO -          dashboard at:          10.201.1.75:33191
2024-04-19 17:16:17,585 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:37383
2024-04-19 17:16:17,584 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:37383
2024-04-19 17:16:17,584 - distributed.worker - INFO -          dashboard at:          10.201.1.75:43919
2024-04-19 17:16:17,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:17,584 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,584 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,584 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yp3_7ts1
2024-04-19 17:16:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,584 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,584 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aku6e6zj
2024-04-19 17:16:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qlrg28ri
2024-04-19 17:16:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:17,585 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:17,585 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:17,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zgnsn5ap
2024-04-19 17:16:17,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,238 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-21c8b773-6624-484e-94a2-c3d6bf0129d4
2024-04-19 17:16:18,238 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3dc140fe-2134-4b0e-8187-c0f5c3cbb612
2024-04-19 17:16:18,238 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:44031
2024-04-19 17:16:18,238 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:44031
2024-04-19 17:16:18,238 - distributed.worker - INFO -          dashboard at:         10.201.1.183:40661
2024-04-19 17:16:18,238 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-89699d5a-b49b-4a6b-9f77-f6944063e706
2024-04-19 17:16:18,238 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:35615
2024-04-19 17:16:18,238 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:35615
2024-04-19 17:16:18,239 - distributed.worker - INFO -          dashboard at:         10.201.1.183:42415
2024-04-19 17:16:18,239 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,238 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:37325
2024-04-19 17:16:18,238 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:37325
2024-04-19 17:16:18,238 - distributed.worker - INFO -          dashboard at:         10.201.1.183:42765
2024-04-19 17:16:18,238 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,239 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,239 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ml5m3tjs
2024-04-19 17:16:18,238 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,239 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,239 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9ossky7
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,238 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-84862c88-02ad-4963-a3f7-aad0c5b1f7aa
2024-04-19 17:16:18,238 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:45183
2024-04-19 17:16:18,239 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:45183
2024-04-19 17:16:18,239 - distributed.worker - INFO -          dashboard at:         10.201.1.183:36419
2024-04-19 17:16:18,239 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 17:16:18,239 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,239 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ap4hpfqt
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:18,239 - distributed.worker - INFO -               Threads:                          8
2024-04-19 17:16:18,239 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 17:16:18,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pe3l85w5
2024-04-19 17:16:18,239 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,913 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,913 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,913 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,915 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,915 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,916 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,918 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,918 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,919 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,920 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,920 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,920 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,921 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,921 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,922 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,922 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,922 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,923 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,923 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,923 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,924 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:20,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 17:16:20,924 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 17:16:20,924 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 17:16:20,925 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 17:16:31,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:31,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:35,647 - distributed.utils_perf - INFO - full garbage collection released 355.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:36,114 - distributed.utils_perf - INFO - full garbage collection released 71.72 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:36,670 - distributed.utils_perf - INFO - full garbage collection released 1.16 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,320 - distributed.utils_perf - INFO - full garbage collection released 1.62 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:38,920 - distributed.utils_perf - INFO - full garbage collection released 455.30 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:39,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:40,872 - distributed.utils_perf - INFO - full garbage collection released 827.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:41,324 - distributed.utils_perf - INFO - full garbage collection released 1.73 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:43,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:43,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:44,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:45,095 - distributed.utils_perf - INFO - full garbage collection released 295.63 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,244 - distributed.utils_perf - INFO - full garbage collection released 3.86 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:45,577 - distributed.utils_perf - INFO - full garbage collection released 2.59 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:46,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:46,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:47,978 - distributed.utils_perf - INFO - full garbage collection released 529.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,297 - distributed.utils_perf - INFO - full garbage collection released 570.90 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:48,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:48,789 - distributed.utils_perf - INFO - full garbage collection released 4.33 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:49,241 - distributed.utils_perf - INFO - full garbage collection released 1.15 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:16:50,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:53,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:54,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:55,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:57,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:16:58,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:00,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:02,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:04,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:05,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:06,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:06,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:07,456 - distributed.utils_perf - INFO - full garbage collection released 96.79 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:13,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:14,021 - distributed.utils_perf - INFO - full garbage collection released 27.89 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:16,159 - distributed.utils_perf - INFO - full garbage collection released 90.65 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:19,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:19,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:22,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:24,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:25,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:27,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:28,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:30,143 - distributed.utils_perf - INFO - full garbage collection released 109.64 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:30,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:31,851 - distributed.utils_perf - INFO - full garbage collection released 18.22 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:33,844 - distributed.utils_perf - INFO - full garbage collection released 184.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:37,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:42,799 - distributed.utils_perf - INFO - full garbage collection released 74.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:43,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:44,694 - distributed.utils_perf - INFO - full garbage collection released 156.13 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:45,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,466 - distributed.utils_perf - INFO - full garbage collection released 167.20 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:52,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:52,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:53,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:55,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:17:56,325 - distributed.utils_perf - INFO - full garbage collection released 229.85 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:17:58,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:02,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:03,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 46.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:06,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:07,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:10,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:11,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:13,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:14,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:16,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:18,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:21,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:22,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:24,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:25,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:28,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:30,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:36,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:39,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:43,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:44,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:45,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:46,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:47,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:48,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:51,896 - distributed.utils_perf - INFO - full garbage collection released 246.43 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:18:51,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:54,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:18:59,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:00,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:02,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:05,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:07,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:08,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:12,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:13,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:13,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:14,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:15,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:17,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:19,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:20,409 - distributed.utils_perf - INFO - full garbage collection released 131.05 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:19:22,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:22,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:23,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:24,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:26,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:28,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:33,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:35,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:36,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:40,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:43,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:45,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:46,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:49,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:54,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:55,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:19:57,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:01,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:02,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:05,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:06,071 - distributed.utils_perf - INFO - full garbage collection released 2.70 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:06,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:07,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:11,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:14,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:15,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:16,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:22,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:23,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:26,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:30,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:31,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:32,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:33,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:37,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:37,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:38,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:40,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:42,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:44,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:45,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:46,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:48,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:50,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:53,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:54,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:20:55,334 - distributed.utils_perf - INFO - full garbage collection released 151.71 MiB from 112 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:20:55,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:00,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:01,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:04,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:05,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:10,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:11,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:14,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:15,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:18,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:20,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:26,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:28,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:29,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:30,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:32,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:40,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:41,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:42,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:44,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:47,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:50,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:52,549 - distributed.utils_perf - INFO - full garbage collection released 203.85 MiB from 37 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:21:54,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:55,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:56,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:57,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:21:58,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:03,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:04,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:08,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:13,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:14,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:19,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:21,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:22,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:23,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:28,983 - distributed.comm.tcp - INFO - Connection from tcp://10.201.1.183:57534 closed before handshake completed
2024-04-19 17:22:29,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:30,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:32,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:33,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 34.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:36,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:39,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:40,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:45,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:53,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:54,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:55,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:56,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:22:57,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:02,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:03,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:07,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:08,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,133 - distributed.utils_perf - INFO - full garbage collection released 1.17 GiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:12,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:12,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:13,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:16,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:17,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:19,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:20,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:21,450 - distributed.utils_perf - INFO - full garbage collection released 22.73 MiB from 284 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:25,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:26,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:29,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:31,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 35.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:32,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:33,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:37,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:40,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:41,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:43,745 - distributed.utils_perf - WARNING - full garbage collections took 47% CPU time recently (threshold: 10%)
2024-04-19 17:23:44,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:46,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:48,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:49,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:51,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:52,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,051 - distributed.utils_perf - INFO - full garbage collection released 34.33 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:23:53,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:53,927 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:54,340 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:54,840 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:55,451 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:56,201 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:57,128 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:57,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:57,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:23:58,273 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:23:59,696 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:00,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:01,467 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:24:03,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:03,664 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,383 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:24:06,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:07,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:08,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:09,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:10,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:11,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:12,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:15,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:16,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:18,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:20,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:20,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:21,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:23,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:24,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:25,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:26,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:28,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:30,325 - distributed.utils_perf - INFO - full garbage collection released 24.51 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:30,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:31,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:34,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:35,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:39,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:41,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:44,607 - distributed.utils_perf - INFO - full garbage collection released 18.61 MiB from 152 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:45,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:46,626 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:46,927 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:47,297 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:47,751 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:48,301 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:48,970 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:49,787 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:50,782 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:50,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:51,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:52,029 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:53,600 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:54,091 - distributed.utils_perf - INFO - full garbage collection released 254.43 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:24:55,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:55,557 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:24:55,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:56,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:24:57,974 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:00,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:00,953 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:25:02,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:03,306 - distributed.utils_perf - INFO - full garbage collection released 1.76 GiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:03,833 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:25:04,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:04,887 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:25:05,144 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:25:05,447 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:25:05,811 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:25:05,999 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:06,251 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:25:06,793 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:25:07,423 - distributed.utils_perf - INFO - full garbage collection released 17.73 MiB from 57 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:07,444 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:25:07,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:08,249 - distributed.utils_perf - WARNING - full garbage collections took 52% CPU time recently (threshold: 10%)
2024-04-19 17:25:08,704 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)
2024-04-19 17:25:09,232 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:25:10,432 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)
2024-04-19 17:25:11,958 - distributed.utils_perf - WARNING - full garbage collections took 50% CPU time recently (threshold: 10%)
2024-04-19 17:25:13,828 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)
2024-04-19 17:25:14,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:15,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:16,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:25,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:25:27,643 - distributed.utils_perf - INFO - full garbage collection released 1.42 GiB from 208 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:25:35,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:26:04] task [xgboost.dask-tcp://10.201.1.183:35615]:tcp://10.201.1.183:35615 got new rank 0
[17:26:04] task [xgboost.dask-tcp://10.201.1.183:37325]:tcp://10.201.1.183:37325 got new rank 1
[17:26:04] task [xgboost.dask-tcp://10.201.1.183:44031]:tcp://10.201.1.183:44031 got new rank 2
[17:26:04] task [xgboost.dask-tcp://10.201.1.183:45183]:tcp://10.201.1.183:45183 got new rank 3
[17:26:04] task [xgboost.dask-tcp://10.201.1.75:33141]:tcp://10.201.1.75:33141 got new rank 4
[17:26:04] task [xgboost.dask-tcp://10.201.1.75:34007]:tcp://10.201.1.75:34007 got new rank 5
[17:26:04] task [xgboost.dask-tcp://10.201.1.75:37383]:tcp://10.201.1.75:37383 got new rank 6
[17:26:04] task [xgboost.dask-tcp://10.201.1.75:41245]:tcp://10.201.1.75:41245 got new rank 7
[17:29:00] task [xgboost.dask-tcp://10.201.1.183:35615]:tcp://10.201.1.183:35615 got new rank 0
[17:29:00] task [xgboost.dask-tcp://10.201.1.183:37325]:tcp://10.201.1.183:37325 got new rank 1
[17:29:00] task [xgboost.dask-tcp://10.201.1.183:44031]:tcp://10.201.1.183:44031 got new rank 2
[17:29:00] task [xgboost.dask-tcp://10.201.1.183:45183]:tcp://10.201.1.183:45183 got new rank 3
[17:29:00] task [xgboost.dask-tcp://10.201.1.75:33141]:tcp://10.201.1.75:33141 got new rank 4
[17:29:00] task [xgboost.dask-tcp://10.201.1.75:34007]:tcp://10.201.1.75:34007 got new rank 5
[17:29:00] task [xgboost.dask-tcp://10.201.1.75:37383]:tcp://10.201.1.75:37383 got new rank 6
[17:29:00] task [xgboost.dask-tcp://10.201.1.75:41245]:tcp://10.201.1.75:41245 got new rank 7
2024-04-19 17:31:30,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:32:06] task [xgboost.dask-tcp://10.201.1.183:35615]:tcp://10.201.1.183:35615 got new rank 0
[17:32:06] task [xgboost.dask-tcp://10.201.1.183:37325]:tcp://10.201.1.183:37325 got new rank 1
[17:32:06] task [xgboost.dask-tcp://10.201.1.183:44031]:tcp://10.201.1.183:44031 got new rank 2
[17:32:06] task [xgboost.dask-tcp://10.201.1.183:45183]:tcp://10.201.1.183:45183 got new rank 3
[17:32:06] task [xgboost.dask-tcp://10.201.1.75:33141]:tcp://10.201.1.75:33141 got new rank 4
[17:32:06] task [xgboost.dask-tcp://10.201.1.75:34007]:tcp://10.201.1.75:34007 got new rank 5
[17:32:06] task [xgboost.dask-tcp://10.201.1.75:37383]:tcp://10.201.1.75:37383 got new rank 6
[17:32:06] task [xgboost.dask-tcp://10.201.1.75:41245]:tcp://10.201.1.75:41245 got new rank 7
2024-04-19 17:34:30,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:34:30,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:35:01] task [xgboost.dask-tcp://10.201.1.183:35615]:tcp://10.201.1.183:35615 got new rank 0
[17:35:01] task [xgboost.dask-tcp://10.201.1.183:37325]:tcp://10.201.1.183:37325 got new rank 1
[17:35:01] task [xgboost.dask-tcp://10.201.1.183:44031]:tcp://10.201.1.183:44031 got new rank 2
[17:35:01] task [xgboost.dask-tcp://10.201.1.183:45183]:tcp://10.201.1.183:45183 got new rank 3
[17:35:01] task [xgboost.dask-tcp://10.201.1.75:33141]:tcp://10.201.1.75:33141 got new rank 4
[17:35:01] task [xgboost.dask-tcp://10.201.1.75:34007]:tcp://10.201.1.75:34007 got new rank 5
[17:35:01] task [xgboost.dask-tcp://10.201.1.75:37383]:tcp://10.201.1.75:37383 got new rank 6
[17:35:01] task [xgboost.dask-tcp://10.201.1.75:41245]:tcp://10.201.1.75:41245 got new rank 7
2024-04-19 17:35:05,031 - distributed.utils_perf - WARNING - full garbage collections took 42% CPU time recently (threshold: 10%)
2024-04-19 17:35:05,032 - distributed.utils_perf - INFO - full garbage collection released 262.19 MiB from 517 reference cycles (threshold: 9.54 MiB)
2024-04-19 17:37:24,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 17:37:25,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[17:37:52] task [xgboost.dask-tcp://10.201.1.183:35615]:tcp://10.201.1.183:35615 got new rank 0
[17:37:52] task [xgboost.dask-tcp://10.201.1.183:37325]:tcp://10.201.1.183:37325 got new rank 1
[17:37:52] task [xgboost.dask-tcp://10.201.1.183:44031]:tcp://10.201.1.183:44031 got new rank 2
[17:37:52] task [xgboost.dask-tcp://10.201.1.183:45183]:tcp://10.201.1.183:45183 got new rank 3
[17:37:52] task [xgboost.dask-tcp://10.201.1.75:33141]:tcp://10.201.1.75:33141 got new rank 4
[17:37:52] task [xgboost.dask-tcp://10.201.1.75:34007]:tcp://10.201.1.75:34007 got new rank 5
[17:37:52] task [xgboost.dask-tcp://10.201.1.75:37383]:tcp://10.201.1.75:37383 got new rank 6
[17:37:52] task [xgboost.dask-tcp://10.201.1.75:41245]:tcp://10.201.1.75:41245 got new rank 7
2024-04-19 17:40:34,806 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:41245. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:33141. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:45183. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:35615. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:37325. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:34007. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:37383. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:44031. Reason: scheduler-close
2024-04-19 17:40:34,808 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:38073'. Reason: scheduler-close
2024-04-19 17:40:34,808 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:36737'. Reason: scheduler-close
2024-04-19 17:40:34,808 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49738 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49738 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,807 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49740 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49740 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,807 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49746 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:49746 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,810 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:41615'. Reason: scheduler-close
2024-04-19 17:40:34,810 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:36025'. Reason: scheduler-close
2024-04-19 17:40:34,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:45469'. Reason: scheduler-close
2024-04-19 17:40:34,807 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55976 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55976 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,807 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55974 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55974 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,812 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:42199'. Reason: scheduler-close
2024-04-19 17:40:34,808 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55956 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:55956 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:34,812 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:42207'. Reason: scheduler-close
2024-04-19 17:40:34,813 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:39839'. Reason: scheduler-close
2024-04-19 17:40:34,929 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:34,930 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:34,938 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:34,939 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:34,968 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:34,968 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:35,014 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:35,015 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:35,569 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:38900 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:35,569 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:35,569 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 17:40:35,569 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:48048 remote=tcp://10.201.3.1:8786>: Stream is closed
/10.201.1.183:38882 remote=tcp://10.201.3.1:8786>: Stream is closed
/10.201.1.183:38888 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 17:40:35,572 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:35,573 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:35,609 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:35,609 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:35,627 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:35,628 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:35,629 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 17:40:35,629 - distributed.nanny - INFO - Worker closed
2024-04-19 17:40:36,972 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,009 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,035 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,041 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,489 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:36737'. Reason: nanny-close-gracefully
2024-04-19 17:40:37,489 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:37,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:36025'. Reason: nanny-close-gracefully
2024-04-19 17:40:37,563 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:37,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:45469'. Reason: nanny-close-gracefully
2024-04-19 17:40:37,571 - distributed.dask_worker - INFO - End worker
2024-04-19 17:40:37,647 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,681 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:37,686 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 17:40:38,040 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:41615'. Reason: nanny-close-gracefully
2024-04-19 17:40:38,040 - distributed.dask_worker - INFO - End worker
