2024-04-19 20:53:28,139 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,139 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,139 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,139 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,149 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,149 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,150 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,150 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:28,391 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,391 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,391 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,391 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:40285'
2024-04-19 20:53:28,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:34899'
2024-04-19 20:53:28,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:41687'
2024-04-19 20:53:28,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.183:37537'
2024-04-19 20:53:28,484 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,484 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,484 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,484 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:28,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:46267'
2024-04-19 20:53:28,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:40897'
2024-04-19 20:53:28,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:39755'
2024-04-19 20:53:28,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.1.75:41009'
2024-04-19 20:53:29,253 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,253 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,253 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,254 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,254 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,254 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,254 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,255 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,297 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,297 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,298 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,298 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,667 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,667 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,667 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,667 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 20:53:29,668 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,668 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,668 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,668 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 20:53:29,710 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,711 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,711 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:29,711 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 20:53:30,076 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,076 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,076 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,076 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,652 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,652 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,652 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:30,652 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 20:53:31,126 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-75249a65-efb1-4f72-835f-b7e9cf125fca
2024-04-19 20:53:31,126 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-37950cd6-3c14-497b-bd74-1ad29dfe0573
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:42365
2024-04-19 20:53:31,127 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:42365
2024-04-19 20:53:31,127 - distributed.worker - INFO -          dashboard at:         10.201.1.183:34531
2024-04-19 20:53:31,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,127 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xr4pn8u_
2024-04-19 20:53:31,127 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1903d234-0ead-482b-ae3e-99a2a6e201c4
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:33075
2024-04-19 20:53:31,127 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:33075
2024-04-19 20:53:31,127 - distributed.worker - INFO -          dashboard at:         10.201.1.183:34963
2024-04-19 20:53:31,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:33863
2024-04-19 20:53:31,127 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:33863
2024-04-19 20:53:31,127 - distributed.worker - INFO -          dashboard at:         10.201.1.183:44733
2024-04-19 20:53:31,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,127 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m73emdzu
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f977e58c-6f34-4d16-b98b-b87a34a42abb
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Start worker at:   tcp://10.201.1.183:34829
2024-04-19 20:53:31,127 - distributed.worker - INFO -          Listening to:   tcp://10.201.1.183:34829
2024-04-19 20:53:31,127 - distributed.worker - INFO -          dashboard at:         10.201.1.183:44493
2024-04-19 20:53:31,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,127 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u26na_9_
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,127 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,127 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bmme5h87
2024-04-19 20:53:31,127 - distributed.worker - INFO - -------------------------------------------------
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
[error] Could not initialize hg_class with protocol cxi
2024-04-19 20:53:31,462 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-d_mpojay/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-d_mpojay/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 20:53:31,462 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-3wn02ya1/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-3wn02ya1/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:44397
2024-04-19 20:53:31,472 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:44397
2024-04-19 20:53:31,472 - distributed.worker - INFO -          dashboard at:          10.201.1.75:44769
2024-04-19 20:53:31,462 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-wku6w6uy/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-wku6w6uy/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:35877
2024-04-19 20:53:31,472 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:35877
2024-04-19 20:53:31,472 - distributed.worker - INFO -          dashboard at:          10.201.1.75:45125
2024-04-19 20:53:31,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:38631
2024-04-19 20:53:31,472 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:38631
2024-04-19 20:53:31,472 - distributed.worker - INFO -          dashboard at:          10.201.1.75:36031
2024-04-19 20:53:31,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,472 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d_mpojay
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,472 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wku6w6uy
2024-04-19 20:53:31,462 - distributed.preloading - ERROR - Failed to start preload: MofkaWorkerPlugin.py
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 234, in start
    await preload.start()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/preloading.py", line 209, in start
    await result
  File "/tmp/dask-scratch-space/worker-7sqxix8_/MofkaWorkerPlugin.py", line 163, in dask_setup
    plugin = MofkaWorkerPlugin(worker, mofka_protocol, ssg_file)
  File "/tmp/dask-scratch-space/worker-7sqxix8_/MofkaWorkerPlugin.py", line 35, in __init__
    self.engine = Engine(self.protocol, use_progress_thread=True)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/pymargo/core.py", line 353, in __init__
    self._mid = _pymargo.init(
RuntimeError: margo_init() returned MARGO_INSTANCE_NULL
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Start worker at:    tcp://10.201.1.75:35017
2024-04-19 20:53:31,472 - distributed.worker - INFO -          Listening to:    tcp://10.201.1.75:35017
2024-04-19 20:53:31,472 - distributed.worker - INFO -          dashboard at:          10.201.1.75:33145
2024-04-19 20:53:31,472 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.3.1:8786
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3wn02ya1
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:31,472 - distributed.worker - INFO -               Threads:                          8
2024-04-19 20:53:31,472 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 20:53:31,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7sqxix8_
2024-04-19 20:53:31,472 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,577 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,577 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,577 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,578 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,579 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,579 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,580 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,582 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,583 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,583 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,583 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,584 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,584 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,584 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,585 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,585 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,586 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,586 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,586 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,587 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,587 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,588 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,588 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:34,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 20:53:34,588 - distributed.worker - INFO -         Registered to:      tcp://10.201.3.1:8786
2024-04-19 20:53:34,589 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 20:53:34,589 - distributed.core - INFO - Starting established connection to tcp://10.201.3.1:8786
2024-04-19 20:53:44,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:44,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:48,670 - distributed.utils_perf - INFO - full garbage collection released 2.27 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,067 - distributed.utils_perf - INFO - full garbage collection released 242.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:49,272 - distributed.utils_perf - INFO - full garbage collection released 703.06 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:50,722 - distributed.utils_perf - INFO - full garbage collection released 345.84 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:52,462 - distributed.utils_perf - INFO - full garbage collection released 1.14 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:53,714 - distributed.utils_perf - INFO - full garbage collection released 82.86 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:54,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:56,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:57,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:57,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:53:58,814 - distributed.utils_perf - INFO - full garbage collection released 2.37 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:59,071 - distributed.utils_perf - INFO - full garbage collection released 2.85 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:53:59,891 - distributed.utils_perf - INFO - full garbage collection released 3.42 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:00,814 - distributed.utils_perf - INFO - full garbage collection released 1.05 GiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:02,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:03,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:03,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:04,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:06,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:07,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:08,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:12,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:12,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:14,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:14,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:15,842 - distributed.utils_perf - INFO - full garbage collection released 217.23 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:17,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:17,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:19,568 - distributed.utils_perf - INFO - full garbage collection released 304.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:19,930 - distributed.utils_perf - INFO - full garbage collection released 637.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:20,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:20,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:21,943 - distributed.utils_perf - INFO - full garbage collection released 21.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:22,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:26,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:30,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:31,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:31,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:32,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:37,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:40,086 - distributed.utils_perf - INFO - full garbage collection released 546.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:43,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:44,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:45,290 - distributed.utils_perf - INFO - full garbage collection released 145.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:45,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:46,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:47,340 - distributed.utils_perf - INFO - full garbage collection released 775.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:54:48,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:54,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:55,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:56,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:54:59,411 - distributed.utils_perf - INFO - full garbage collection released 263.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:00,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:01,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:01,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:02,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:07,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:09,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:11,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:11,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:15,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:16,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:16,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 43.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:18,258 - distributed.utils_perf - INFO - full garbage collection released 749.05 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:19,686 - distributed.utils_perf - INFO - full garbage collection released 364.73 MiB from 19 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:20,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:22,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:23,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:24,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:25,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:25,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 29.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:28,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:31,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:32,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:34,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:37,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:38,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:39,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:40,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:40,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:45,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:47,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:48,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:48,350 - distributed.utils_perf - INFO - full garbage collection released 120.16 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:55:48,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:51,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:54,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:56,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:58,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:55:58,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:05,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:06,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:07,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:10,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:12,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:13,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:16,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:16,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:17,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:17,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:21,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:25,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:30,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:31,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:31,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:31,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:35,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:36,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:37,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:38,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:39,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:43,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:43,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:44,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:46,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:48,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:50,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:51,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:53,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:58,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:56:59,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:00,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:00,369 - distributed.utils_perf - INFO - full garbage collection released 263.12 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:02,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:03,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:06,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:08,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:08,366 - distributed.utils_perf - INFO - full garbage collection released 151.52 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:09,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:10,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:10,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:13,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:16,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:19,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:20,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:21,096 - distributed.utils_perf - INFO - full garbage collection released 278.69 MiB from 18 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:57:21,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:22,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:23,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:27,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:28,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:30,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:32,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:34,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:37,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:37,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:38,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:40,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:42,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:42,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:45,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:45,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:47,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:48,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:50,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:51,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:57:55,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:00,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:02,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:02,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:03,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:03,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:04,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:04,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:06,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:10,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:12,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:13,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:15,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:18,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:20,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:22,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:22,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:26,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:27,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:29,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:31,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:38,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:40,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:44,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:45,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:45,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:47,045 - distributed.utils_perf - INFO - full garbage collection released 0.96 GiB from 76 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:47,866 - distributed.utils_perf - INFO - full garbage collection released 2.29 GiB from 113 reference cycles (threshold: 9.54 MiB)
2024-04-19 20:58:47,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:48,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:49,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:52,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:53,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:54,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:57,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:58:58,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:00,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:02,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:03,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:12,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:14,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:18,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 40.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:23,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:26,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:26,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:27,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:28,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:29,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:30,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:34,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:35,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:38,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:39,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:40,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:42,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:45,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:49,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:52,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:55,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:56,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:57,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:58,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:58,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 20:59:58,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:02,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:03,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:04,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:07,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:09,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:09,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:11,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:14,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:14,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:15,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:16,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:20,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:21,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:21,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:22,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:24,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:28,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:29,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:31,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:32,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:38,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:39,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:41,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:45,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:46,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:46,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 31.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:52,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:53,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:53,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:55,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 33.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:56,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:57,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:57,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:00:57,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:02,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:04,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:07,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:09,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:10,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:11,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:11,612 - distributed.utils_perf - INFO - full garbage collection released 191.99 MiB from 95 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:11,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:12,621 - distributed.utils_perf - INFO - full garbage collection released 396.76 MiB from 0 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:15,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:17,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:17,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:20,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:22,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:25,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:25,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:27,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:27,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:29,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:32,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:32,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:34,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:35,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:35,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:37,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:38,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:39,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:40,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:44,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:44,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:45,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:46,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:51,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:52,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:53,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:01:54,334 - distributed.utils_perf - INFO - full garbage collection released 22.69 MiB from 208 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:01:57,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:01,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:02,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:06,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:10,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:11,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:13,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:15,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:16,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:19,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:20,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:22,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:23,012 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 21:02:23,179 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 21:02:25,092 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 21:02:25,605 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 21:02:27,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:27,428 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)
2024-04-19 21:02:28,793 - distributed.utils_perf - WARNING - full garbage collections took 40% CPU time recently (threshold: 10%)
2024-04-19 21:02:30,268 - distributed.utils_perf - INFO - full garbage collection released 15.79 MiB from 94 reference cycles (threshold: 9.54 MiB)
2024-04-19 21:02:35,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:02:35,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:03:14] task [xgboost.dask-tcp://10.201.1.183:33075]:tcp://10.201.1.183:33075 got new rank 0
[21:03:14] task [xgboost.dask-tcp://10.201.1.183:33863]:tcp://10.201.1.183:33863 got new rank 1
[21:03:14] task [xgboost.dask-tcp://10.201.1.183:34829]:tcp://10.201.1.183:34829 got new rank 2
[21:03:14] task [xgboost.dask-tcp://10.201.1.183:42365]:tcp://10.201.1.183:42365 got new rank 3
[21:03:14] task [xgboost.dask-tcp://10.201.1.75:35017]:tcp://10.201.1.75:35017 got new rank 4
[21:03:14] task [xgboost.dask-tcp://10.201.1.75:35877]:tcp://10.201.1.75:35877 got new rank 5
[21:03:14] task [xgboost.dask-tcp://10.201.1.75:38631]:tcp://10.201.1.75:38631 got new rank 6
[21:03:14] task [xgboost.dask-tcp://10.201.1.75:44397]:tcp://10.201.1.75:44397 got new rank 7
2024-04-19 21:05:28,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:06:09] task [xgboost.dask-tcp://10.201.1.183:33075]:tcp://10.201.1.183:33075 got new rank 0
[21:06:09] task [xgboost.dask-tcp://10.201.1.183:33863]:tcp://10.201.1.183:33863 got new rank 1
[21:06:09] task [xgboost.dask-tcp://10.201.1.183:34829]:tcp://10.201.1.183:34829 got new rank 2
[21:06:09] task [xgboost.dask-tcp://10.201.1.183:42365]:tcp://10.201.1.183:42365 got new rank 3
[21:06:09] task [xgboost.dask-tcp://10.201.1.75:35017]:tcp://10.201.1.75:35017 got new rank 4
[21:06:09] task [xgboost.dask-tcp://10.201.1.75:35877]:tcp://10.201.1.75:35877 got new rank 5
[21:06:09] task [xgboost.dask-tcp://10.201.1.75:38631]:tcp://10.201.1.75:38631 got new rank 6
[21:06:09] task [xgboost.dask-tcp://10.201.1.75:44397]:tcp://10.201.1.75:44397 got new rank 7
2024-04-19 21:08:29,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:09:07] task [xgboost.dask-tcp://10.201.1.183:33075]:tcp://10.201.1.183:33075 got new rank 0
[21:09:07] task [xgboost.dask-tcp://10.201.1.183:33863]:tcp://10.201.1.183:33863 got new rank 1
[21:09:07] task [xgboost.dask-tcp://10.201.1.183:34829]:tcp://10.201.1.183:34829 got new rank 2
[21:09:07] task [xgboost.dask-tcp://10.201.1.183:42365]:tcp://10.201.1.183:42365 got new rank 3
[21:09:07] task [xgboost.dask-tcp://10.201.1.75:35017]:tcp://10.201.1.75:35017 got new rank 4
[21:09:07] task [xgboost.dask-tcp://10.201.1.75:35877]:tcp://10.201.1.75:35877 got new rank 5
[21:09:07] task [xgboost.dask-tcp://10.201.1.75:38631]:tcp://10.201.1.75:38631 got new rank 6
[21:09:07] task [xgboost.dask-tcp://10.201.1.75:44397]:tcp://10.201.1.75:44397 got new rank 7
2024-04-19 21:11:30,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:11:30,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:12:11] task [xgboost.dask-tcp://10.201.1.183:33075]:tcp://10.201.1.183:33075 got new rank 0
[21:12:11] task [xgboost.dask-tcp://10.201.1.183:33863]:tcp://10.201.1.183:33863 got new rank 1
[21:12:11] task [xgboost.dask-tcp://10.201.1.183:34829]:tcp://10.201.1.183:34829 got new rank 2
[21:12:11] task [xgboost.dask-tcp://10.201.1.183:42365]:tcp://10.201.1.183:42365 got new rank 3
[21:12:11] task [xgboost.dask-tcp://10.201.1.75:35017]:tcp://10.201.1.75:35017 got new rank 4
[21:12:11] task [xgboost.dask-tcp://10.201.1.75:35877]:tcp://10.201.1.75:35877 got new rank 5
[21:12:11] task [xgboost.dask-tcp://10.201.1.75:38631]:tcp://10.201.1.75:38631 got new rank 6
[21:12:11] task [xgboost.dask-tcp://10.201.1.75:44397]:tcp://10.201.1.75:44397 got new rank 7
2024-04-19 21:14:26,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:27,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:38,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-19 21:14:39,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[21:15:09] task [xgboost.dask-tcp://10.201.1.183:33075]:tcp://10.201.1.183:33075 got new rank 0
[21:15:09] task [xgboost.dask-tcp://10.201.1.183:33863]:tcp://10.201.1.183:33863 got new rank 1
[21:15:09] task [xgboost.dask-tcp://10.201.1.183:34829]:tcp://10.201.1.183:34829 got new rank 2
[21:15:09] task [xgboost.dask-tcp://10.201.1.183:42365]:tcp://10.201.1.183:42365 got new rank 3
[21:15:09] task [xgboost.dask-tcp://10.201.1.75:35017]:tcp://10.201.1.75:35017 got new rank 4
[21:15:09] task [xgboost.dask-tcp://10.201.1.75:35877]:tcp://10.201.1.75:35877 got new rank 5
[21:15:09] task [xgboost.dask-tcp://10.201.1.75:38631]:tcp://10.201.1.75:38631 got new rank 6
[21:15:09] task [xgboost.dask-tcp://10.201.1.75:44397]:tcp://10.201.1.75:44397 got new rank 7
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:33863. Reason: scheduler-close
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:34829. Reason: scheduler-close
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:42365. Reason: scheduler-close
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.183:33075. Reason: scheduler-close
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:35017. Reason: scheduler-close
2024-04-19 21:18:02,327 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:38631. Reason: scheduler-close
2024-04-19 21:18:02,326 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:44397. Reason: scheduler-close
2024-04-19 21:18:02,327 - distributed.worker - INFO - Stopping worker at tcp://10.201.1.75:35877. Reason: scheduler-close
2024-04-19 21:18:02,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:39755'. Reason: scheduler-close
2024-04-19 21:18:02,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56310 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56310 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56308 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56308 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,335 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:37537'. Reason: scheduler-close
2024-04-19 21:18:02,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56294 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56294 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56320 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.183:56320 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:34899'. Reason: scheduler-close
2024-04-19 21:18:02,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:41687'. Reason: scheduler-close
2024-04-19 21:18:02,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.183:40285'. Reason: scheduler-close
2024-04-19 21:18:02,328 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47666 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47666 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,328 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47652 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47652 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,328 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47654 remote=tcp://10.201.3.1:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.1.75:47654 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:02,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:46267'. Reason: scheduler-close
2024-04-19 21:18:02,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:41009'. Reason: scheduler-close
2024-04-19 21:18:02,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.1.75:40897'. Reason: scheduler-close
2024-04-19 21:18:02,520 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:02,520 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:02,564 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:02,565 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:02,576 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:02,576 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:02,578 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:02,579 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:03,010 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp:/2024-04-19 21:18:03,011 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:51308 remote=tcp://10.201.3.1:8786>: Stream is closed
/10.201.1.183:51312 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:03,011 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:47892 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:03,012 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.201.1.183:47884 remote=tcp://10.201.3.1:8786>: Stream is closed
2024-04-19 21:18:03,013 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:03,014 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:03,015 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:03,015 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:03,016 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:03,016 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:03,068 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.1:8786; closing.
2024-04-19 21:18:03,068 - distributed.nanny - INFO - Worker closed
2024-04-19 21:18:04,524 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:04,578 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:04,580 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:05,044 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:05,086 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:05,138 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 21:18:05,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:46267'. Reason: nanny-close-gracefully
2024-04-19 21:18:05,723 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:05,730 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:41009'. Reason: nanny-close-gracefully
2024-04-19 21:18:05,731 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:05,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:40897'. Reason: nanny-close-gracefully
2024-04-19 21:18:05,737 - distributed.dask_worker - INFO - End worker
2024-04-19 21:18:06,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.1.75:39755'. Reason: nanny-close-gracefully
2024-04-19 21:18:06,190 - distributed.dask_worker - INFO - End worker
