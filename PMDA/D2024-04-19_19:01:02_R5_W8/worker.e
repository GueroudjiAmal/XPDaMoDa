2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,671 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,671 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,671 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,671 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,671 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,672 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,978 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:35083'
2024-04-19 19:01:36,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:40343'
2024-04-19 19:01:36,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:36921'
2024-04-19 19:01:36,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:34669'
2024-04-19 19:01:36,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:34261'
2024-04-19 19:01:36,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:41223'
2024-04-19 19:01:36,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:40801'
2024-04-19 19:01:36,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.251:46815'
2024-04-19 19:01:37,011 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,012 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,014 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,014 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,015 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,015 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,015 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,015 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,015 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,015 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,015 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,015 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,015 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:37,016 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,016 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,016 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:37,057 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,060 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,060 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,060 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,061 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,061 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,061 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,061 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,051 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,058 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:38,058 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:39,361 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-782b8766-3b55-4a06-9a94-c3b0ef505721
2024-04-19 19:01:39,361 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:44465
2024-04-19 19:01:39,361 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:44465
2024-04-19 19:01:39,361 - distributed.worker - INFO -          dashboard at:         10.201.3.251:46257
2024-04-19 19:01:39,361 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-19ec2469-a062-41e8-a7e8-64361f2b9121
2024-04-19 19:01:39,361 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a1c06371-4f6a-44ec-8cf3-573045bdf272
2024-04-19 19:01:39,361 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,361 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,361 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,361 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-594e36c2-cd6a-4857-8f85-175e83adff45
2024-04-19 19:01:39,361 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:41213
2024-04-19 19:01:39,361 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:41213
2024-04-19 19:01:39,361 - distributed.worker - INFO -          dashboard at:         10.201.3.251:38299
2024-04-19 19:01:39,361 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ughuscl5
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,361 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:34939
2024-04-19 19:01:39,362 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:34939
2024-04-19 19:01:39,362 - distributed.worker - INFO -          dashboard at:         10.201.3.251:33779
2024-04-19 19:01:39,362 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,362 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-axsc6t0_
2024-04-19 19:01:39,362 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-64c2acc6-7e08-41d5-9c96-0f47f6952948
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:43083
2024-04-19 19:01:39,362 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:43083
2024-04-19 19:01:39,361 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:37849
2024-04-19 19:01:39,362 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:37849
2024-04-19 19:01:39,362 - distributed.worker - INFO -          dashboard at:         10.201.3.251:40455
2024-04-19 19:01:39,362 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,362 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ql5r4tmd
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,362 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4lz4sqvu
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-67e7aaf5-74b4-4d4f-b7e2-13bef58f3125
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:38881
2024-04-19 19:01:39,362 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:38881
2024-04-19 19:01:39,362 - distributed.worker - INFO -          dashboard at:         10.201.3.251:43461
2024-04-19 19:01:39,362 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,362 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0kg1n01n
2024-04-19 19:01:39,362 - distributed.worker - INFO -          dashboard at:         10.201.3.251:39007
2024-04-19 19:01:39,362 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,362 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,362 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z9v9z_9q
2024-04-19 19:01:39,363 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,367 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a1f440b3-4f74-4f4f-9131-49842b57846c
2024-04-19 19:01:39,368 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:37899
2024-04-19 19:01:39,368 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:37899
2024-04-19 19:01:39,368 - distributed.worker - INFO -          dashboard at:         10.201.3.251:42241
2024-04-19 19:01:39,368 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,368 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,368 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,368 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k5nexkwu
2024-04-19 19:01:39,368 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,369 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e30fc8fb-75e3-40b1-84af-1e46821b1fba
2024-04-19 19:01:39,369 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.251:45171
2024-04-19 19:01:39,369 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.251:45171
2024-04-19 19:01:39,369 - distributed.worker - INFO -          dashboard at:         10.201.3.251:38471
2024-04-19 19:01:39,369 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.229:8786
2024-04-19 19:01:39,369 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,369 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,369 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gifl6y_p
2024-04-19 19:01:39,369 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,167 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,167 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,167 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,168 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,168 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,169 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,174 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,174 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,174 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,175 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,175 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,176 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,176 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,176 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,177 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,177 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,177 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,178 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,178 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,179 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,179 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:01:42,180 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.229:8786
2024-04-19 19:01:42,180 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,180 - distributed.core - INFO - Starting established connection to tcp://10.201.3.229:8786
2024-04-19 19:02:20,434 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:34939. Reason: scheduler-close
2024-04-19 19:02:20,434 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:41213. Reason: scheduler-close
2024-04-19 19:02:20,434 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:45171. Reason: scheduler-close
2024-04-19 19:02:20,434 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:37899. Reason: scheduler-close
2024-04-19 19:02:20,434 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:37849. Reason: scheduler-close
2024-04-19 19:02:20,435 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:44465. Reason: scheduler-close
2024-04-19 19:02:20,435 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:38881. Reason: scheduler-close
2024-04-19 19:02:20,435 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.251:43083. Reason: scheduler-close
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42758 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42758 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42754 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42754 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42772 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42772 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42722 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42722 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42710 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42710 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42738 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42738 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42776 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42776 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,435 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42716 remote=tcp://10.201.3.229:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.251:42716 remote=tcp://10.201.3.229:8786>: Stream is closed
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:40801'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:46815'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:40343'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:34261'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:36921'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:41223'. Reason: scheduler-close
2024-04-19 19:02:20,442 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:34669'. Reason: scheduler-close
2024-04-19 19:02:20,443 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.251:35083'. Reason: scheduler-close
2024-04-19 19:02:20,557 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,557 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,557 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,557 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,557 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,557 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,557 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,558 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,558 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,558 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,558 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,558 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,558 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,558 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.229:8786; closing.
2024-04-19 19:02:20,558 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,558 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,560 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,560 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:35083'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,938 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:41223'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,944 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:34669'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,995 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:34261'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,000 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:46815'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,072 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:40343'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,078 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:36921'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,085 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.251:40801'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,419 - distributed.dask_worker - INFO - End worker
