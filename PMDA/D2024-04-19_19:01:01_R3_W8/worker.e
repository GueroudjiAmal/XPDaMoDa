2024-04-19 19:01:35,424 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,424 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,425 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,425 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,425 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,425 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,426 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,426 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,673 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:46709'
2024-04-19 19:01:35,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:37407'
2024-04-19 19:01:35,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:41505'
2024-04-19 19:01:35,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:44203'
2024-04-19 19:01:35,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:36107'
2024-04-19 19:01:35,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:39075'
2024-04-19 19:01:35,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:36113'
2024-04-19 19:01:35,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.137:37329'
2024-04-19 19:01:36,705 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,705 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,705 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,706 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,708 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,709 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,709 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,709 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,710 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,710 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,710 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,751 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,751 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,753 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,753 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,754 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,754 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,754 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,755 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,746 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:39,072 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a5038c3a-bab3-4918-bf16-128d4a0a21ac
2024-04-19 19:01:39,072 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-83b29a9c-6e47-4bb4-9097-d03422d281b5
2024-04-19 19:01:39,072 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-476111e1-00a1-46c8-b6a3-eeaa12b6f12e
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:46069
2024-04-19 19:01:39,073 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:46069
2024-04-19 19:01:39,072 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-345410aa-f8d7-409a-8ffb-7a62683f85d0
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:37489
2024-04-19 19:01:39,073 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:37489
2024-04-19 19:01:39,073 - distributed.worker - INFO -          dashboard at:         10.201.2.137:46799
2024-04-19 19:01:39,072 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:33089
2024-04-19 19:01:39,073 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:33089
2024-04-19 19:01:39,073 - distributed.worker - INFO -          dashboard at:         10.201.2.137:35661
2024-04-19 19:01:39,073 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,073 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqv_rqwz
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35753
2024-04-19 19:01:39,073 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35753
2024-04-19 19:01:39,073 - distributed.worker - INFO -          dashboard at:         10.201.2.137:44989
2024-04-19 19:01:39,073 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,073 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d727sdzq
2024-04-19 19:01:39,073 - distributed.worker - INFO -          dashboard at:         10.201.2.137:36935
2024-04-19 19:01:39,073 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,073 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xhtzlr5q
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7209875e-e37f-4649-bd63-0561637aca31
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:38981
2024-04-19 19:01:39,073 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,073 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wxy0nx0g
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,073 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:38981
2024-04-19 19:01:39,073 - distributed.worker - INFO -          dashboard at:         10.201.2.137:38533
2024-04-19 19:01:39,073 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,074 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,074 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,074 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b41ltu4h
2024-04-19 19:01:39,074 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,079 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-d9cdddd3-93c5-47b9-9048-c8a1d114f32a
2024-04-19 19:01:39,079 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:46839
2024-04-19 19:01:39,079 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:46839
2024-04-19 19:01:39,079 - distributed.worker - INFO -          dashboard at:         10.201.2.137:36031
2024-04-19 19:01:39,079 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,079 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,079 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,079 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p5kdgnub
2024-04-19 19:01:39,079 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,083 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-831231b5-951d-4909-be7e-550c4f951571
2024-04-19 19:01:39,083 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:38977
2024-04-19 19:01:39,083 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:38977
2024-04-19 19:01:39,083 - distributed.worker - INFO -          dashboard at:         10.201.2.137:41141
2024-04-19 19:01:39,083 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,083 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,083 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,083 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-14r30qfn
2024-04-19 19:01:39,084 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,085 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3b5c1029-ec4f-481e-bdd5-e04efee585f4
2024-04-19 19:01:39,085 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.137:35217
2024-04-19 19:01:39,085 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.137:35217
2024-04-19 19:01:39,085 - distributed.worker - INFO -          dashboard at:         10.201.2.137:43663
2024-04-19 19:01:39,085 - distributed.worker - INFO - Waiting to connect to:    tcp://10.201.3.173:8786
2024-04-19 19:01:39,085 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,085 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,085 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kai5ctyv
2024-04-19 19:01:39,085 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,891 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,891 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,891 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,891 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,892 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,892 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,900 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,900 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,901 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,901 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,901 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,902 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,902 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,902 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,903 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,903 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,903 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,904 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,904 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,904 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,905 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:01:41,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:41,905 - distributed.worker - INFO -         Registered to:    tcp://10.201.3.173:8786
2024-04-19 19:01:41,906 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:41,906 - distributed.core - INFO - Starting established connection to tcp://10.201.3.173:8786
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35217. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:38981. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:38977. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:35753. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:46069. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:46839. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:37489. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.137:33089. Reason: scheduler-close
2024-04-19 19:02:20,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:37407'. Reason: scheduler-close
2024-04-19 19:02:20,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:44203'. Reason: scheduler-close
2024-04-19 19:02:20,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:46709'. Reason: scheduler-close
2024-04-19 19:02:20,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:36113'. Reason: scheduler-close
2024-04-19 19:02:20,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:39075'. Reason: scheduler-close
2024-04-19 19:02:20,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:37329'. Reason: scheduler-close
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:48178 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:48178 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:48200 remote=tcp://10.201.3.173:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.137:48200 remote=tcp://10.201.3.173:8786>: Stream is closed
2024-04-19 19:02:20,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:41505'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.137:36107'. Reason: scheduler-close
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,427 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,427 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.3.173:8786; closing.
2024-04-19 19:02:20,433 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,433 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,433 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,790 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:36107'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,791 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:46709'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,798 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:41505'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,813 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,921 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:37329'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,921 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:36113'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,927 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,933 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:44203'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,934 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:39075'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,940 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.137:37407'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,297 - distributed.dask_worker - INFO - End worker
