2024-04-19 19:01:35,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,669 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,670 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,892 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:35,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:46259'
2024-04-19 19:01:35,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:42125'
2024-04-19 19:01:35,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:39475'
2024-04-19 19:01:35,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:36465'
2024-04-19 19:01:35,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:42275'
2024-04-19 19:01:35,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:33521'
2024-04-19 19:01:35,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:35447'
2024-04-19 19:01:35,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.254:34475'
2024-04-19 19:01:36,938 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,938 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,941 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,941 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,942 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,942 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,942 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-19 19:01:36,942 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,942 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,943 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,943 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-19 19:01:36,981 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,985 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,986 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,987 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,987 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:36,987 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:37,974 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-19 19:01:39,273 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-ec8607c8-541c-450e-9d5d-7326728e6d7f
2024-04-19 19:01:39,273 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f8e43f18-95f4-4ca1-889f-850f616272be
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:39001
2024-04-19 19:01:39,273 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:39001
2024-04-19 19:01:39,273 - distributed.worker - INFO -          dashboard at:         10.201.3.254:34305
2024-04-19 19:01:39,273 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,273 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-8856f78e-9653-4eb1-8e57-dfa5764dc265
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:42141
2024-04-19 19:01:39,273 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:42141
2024-04-19 19:01:39,273 - distributed.worker - INFO -          dashboard at:         10.201.3.254:33553
2024-04-19 19:01:39,273 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,273 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,273 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,273 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ecrv0xak
2024-04-19 19:01:39,273 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:38357
2024-04-19 19:01:39,273 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:38357
2024-04-19 19:01:39,273 - distributed.worker - INFO -          dashboard at:         10.201.3.254:39215
2024-04-19 19:01:39,273 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,273 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,273 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,273 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,273 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,273 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pbucvn5y
2024-04-19 19:01:39,273 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i7wq446d
2024-04-19 19:01:39,274 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,273 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-fb6b1c17-8fd3-42cf-901f-1c2cbd8031e6
2024-04-19 19:01:39,274 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:37823
2024-04-19 19:01:39,274 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:37823
2024-04-19 19:01:39,274 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,274 - distributed.worker - INFO -          dashboard at:         10.201.3.254:44191
2024-04-19 19:01:39,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,274 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,274 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,274 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ixvgimfx
2024-04-19 19:01:39,274 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,278 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-07dedb52-0da7-46c8-b2e2-5d9d1d0e592f
2024-04-19 19:01:39,278 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:42649
2024-04-19 19:01:39,278 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:42649
2024-04-19 19:01:39,278 - distributed.worker - INFO -          dashboard at:         10.201.3.254:33029
2024-04-19 19:01:39,278 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,278 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,279 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,279 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nisn4x9r
2024-04-19 19:01:39,279 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,284 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-abbe66cc-a91c-46d1-8c68-ad3f6eeb6d55
2024-04-19 19:01:39,285 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:38761
2024-04-19 19:01:39,285 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:38761
2024-04-19 19:01:39,285 - distributed.worker - INFO -          dashboard at:         10.201.3.254:41159
2024-04-19 19:01:39,285 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,285 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,285 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,285 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llu61lac
2024-04-19 19:01:39,285 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,289 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-3401ab75-73ab-47b6-85da-0e706fba6dbf
2024-04-19 19:01:39,289 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:41797
2024-04-19 19:01:39,289 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:41797
2024-04-19 19:01:39,289 - distributed.worker - INFO -          dashboard at:         10.201.3.254:36501
2024-04-19 19:01:39,289 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,290 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,290 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,290 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v72qm_a6
2024-04-19 19:01:39,290 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,291 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-b9aa77f1-042b-4e51-a446-3b269a8fa572
2024-04-19 19:01:39,291 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.254:43195
2024-04-19 19:01:39,291 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.254:43195
2024-04-19 19:01:39,291 - distributed.worker - INFO -          dashboard at:         10.201.3.254:33049
2024-04-19 19:01:39,291 - distributed.worker - INFO - Waiting to connect to:      tcp://10.201.4.7:8786
2024-04-19 19:01:39,291 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:39,291 - distributed.worker - INFO -               Threads:                          8
2024-04-19 19:01:39,291 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-19 19:01:39,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p8lo_p0s
2024-04-19 19:01:39,292 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,343 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,343 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,343 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,344 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,344 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,345 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,345 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,352 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,352 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,352 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,353 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,353 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,353 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,354 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,354 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,355 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,355 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,355 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,356 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,356 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,357 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-19 19:01:42,357 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:01:42,358 - distributed.worker - INFO -         Registered to:      tcp://10.201.4.7:8786
2024-04-19 19:01:42,358 - distributed.worker - INFO - -------------------------------------------------
2024-04-19 19:01:42,358 - distributed.core - INFO - Starting established connection to tcp://10.201.4.7:8786
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:38761. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:39001. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:41797. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:38357. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:43195. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:37823. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:42141. Reason: scheduler-close
2024-04-19 19:02:20,304 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.254:42649. Reason: scheduler-close
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42258 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42258 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42246 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42246 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42284 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42284 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42274 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42274 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42312 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42312 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42326 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42326 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42342 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42342 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,305 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42298 remote=tcp://10.201.4.7:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.254:42298 remote=tcp://10.201.4.7:8786>: Stream is closed
2024-04-19 19:02:20,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:42125'. Reason: scheduler-close
2024-04-19 19:02:20,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:39475'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:42275'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:34475'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:36465'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:33521'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:46259'. Reason: scheduler-close
2024-04-19 19:02:20,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.254:35447'. Reason: scheduler-close
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,427 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,428 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,431 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,431 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,431 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,431 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,431 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:20,432 - distributed.core - INFO - Received 'close-stream' from tcp://10.201.4.7:8786; closing.
2024-04-19 19:02:20,432 - distributed.nanny - INFO - Worker closed
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,433 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-19 19:02:22,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:35447'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,773 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:33521'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,781 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,828 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:39475'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,829 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:42125'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,902 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:36465'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,908 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:34475'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,914 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:22,920 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:42275'. Reason: nanny-close-gracefully
2024-04-19 19:02:22,920 - distributed.dask_worker - INFO - End worker
2024-04-19 19:02:23,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.254:46259'. Reason: nanny-close-gracefully
2024-04-19 19:02:23,278 - distributed.dask_worker - INFO - End worker
