2024-04-18 19:26:34,224 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,224 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,224 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,224 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,423 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,423 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,423 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,423 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:41523'
2024-04-18 19:26:34,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:38161'
2024-04-18 19:26:34,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:39849'
2024-04-18 19:26:34,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.105:42687'
2024-04-18 19:26:34,691 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,691 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,691 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,691 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,919 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,919 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,919 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,919 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:40409'
2024-04-18 19:26:34,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:45261'
2024-04-18 19:26:34,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:35961'
2024-04-18 19:26:34,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.107:40355'
2024-04-18 19:26:35,170 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,170 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,171 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,171 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,171 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,171 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,171 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,194 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,195 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,195 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,195 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,923 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,924 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,924 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,924 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,924 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,924 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,924 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,924 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,968 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,968 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,969 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,969 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:36,067 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,067 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,067 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,067 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,961 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,961 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,961 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,961 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:37,011 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a9b0a9a2-5586-4604-aea0-0d85d13bce4f
2024-04-18 19:26:37,011 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-ece044f3-49e9-4ba7-a156-4dc4614db9e8
2024-04-18 19:26:37,011 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-aed665c1-50f5-4887-b70d-bd162c4ca94c
2024-04-18 19:26:37,011 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-0ab6cbde-70f7-4c68-ba91-add2a7e7b108
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:45205
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:40421
2024-04-18 19:26:37,011 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:40421
2024-04-18 19:26:37,011 - distributed.worker - INFO -          dashboard at:         10.201.3.105:41587
2024-04-18 19:26:37,011 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,011 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:45285
2024-04-18 19:26:37,011 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:45285
2024-04-18 19:26:37,011 - distributed.worker - INFO -          dashboard at:         10.201.3.105:40301
2024-04-18 19:26:37,011 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.105:36179
2024-04-18 19:26:37,011 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:36179
2024-04-18 19:26:37,011 - distributed.worker - INFO -          dashboard at:         10.201.3.105:46553
2024-04-18 19:26:37,011 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.105:45205
2024-04-18 19:26:37,011 - distributed.worker - INFO -          dashboard at:         10.201.3.105:39505
2024-04-18 19:26:37,011 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,011 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xmd7ak4b
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tq_xd_b9
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,011 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fpxchrw5
2024-04-18 19:26:37,011 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,011 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ieglpp7
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,011 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1421cf1c-bcd9-4981-9356-329dfdcbbe8c
2024-04-18 19:26:38,031 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-2b8d5197-e5ac-41c0-b115-366ac3aa01ae
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:43809
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:37823
2024-04-18 19:26:38,031 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:37823
2024-04-18 19:26:38,031 - distributed.worker - INFO -          dashboard at:         10.201.3.107:39973
2024-04-18 19:26:38,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-046e771c-bc9a-463b-9bd0-7273ca3e6016
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:38293
2024-04-18 19:26:38,031 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:38293
2024-04-18 19:26:38,031 - distributed.worker - INFO -          dashboard at:         10.201.3.107:40045
2024-04-18 19:26:38,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-1ad107b6-2eef-4465-939b-69a95477de44
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Start worker at:   tcp://10.201.3.107:45307
2024-04-18 19:26:38,031 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:45307
2024-04-18 19:26:38,031 - distributed.worker - INFO -          dashboard at:         10.201.3.107:36095
2024-04-18 19:26:38,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:38,031 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7dc7e3n0
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO -          Listening to:   tcp://10.201.3.107:43809
2024-04-18 19:26:38,031 - distributed.worker - INFO -          dashboard at:         10.201.3.107:38403
2024-04-18 19:26:38,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.97:8786
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:38,031 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pfn1r16j
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:38,031 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lpb_dmb6
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:38,031 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:38,031 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:38,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bkz6xwr8
2024-04-18 19:26:38,031 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,532 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,532 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,533 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,534 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,534 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,534 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,537 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,537 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,538 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,538 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,538 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,539 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,540 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,540 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,540 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,541 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,541 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,541 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,542 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,542 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,542 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:40,542 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:26:40,543 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.97:8786
2024-04-18 19:26:40,543 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:40,543 - distributed.core - INFO - Starting established connection to tcp://10.201.3.97:8786
2024-04-18 19:27:20,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:25,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:25,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:25,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:28,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:28,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:28,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:28,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:28:53,072 - distributed.utils_perf - INFO - full garbage collection released 31.06 MiB from 138 reference cycles (threshold: 9.54 MiB)
2024-04-18 19:30:11,136 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,137 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:36179. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,138 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,138 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:45205. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,140 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,140 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:45285. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,140 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,141 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.105:40421. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,138 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43056 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43056 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,141 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43068 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43068 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,142 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43034 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43034 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,139 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43040 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.105:43040 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,144 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:39849'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,144 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,144 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:42687'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,144 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:37823. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,145 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:41523'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,145 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.105:38161'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,145 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,145 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:38293. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,145 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,145 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:45307. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,148 - distributed.core - INFO - Connection to tcp://10.201.3.97:8786 has been closed.
2024-04-18 19:30:11,148 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.107:43809. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,145 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34454 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34454 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,147 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34474 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34474 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,147 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34468 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34468 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,149 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34466 remote=tcp://10.201.3.97:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.107:34466 remote=tcp://10.201.3.97:8786>: Stream is closed
2024-04-18 19:30:11,152 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:45261'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,152 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:40409'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,152 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:40355'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,152 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.107:35961'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:11,248 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,248 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,248 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,248 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,256 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,257 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,257 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:11,257 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:13,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,257 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,258 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,258 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:13,258 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:41523'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,212 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:40355'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,213 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,314 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:38161'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,315 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:45261'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,318 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,321 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:42687'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,321 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:40409'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,325 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.107:35961'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,649 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:14,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.105:39849'. Reason: nanny-close-gracefully
2024-04-18 19:30:14,656 - distributed.dask_worker - INFO - End worker
