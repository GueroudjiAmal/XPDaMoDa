2024-04-18 19:26:34,598 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,598 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,599 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,599 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,611 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,611 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,612 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,612 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:34,859 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,859 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,859 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,860 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,860 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,860 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,860 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,860 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.255:34965'
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.255:41167'
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.4:44333'
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.4:36501'
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.4:35323'
2024-04-18 19:26:34,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.3.4:41183'
2024-04-18 19:26:34,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.255:46687'
2024-04-18 19:26:34,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.201.2.255:34235'
2024-04-18 19:26:35,883 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,883 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,884 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,884 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,884 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,884 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,884 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,885 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,887 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,888 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,888 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,888 - distributed.preloading - INFO - Creating preload: MofkaWorkerPlugin.py
2024-04-18 19:26:35,888 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,888 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,889 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,889 - distributed.utils - INFO - Reload module MofkaWorkerPlugin from .py file
2024-04-18 19:26:35,929 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,929 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,929 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,929 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,933 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,933 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,933 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:35,933 - distributed.preloading - INFO - Import preload module: MofkaWorkerPlugin.py
2024-04-18 19:26:36,907 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,907 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,907 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,907 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,908 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,908 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,908 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:36,908 - distributed.preloading - INFO - Run preload setup: MofkaWorkerPlugin.py
2024-04-18 19:26:37,979 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-14bb2b2b-c69d-4486-ba2c-309beffa260b
2024-04-18 19:26:37,979 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.255:40505
2024-04-18 19:26:37,979 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.255:40505
2024-04-18 19:26:37,980 - distributed.worker - INFO -          dashboard at:         10.201.2.255:39997
2024-04-18 19:26:37,980 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,980 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,980 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,980 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,980 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-7b78de5a-1362-489b-b2d9-37841ed2a6b0
2024-04-18 19:26:37,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-beawopzo
2024-04-18 19:26:37,980 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,980 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.255:35261
2024-04-18 19:26:37,980 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.255:35261
2024-04-18 19:26:37,980 - distributed.worker - INFO -          dashboard at:         10.201.2.255:34803
2024-04-18 19:26:37,980 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,980 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,980 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,980 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vhd61zfa
2024-04-18 19:26:37,980 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,984 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-245e47a5-847f-4fb8-81e1-f69b31e5c39e
2024-04-18 19:26:37,984 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.255:36919
2024-04-18 19:26:37,984 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.255:36919
2024-04-18 19:26:37,984 - distributed.worker - INFO -          dashboard at:         10.201.2.255:40533
2024-04-18 19:26:37,984 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,984 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,984 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,984 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,984 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tz850op0
2024-04-18 19:26:37,984 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,987 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-f716c80b-a537-49d6-a0a1-4c92314a7a1b
2024-04-18 19:26:37,987 - distributed.worker - INFO -       Start worker at:   tcp://10.201.2.255:42543
2024-04-18 19:26:37,987 - distributed.worker - INFO -          Listening to:   tcp://10.201.2.255:42543
2024-04-18 19:26:37,988 - distributed.worker - INFO -          dashboard at:         10.201.2.255:38039
2024-04-18 19:26:37,988 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,988 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,988 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,988 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,988 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nqv6vij1
2024-04-18 19:26:37,988 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-95d7cf34-d69a-4e1b-a787-8356b9b658e8
2024-04-18 19:26:37,991 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-e4fff5de-5004-4689-8393-ebb0aeabe090
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Start worker at:     tcp://10.201.3.4:37935
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Start worker at:     tcp://10.201.3.4:33919
2024-04-18 19:26:37,991 - distributed.worker - INFO -          Listening to:     tcp://10.201.3.4:33919
2024-04-18 19:26:37,991 - distributed.worker - INFO -          dashboard at:           10.201.3.4:44511
2024-04-18 19:26:37,991 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO -          Listening to:     tcp://10.201.3.4:37935
2024-04-18 19:26:37,991 - distributed.worker - INFO -          dashboard at:           10.201.3.4:33165
2024-04-18 19:26:37,991 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-a804f4d1-b642-43fe-ae28-985f5ef25658
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Start worker at:     tcp://10.201.3.4:36599
2024-04-18 19:26:37,991 - distributed.worker - INFO -          Listening to:     tcp://10.201.3.4:36599
2024-04-18 19:26:37,991 - distributed.worker - INFO -          dashboard at:           10.201.3.4:33247
2024-04-18 19:26:37,991 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO - Starting Worker plugin MofkaWorkerPlugin-52ef75ae-b44e-4773-88ad-a4216e3932df
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Start worker at:     tcp://10.201.3.4:46821
2024-04-18 19:26:37,991 - distributed.worker - INFO -          Listening to:     tcp://10.201.3.4:46821
2024-04-18 19:26:37,991 - distributed.worker - INFO -          dashboard at:           10.201.3.4:33973
2024-04-18 19:26:37,991 - distributed.worker - INFO - Waiting to connect to:     tcp://10.201.3.27:8786
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,991 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t5t2orcr
2024-04-18 19:26:37,991 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,991 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z2aw4u27
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,991 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ezef185o
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO -               Threads:                          8
2024-04-18 19:26:37,992 - distributed.worker - INFO -                Memory:                 503.22 GiB
2024-04-18 19:26:37,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nn7v9me9
2024-04-18 19:26:37,992 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:37,991 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,718 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,718 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,719 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,720 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,720 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,721 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,723 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,723 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,724 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,724 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,725 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,725 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,726 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,726 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,726 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,727 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,727 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,728 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,728 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,728 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-04-18 19:26:41,728 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:26:41,729 - distributed.worker - INFO -         Registered to:     tcp://10.201.3.27:8786
2024-04-18 19:26:41,729 - distributed.worker - INFO - -------------------------------------------------
2024-04-18 19:26:41,729 - distributed.core - INFO - Starting established connection to tcp://10.201.3.27:8786
2024-04-18 19:27:21,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:27:27,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2024-04-18 19:30:12,011 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,011 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.255:42543. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,014 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,015 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.4:46821. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,015 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,015 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,016 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.255:36919. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,015 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,016 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.4:37935. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,016 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.4:36599. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,017 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,017 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.255:35261. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,013 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57642 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57642 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,017 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57636 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57636 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,019 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,019 - distributed.worker - INFO - Stopping worker at tcp://10.201.3.4:33919. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.255:46687'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,019 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57628 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57628 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,020 - distributed.core - INFO - Connection to tcp://10.201.3.27:8786 has been closed.
2024-04-18 19:30:12,020 - distributed.worker - INFO - Stopping worker at tcp://10.201.2.255:40505. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.255:34965'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,021 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.255:41167'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,016 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37672 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37672 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,017 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37670 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37670 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,017 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37678 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37678 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,021 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37654 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.3.4:37654 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,022 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57624 remote=tcp://10.201.3.27:8786>
Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/agueroudji/spack/var/spack/environments/mofkadask/.spack-env/view/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.201.2.255:57624 remote=tcp://10.201.3.27:8786>: Stream is closed
2024-04-18 19:30:12,023 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.4:44333'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,023 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.4:35323'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,023 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.4:41183'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,024 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.3.4:36501'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,024 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.201.2.255:34235'. Reason: worker-handle-scheduler-connection-broken
2024-04-18 19:30:12,123 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,124 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,127 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,127 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,127 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,128 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,128 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:12,140 - distributed.nanny - INFO - Worker closed
2024-04-18 19:30:14,128 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,128 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,128 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,129 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,130 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,130 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,140 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:14,141 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-04-18 19:30:15,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.4:41183'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,227 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.255:41167'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,228 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.4:44333'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.255:34965'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,333 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,333 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.4:36501'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,339 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.255:46687'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,340 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.3.4:35323'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,666 - distributed.dask_worker - INFO - End worker
2024-04-18 19:30:15,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.201.2.255:34235'. Reason: nanny-close-gracefully
2024-04-18 19:30:15,679 - distributed.dask_worker - INFO - End worker
